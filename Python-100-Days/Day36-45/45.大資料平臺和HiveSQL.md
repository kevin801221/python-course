## Hive簡介

[Hive](https://hive.apache.org/) 是 Facebook 開源的一款基於 Hadoop 的資料倉儲工具，目前由 Apache 軟體基金會維護，它是應用最廣泛的大資料處理解決方案，它能將 SQL 查詢轉變為 MapReduce（Google提出的一個軟體架構，用於大規模資料集的並行運算）任務，對 SQL 提供了完美的支援，能夠非常方便的實現大資料統計。

<img src="res/sql_to_mr.png" style="zoom:50%;">

<img src="res/HADOOP-ECOSYSTEM-Edureka.png">

> **說明**：可以透過<https://www.edureka.co/blog/hadoop-ecosystem>來了解 Hadoop 生態圈。

如果要簡單的介紹 Hive，那麼以下兩點是其核心：

1. 把 HDFS 中結構化的資料對映成表。
2. 透過把 HQL 進行解析和轉換，最終生成一系列基於 Hadoop 的 MapReduce 任務或 Spark 任務，透過執行這些任務完成對資料的處理。也就是說，即便不學習 Java、Scala 這樣的程式語言，一樣可以實現對資料的處理。

Hive的應用場景。

<img src="res/what_hive_can_do.png" style="zoom:50%;">

<img src="res/what_hive_can_not_do.png" style="zoom:35%;">

Hive和傳統關係型資料庫的對比如下圖和下表所示。

<img src="res/hive_vs_rdbms.png" style="zoom:50%;">

|          | Hive              | RDBMS        |
| -------- | ----------------- | ------------ |
| 查詢語言 | HQL               | SQL          |
| 儲存資料 | HDFS              | 本地檔案系統 |
| 執行方式 | MapReduce / Spark | Executor     |
| 執行延遲 | 高                | 低           |
| 資料規模 | 大                | 小           |

### 準備工作

1. 搭建如下圖所示的大資料平臺。

    <img src="res/20220210080638.png" style="zoom:60%;">

2. 透過Client節點（跳板機）訪問大資料平臺。

    <img src="res/20220210080655.png" style="zoom:50%;">

3. 建立檔案Hadoop的檔案系統。

    ```Shell
    hdfs dfs -mkdir /user/root
    ```

4. 將準備好的資料檔案複製到Hadoop檔案系統中。

    ```Shell
    hdfs dfs -put /home/ubuntu/data/* /user/root
    ```

5. 進入 hive 命令列。

    ```Shell
    hive
    ```

### 建庫建表

1. 建立。

    ```SQL
    create database eshop;
    ```

2. 刪除。

    ```SQL
    drop database eshop cascade;
    ```

3. 切換。

    ```SQL
    use eshop;
    ```

#### 資料型別

Hive的資料型別如下所示。

<img src="res/hive_data_types.png" style="zoom:50%;">

基本資料型別：

| 資料型別  | 佔用空間 | 支援版本 |
| --------- | -------- | -------- |
| tinyint   | 1-Byte   |          |
| smallint  | 2-Byte   |          |
| int       | 4-Byte   |          |
| bigint    | 8-Byte   |          |
| boolean   |          |          |
| float     | 4-Byte   |          |
| double    | 8-Byte   |          |
| string    |          |          |
| binary    |          | 0.8版本  |
| timestamp |          | 0.8版本  |
| decimal   |          | 0.11版本 |
| char      |          | 0.13版本 |
| varchar   |          | 0.12版本 |
| date      |          | 0.12版本 |

複合資料型別：

| 資料型別 | 描述                     | 例子                                          |
| -------- | ------------------------ | --------------------------------------------- |
| struct   | 和C語言中的結構體類似    | `struct<first_name:string, last_name:string>` |
| map      | 由鍵值對構成的元素的集合 | `map<string,int>`                             |
| array    | 具有相同型別的變數的容器 | `array<string>`                               |

4. 建立內部表。

    ```SQL
    create table if not exists dim_user_info 
    (
    user_id string,
    user_name string, 
    sex string,
    age int,
    city string,
    firstactivetime string,
    level int,
    extra1 string,
    extra2 map<string,string>
    )
    row format delimited fields terminated by '\t'
    collection items terminated by ','
    map keys terminated by ':'
    lines terminated by '\n'
    stored as textfile;
    ```

5. 載入資料。

    ```SQL
    load data local inpath '/home/ubuntu/data/user_info/user_info.txt' overwrite into table dim_user_info;
    ```

    或

    ```SQL
    load data inpath '/user/root/user_info.txt' overwrite into table dim_user_info;
    ```

6. 建立分割槽表。

    ```SQL
    create table if not exists fact_user_trade 
    (
    user_name string,
    piece int,
    price double,
    pay_amount double,
    goods_category string,
    pay_time bigint
    )  
    partitioned by (dt string)
    row format delimited fields terminated by '\t';
    ```

7. 提供分割槽資料。

    ```Shell
    hdfs dfs -put /home/ubuntu/data/user_trade/* /user/hive/warehouse/eshop.db/fact_user_trade
    ```

8. 設定動態分割槽。

    ```SQL
    set hive.exec.dynamic.partition=true;
    set hive.exec.dynamic.partition.mode=nonstrict;
    set hive.exec.max.dynamic.partitions=10000;
    set hive.exec.max.dynamic.partitions.pernode=10000;
    ```

9. 修復分割槽。

    ```SQL
    msck repair table fact_user_trade;
    ```

### 查詢

#### 基本語法

```SQL
-- 查詢北京女使用者的姓名取前10個
select user_name from dim_user_info where city='beijing' and sex='female' limit 10;

-- 查詢2019年3月24日購買了food類商品的使用者名稱、購買數量和支付金額（不聚合）
select user_name, piece, pay_amount from fact_user_trade where dt='2019-03-24' and goods_category='food';

-- 統計使用者 ELLA 在2018年的總支付金額和最近最遠兩次消費間隔天數
select sum(pay_amount) as total, datediff(max(from_unixtime(pay_time, 'yyyy-MM-dd')), min(from_unixtime(pay_time, 'yyyy-MM-dd'))) from fact_user_trade where year(dt)='2018' and user_name='ELLA';
```

#### group by

```SQL
-- 查詢2019年1月到4月，每個品類有多少人購買，累計金額是多少
select goods_category, count(distinct user_name) as total_user, sum(pay_amount) as total_pay from fact_user_trade where dt between '2019-01-01' and '2019-04-30' group by goods_category;
```

```SQL
-- 查詢2019年4月支付金額超過5萬元的使用者
select user_name, sum(pay_amount) as total from fact_user_trade where dt between '2019-04-01' and '2019-04-30' group by user_name having sum(pay_amount) > 50000;
```

```hive
-- 查詢2018年購買的商品品類在兩個以上的使用者數
select count(tmp.user_name) from (select user_name, count(distinct goods_category) as total from fact_user_trade where year(dt)='2018' group by user_name having count(distinct goods_category)>2) tmp;
```

#### order by

```SQL
-- 查詢2019年4月支付金額最多的使用者前5名
select user_name, sum(pay_amount) as total from fact_user_trade where dt between '2019-04-01' and '2019-04-30' group by user_name order by total desc limit 5;
```

#### 常用函式

1. `from_unixtime`：將時間戳轉換成日期

    ```hive
    select from_unixtime(pay_time, 'yyyy-MM-dd hh:mm:ss') from fact_user_trade limit 10;
    ```

2. `unix_timestamp`：將日期轉換成時間戳

3. `datediff`：計算兩個日期的時間差

    ```Hive
    -- 使用者首次啟用時間與設定參照時間的間隔
    select user_name, datediff('2019-4-1', to_date(firstactivetime)) from dim_user_info limit 10;
    ```

4. `if`：根據條件返回不同的值

    ```Hive
    -- 統計不同年齡段的使用者數
    select case when age < 20 then '20歲以下' when age < 30 then '30歲以下' when age < 40 then '40歲以下' else '40歲以上' end as age_seg, count(distinct user_id) as total from dim_user_info group by case when age < 20 then '20歲以下' when age < 30 then '30歲以下' when age < 40 then '40歲以下' else '40歲以上' end;
    ```

    ```Hive
    -- 不同性別高階等使用者數量
    select sex, if(level > 5, '高', '低') as level_type, count(distinct user_id) as total from dim_user_info group by sex, if(level > 5, '高', '低');
    ```

5. `substr`：字串取子串

    ```Hive
    -- 統計每個月啟用的新使用者數
    select substr(firstactivetime, 1, 7) as month, count(distinct user_id) as total from dim_user_info group by substr(firstactivetime, 1, 7);
    ```

6. `get_json_object`：從JSON字串中取出指定的`key`對應的`value`，如：`get_json_object(info, '$.first_name')`。

    ```Hive
    -- 統計不同手機品牌的使用者數
    select get_json_object(extra1, '$.phonebrand') as phone, count(distinct user_id) as total from user_info group by get_json_object(extra1, '$.phonebrand');
    
    select extra2['phonebrand'] as phone, count(distinct user_id) as total from user_info group by extra2['phonebrand'];
    ```

    > 說明：MySQL對應的函式名字叫`json_extract`。
