## 決策樹和隨機森林

**決策樹**（Decision Tree）是一種基於樹結構的監督學習演算法，可用於**分類**和**迴歸**任務。它透過將資料集逐步分割成不同的子集，直到滿足某些停止條件，以此實現預測目標。我們生活中做決策的時候也會用到類似的方法，例如某位女生約見相親物件的決策方法就可以繪製成如下所示的決策樹。

<img src="res/03_decision_tree_in_life.png" style="zoom:38%;">

> **說明**：上圖僅用於幫助大家理解什麼是決策樹，無不良引導，也不代表本人的愛情觀和婚姻觀。

如果具備一定的程式設計常識，你會發現用決策樹做預測的過程相當於是執行了一系列的`if...else...`結構；如果你對機率論的知識更熟悉，那麼決策樹的構建也可以視為計算以特徵空間為前提的條件機率的過程。決策樹中的結點可以分為兩類：內部結點（上圖中藍色的結點）和葉結點（上圖中紅色和綠色的結點），其中內部結點對應樣本特徵屬性測試（特徵分割條件），而葉結點代表決策的結果（分類標籤或迴歸目標值）。決策樹可以用於解決分類問題和迴歸問題，本章我們仍然把重點放在分類問題上。

### 決策樹的構建

#### 特徵選擇

訓練決策樹模型有三個核心的步驟：特徵選擇、決策樹構建和決策樹剪枝。特徵選擇決定了使用哪些特徵來做判斷，在訓練資料集中，每個樣本的屬性可能有很多個，不同屬性的作用有大有小。特徵選擇的作用就是篩選出跟分類結果相關性較高的特徵，也就是分類能力較強的特徵，這樣的特徵要出現在決策樹靠上的位置。如果一個特徵能夠使得分類後的分支結點儘可能屬於同一類別，即該結點有著較高的**純度**（purity），那麼該特徵對資料集而言就具備較強的分類能力。這裡就產生了第一個問題，我們應該按照什麼樣的標準來選擇分類能力強的特徵？在決策樹模型中，我們有三種方式來選擇分類能力強的特徵，分別是：**資訊增益**（information gain）、**資訊增益比**（gain ratio）和**基尼指數**（Gini index）。

要講清楚資訊增益，我們得先介紹**資訊熵**（information entropy）這個概念。1948年，克勞德·夏農（*Claude Shannon*）在他的著名論文《通訊的數學原理》中提出了“資訊熵”的概念，它解決了資訊的度量問題，量化出了資訊的價值。“熵”原本是熱力學領域的概念，它反映的是系統的混亂程度，熵越大，系統的混亂程度就越高。在資訊理論中，熵可以看作隨機變數（資料集）不確定性的度量，熵越大，變數（資料）的不確定性就越大，那麼要確定它所需要獲取的資訊量也就越大；熵越低，資料的純度就越高，不確定性就越小。

例如，甲、乙兩人參加一個射擊比賽，如果從歷史成績來看，甲的勝率是100%，那麼我們很容易接受甲會獲勝這個結果；如果從歷史成績來看，甲的勝率是50%，那麼我們就難以確定到底誰會獲勝。克勞德·夏農提出可以用下面的公式來描述這種不確定性：

$$
H(D) = -\sum_{i = 1}^{k} P(x_i)log_2P(x_i)
$$

其中， $\small{D}$ 代表資料集， $\small{k}$ 代表類別的總數， $\small{P(x_i)}$ 表示資料集中第 $\small{i}$ 類樣本的比例（機率）。我們用 $\small{x_1}$ 和 $\small{x_2}$ 來分別表示甲獲勝和乙獲勝，很顯然，當 $\small{P(x_1)=0.5}$ ， $\small{P(x_2)=0.5}$ 時， $\small{H=1}$ ，不確定性最大；當 $\small{P(x_1)=1}$ ， $\small{P(x_2)=0}$ 時， $\small{H=0}$ ，不確定性最小；當 $\small{P(x_1)=0.8}$ ， $\small{P(x_2)=0.2}$ 時， $\small{H \approx 0.72}$ 。

很顯然，知道的資訊越多，隨機變數（資料集）的不確定性就越小。這些資訊，可以是直接針對我們想了解的隨機事件的資訊，也可以是和我們關心的隨機事件相關的其他事件的資訊。在數學上可以嚴格的證明這些相關的資訊也能夠降低或消除不確定性，為此我們定義**條件熵**，它表示在給定特徵 $\small{A}$ 的條件下，資料集 $\small{D}$ 的不確定性。條件熵的公式如下所示：

$$
H(D \vert A) = \sum_{v \in A}\frac{\lvert D_{v} \rvert}{\lvert D \rvert}H(D_{v})
$$

上面的公式中，我們讓 $\small{v}$ 取得特徵 $\small{A}$ 所有可能的取值，其中 $\small{D_{v}}$ 代表特徵 $\small{A}$ 取值為 $\small{v}$ 的樣本子集， $\small{\frac{\lvert D_{v} \rvert}{\lvert D \rvert}}$ 代表權重，即特徵取值為 $\small{v}$ 的樣本比例。可以證明 $\small{H(D) \ge H(D \vert A)}$ ，也就是說多了特徵 $\small{A}$ 的資訊之後，資料集 $\small{D}$ 的不確定性下降了。當然，還要注意等號成立的情況，也就是說增加了特徵 $\small{A}$ 的資訊，但是 $\small{D}$ 的不確定沒有下降，也就是說我們獲取的資訊與要研究的內容沒有關係。

有了上面的鋪墊，接下來我們就可以給出**資訊增益**的定義，它是在得到特徵 $\small{A}$ 的資訊後，資料集 $\small{D}$ 的不確定性減少的程度。換句話說，資訊增益是一種描述資料集確定性增加的量，特徵的資訊增益越大，特徵的分類能力就越強，在給定該特徵後資料集的確定性就越大。資訊增益可以透過下面的數學公式直觀的描述：

$$
g(D, A) = E(D) - E(D \vert A)
$$

計算資訊熵和資訊增益的函式如下所示：

```python
import numpy as np


def entropy(y):
    """
    計算資訊熵
    :param y: 資料集的目標值
    :return: 資訊熵
    """
    _, counts = np.unique(y, return_counts=True)
    prob = counts / y.size
    return -np.sum(prob * np.log2(prob))


def info_gain(x, y):
    """
    計算資訊增益
    :param x: 給定的特徵
    :param y: 資料集的目標值
    :return: 資訊增益
    """
    values, counts = np.unique(x, return_counts=True)
    new_entropy = 0
    for i, value in enumerate(values):
        prob = counts[i] / x.size
        new_entropy += prob * entropy(y[x == value])
    return entropy(y) - new_entropy
```

經典決策樹演算法中的 ID3 演算法就是基於資訊增益進行特徵選擇的，我們仍然以鳶尾花資料集為例，將資料集拆分為訓練集和測試集，在訓練集上計算出原始的資訊熵以及引入四個特徵（花萼長度、花萼寬度、花瓣長度、花瓣寬度）後的資訊增益分別是多少，程式碼如下所示。

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=3)
print(f'H(D)    = {entropy(y_train)}')
print(f'g(D,A0) = {info_gain(X_train[:, 0], y_train)}')
print(f'g(D,A1) = {info_gain(X_train[:, 1], y_train)}')
print(f'g(D,A2) = {info_gain(X_train[:, 2], y_train)}')
print(f'g(D,A3) = {info_gain(X_train[:, 3], y_train)}')
```

輸出：

```
H(D)    = 1.584962500721156
g(D,A0) = 0.9430813063736728
g(D,A1) = 0.5692093930591595
g(D,A2) = 1.4475439590905472
g(D,A3) = 1.4420095891994646
```

> **注意**：如果在劃分訓練集和測試集的時候，`train_test_split`函式的`random_state`引數跟我上面的程式碼不同，那麼這裡的運算結果可能會存在差異。

根據上面的輸出我們可以得知，花瓣長度（對應上面的`A2`）這個特徵資訊增益最高，也就是說使用花瓣長度這個特徵分類能力最強，分類後的資料集純度最高。需要注意的是，當某個特徵取值較多時，該特徵的資訊增益計算結果就會比較大，所以使用資訊增益選擇特徵時，會偏向於取值較多的特徵。為了解決這個問題，我們可以計算**資訊增益比**，它的定義如下所示：

$$
R(D, A) = \frac{g(D, A)}{E_{A}(D)}
$$

其中， $\small{E_{A}(D) = -\sum_{i=1}^{n}{\frac{\lvert D_{i} \rvert}{\lvert D \rvert}log_{2}\frac{\lvert D_{i} \rvert}{\lvert D \rvert}}}$ ， $\small{n}$ 表示特徵 $\small{A}$ 的取值個數，簡單的說 $\small{E_{A}(D)}$ 就是特徵 $\small{A}$ 的資訊熵，而資訊增益比就是特徵 $\small{A}$ 的資訊增益和特徵 $\small{A}$ 的資訊熵的比值。我們用下面的函式來計算資訊增益比，呼叫該函式可以輸出四個特徵對應的資訊增益比。

```python
def info_gain_ratio(x, y):
    """
    計算資訊增益比
    :param x: 給定的特徵
    :param y: 資料集的目標值
    :return: 資訊增益比
    """
    return info_gain(x, y) / entropy(x)


print(f'R(D,A0) = {info_gain_ratio(X_train[:, 0], y_train)}')
print(f'R(D,A1) = {info_gain_ratio(X_train[:, 1], y_train)}')
print(f'R(D,A2) = {info_gain_ratio(X_train[:, 2], y_train)}')
print(f'R(D,A3) = {info_gain_ratio(X_train[:, 3], y_train)}')
```

輸出：

```
R(D,A0) = 0.19687406476459068
R(D,A1) = 0.14319788821311977
R(D,A2) = 0.28837763858461984
R(D,A3) = 0.35550822529855447
```

> **注意**：如果在劃分訓練集和測試集的時候，`train_test_split`函式的`random_state`引數跟我上面的程式碼不同，那麼這裡的運算結果可能會存在差異。

根據上面的輸出我們可以得知，花瓣寬度（對應上面的`A3`）這個特徵資訊增益比最高，也就是說這個特徵能夠完成一次良好的資料劃分，可以作為我們構建決策樹的根結點。當然，構建決策樹不可能只做一次劃分，我們可以在劃分後的資料集上繼續計算資訊增益比來選擇新的劃分特徵，重複這一過程直到滿足一定的條件就可以構造出一棵完整的決策樹模型。在經典決策樹演算法中，C4.5 演算法就是基於資訊增益比進行特徵選擇的。

除了上面講到的資訊增益和資訊增益比，**基尼指數**也是非常好的特徵選擇方法，它可以用於評價資料集的純度。基尼指數也叫**基尼不純度**（Gini impurity），它的取值在0到1之間，資料集純度越高，基尼指數越靠近0，資料集純度越低，基尼指數越靠近1。如果資料集有 $\small{n}$ 個類別，樣本屬於第 $\small{k}$ 個類別的機率為 $\small{p_{k}}$ ，那麼資料集的基尼指數可以透過下面的公式進行計算：

$$
G(D) = 1 - \sum_{k=1}^{n}{p_{k}}^{2}
$$

例如鳶尾花資料集中，三種鳶尾花的樣本數量都是50條，那麼整個資料集的基尼指數為：

$$
G(D) = 1 - [(\frac{1}{3})^{2} + (\frac{1}{3})^{2} + (\frac{1}{3})^{2}] = \frac{2}{3}
$$

如果三種鳶尾花的樣本數量分別為100條、25條、25條，那麼整個資料集的基尼指數為：

$$
G(D) = 1 - [(\frac{2}{3})^{2} + (\frac{1}{6})^{2} + (\frac{1}{6})^{2}] = \frac{1}{2}
$$

如果三種鳶尾花的樣本數量分別為140條、5條、5條，那麼整個資料集的基尼指數為：

$$
G(D) = 1 - [(\frac{14}{15})^{2} + (\frac{1}{30})^{2} + (\frac{1}{30})^{2}] = \frac{19}{150}
$$

可以看出，隨著資料集純度越來越高，基尼指數的值越來越小。如果資料集 $\small{D}$ 根據特徵 $\small{A}$ 劃分為 $\small{k}$ 個部分，那麼在給定特徵 $\small{A}$ 的前提條件下，資料集的基尼指數可以定義為：

$$
G(D, A) = \sum_{i=1}^{k}\frac{\lvert D_{i} \rvert}{\lvert D \rvert}G(D_{i})
$$

根據上面的公式，我們可以設計出如下所示的計算基尼指數的函式，大家可以對照上面的公式看看是否能夠理解下面的程式碼，或者透過呼叫下面的函式看看鳶尾花資料集的哪個特徵可以對原始資料集或者訓練集做出最好的劃分。經典決策樹演算法中的 CART 演算法就是基於基尼指數進行特徵選擇的。

```python
def gini_index(y):
    """
    計算基尼指數
    :param y: 資料集的目標值
    :return: 基尼指數
    """
    _, counts = np.unique(y, return_counts=True)
    return 1 - np.sum((counts / y.size) ** 2)


def gini_with_feature(x, y):
    """
    計算給定特徵後的基尼指數
    :param x: 給定的特徵
    :param y: 資料集的目標值
    :return: 給定特徵後的基尼指數
    """
    values, counts = np.unique(x, return_counts=True)
    gini = 0
    for value in values:
        prob = x[x == value].size / x.size
        gini += prob * gini_index(y[x == value]) 
    return gini


print(f'G(D)    = {gini_index(y_train)}')
print(f'G(D,A0) = {gini_with_feature(X_train[:, 0], y_train)}')
print(f'G(D,A1) = {gini_with_feature(X_train[:, 1], y_train)}')
print(f'G(D,A2) = {gini_with_feature(X_train[:, 2], y_train)}')
print(f'G(D,A3) = {gini_with_feature(X_train[:, 3], y_train)}')
```

輸出：

```
G(D,A0) = 0.29187830687830685
G(D,A1) = 0.44222582972582963
G(D,A2) = 0.06081349206349207
G(D,A3) = 0.06249999999999998
```

#### 資料分裂

構建決策樹的過程是一個遞迴的過程，透過上面介紹的方法選定特徵後就要進行資料分裂，簡單的說就是根據該特徵將資料集分成兩個或多個子集（兩個子集對應二叉樹，多個子集對應多叉樹），每個子集對應於特徵的不同取值。接下來，我們對每個子集重複特徵選擇和資料分裂的動作，直到滿足停止條件。停止條件對於遞迴過程非常重要，同時可以避免生成過於複雜的樹結構。常見的停止條件包括：

1. 樹達到預設的深度。
2. 當前結點的樣本數量少於預設的閾值。
3. 結點上所有樣本屬於同一個類別。
4. 資訊增益或Gini指數的變動低於某個閾值。

> **說明**：如果大家有編寫樹結構的經驗，上面的過程應該很容易理解。當然，對遞迴（recursion）、二叉樹（binary tree）、多叉樹（multi-way tree）不熟悉的小夥伴也可以隨便找本講資料結構的書來看看，相關的知識並不複雜，我們這裡就不進行贅述了。

在資料分裂的過程中，還有一個值得關注的問題就是特徵連續值和缺失值的處理。對於連續值，可以透過遍歷特徵所有可能的取值，找到切分點 $\small{x}$ 讓切分後的子集在資訊增益比或基尼指數方面達到最優，在資料分裂時以 $\small{x}$ 將資料劃分為 $\small{D_{1}}$ 和 $\small{D_{2}}$ 兩個子集，其中 $\small{D_{1}}$ 包含特徵值小於等於 $\small{x}$ 的樣本， $\small{D_{2}}$ 包含特徵值大於 $\small{x}$ 的樣本。對於缺失值，C4.5 演算法採用了加權分配的方式進行處理，當選擇一個特徵進行分裂時，該特徵存在缺失值的樣本會被分配到每個子集，但是不同的子集中該樣本被賦予的權重是不一樣的，這個權重會根據該特徵在各個類別中的比例進行計算。CART 演算法在處理缺失值時，通常會為每個特徵建立一個預設分支。對於存在缺失值的樣本，CART 演算法會將它們引導到預設分支進行處理，在計算基尼指數時，CART 演算法可以選擇是否將缺失值的樣本納入計算。

#### 樹的剪枝

對於一棵較為複雜的決策樹進行剪枝是很有必要的，剪枝可以減少樹結構的複雜性，同時也避免了過擬合的風險。常用的剪枝策略包括：

1. **後剪枝**（post-pruning）。後剪枝顧名思義是在決策樹構建完成後，透過評估和移除一些不必要的分支來簡化樹結構。後剪枝的過程通常從葉結點開始，至底向上評估每個結點，看看將其替換為葉結點（將該結點及其子樹剪掉）是否能提高模型的效能（在驗證集上的預測效果）。這種方式在減少過擬合風險的同時，還能夠較好的保留對資料的擬合能力，但是計算量較大，而且如果沒有合適的驗證集，剪枝效果就會受到影響。
2. **預剪枝**（pre-pruning）。預剪枝顧名思義是在構建決策樹的過程中動態決定是否停止分裂某個結點。透過在分裂前評估當前結點是否應該繼續分裂，可以避免生成過於複雜的樹，上面我們提到了幾種常用的停止條件。預剪枝策略在決策樹構建階段就減少了不必要的分裂，從而降低了模型的複雜性，但是可能存在欠擬合的風險，因為過早停止分裂可能會遺漏潛在的重要決策規則。

### 實現決策樹模型

我們可以使用 scikit-learn 中`tree`模組的`DecisionTreeClassifier`和`DecisionTreeRegressor`來實現用於分類和迴歸的決策樹，這裡我們重點討論用於解決分類問題的決策樹模型，有興趣的讀者可以自行研究用於解決迴歸問題的決策樹模型。

```python
from sklearn.tree import DecisionTreeClassifier

# 建立模型
model = DecisionTreeClassifier()
# 訓練模型
model.fit(X_train, y_train)
# 預測結果
y_pred = model.predict(X_test)
```

下面，我們對比一下預測值和真實值並看看模型的評估報告。

```python
from sklearn.metrics import classification_report

print(y_test)
print(y_pred)
print(classification_report(y_test, y_pred))
```

輸出：

```
[0 0 0 0 0 2 1 0 2 1 1 0 1 1 2 0 1 2 2 0 2 2 2 1 0 2 2 1 1 1]
[0 0 0 0 0 2 1 0 2 1 1 0 1 1 2 0 1 2 2 0 2 2 2 1 0 2 2 1 2 1]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        10
           1       1.00      0.90      0.95        10
           2       0.91      1.00      0.95        10

    accuracy                           0.97        30
   macro avg       0.97      0.97      0.97        30
weighted avg       0.97      0.97      0.97        30
```

> **說明**：大家執行上面的程式碼看到的輸出可能跟我這裡並不相同，因為在建立`DecisionTreeClassifier`時有一個控制隨機性的引數`random_state`，如果為該引數提供一個確定的整數值，就可以確保你的執行結果是可重複的。

我們可以透過下面的程式碼來視覺化決策樹，相信視覺化的方式會幫助大家對決策樹模型有一個更好的理解。

```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(12, 10))
plot_tree(
    decision_tree=model,               # 決策樹模型
    feature_names=iris.feature_names,  # 特徵的名稱
    class_names=iris.target_names,     # 標籤的名稱
    filled=True                        # 用顏色填充
)
plt.show()
```

輸出：

<img src="res/03_decision_tree_plot_1.png" style="zoom:45%;">

> **注意**：上圖中樹的根結點的基尼指數跟我們之前計算的資料集的基尼指數完全一致。

我們調整一下`DecisionTreeClassifier`的引數重新建立決策樹並對模型進行視覺化。

```python
# 建立模型
model = DecisionTreeClassifier(
    criterion='entropy',
    ccp_alpha=0.01,
    
)
# 訓練模型
model.fit(X_train, y_train)
# 視覺化
plt.figure(figsize=(12, 10))
plot_tree(
    decision_tree=model,               # 決策樹模型
    feature_names=iris.feature_names,  # 特徵的名稱
    class_names=iris.target_names,     # 標籤的名稱
    filled=True                        # 用顏色填充
)
plt.show()
```

輸出：

<img src="res/03_decision_tree_plot_2.png" style="zoom:38%;">

> **注意**：上圖中樹的根結點的資訊熵跟我們之前計算的資料集的資訊熵完全一致。

由此可見，建立`DecisionTreeClassifier`物件時引數不同，生成的決策樹也會存在差異。`DecisionTreeClassifier`建構函式的引數就是決策樹模型的超引數，其中有幾個非常重要的超引數：

1. `criterion`：特徵選擇（資料分裂質量評估）的標準，可以選擇`'gini'`或`'entropy'`，前者代表基尼指數，也是預設值，後者代表資訊增益。
2. `max_depth`：樹的最大深度，預設值為`None`，如果不設定該引數，會存在過擬合風險。
3. `min_samples_split`：一個內部結點再次分裂所需的最小樣本數，預設值為`2`。這個引數可以設定為整數表示最小樣本數，也可以設定為浮點數，表示佔總樣本數的比例。
4. `min_samples_leaf`：葉結點所需的最小樣本數，預設值為`1`。將該引數設定為較大的值可以平滑模型，降低過擬合風險。這個引數也可以設定為整數或浮點數，道理同上。
5. `max_features`：用於最佳分裂的特徵數，預設值為`None`。這個引數可以設定為整數，表示選擇固定數量的特徵；可以設定為浮點數，表示選擇特徵的比例；可以設定為字串，`'auto'`和`'sqrt'`表示將總的特徵數量求平方根，用平方根的值作為選擇特徵的數量，`'log2'`表示將總的特徵數量求對數，用對數值作為選擇特徵的數量。
6. `class_weight`：指定類別的權重，用於處理類別不平衡問題，預設值為`None`。可以用字典的方式手動設定每個類別的權重，也可以使用`'balanced'`讓模型自動調整。
7. `splitter`：選擇分裂結點的策略，預設值為`'best'`，表示最佳分裂，還有一個取值是`'random'`，表示隨機分裂。
8. `max_leaf_nodes`：限制葉結點的最大數量，可以防止樹結構過於複雜。
9. `min_impurity_decrease`：結點分裂所需的最小不純度降低值，任何結點只有在不純度減少超過此值時才會進行分裂。
10. `ccp_alpha`：成本複雜度剪枝中的$\small{\alpha}$引數值。這個引數用於控制後剪枝中成本複雜度計算公式中$\small{\alpha}$的值。較小的$\small{\alpha}$值允許更復雜的樹，而較大的$\small{\alpha}$值傾向於選擇更簡單的樹。透過調整$\small{\alpha}$，可以找到一個最佳的複雜度和誤差之間的平衡點。

我們可以透過之前講到的網格搜尋和交叉驗證的方式來對超引數進行調優，有興趣的讀者可以參考下面的程式碼。

```python
from sklearn.model_selection import GridSearchCV

gs = GridSearchCV(
    estimator=DecisionTreeClassifier(),
    param_grid={
        'criterion': ['gini', 'entropy'],
        'max_depth': np.arange(5, 10),
        'max_features': [None, 'sqrt', 'log2'],
        'min_samples_leaf': np.arange(1, 11),
        'max_leaf_nodes': np.arange(5, 15)
    },
    cv=5
)
gs.fit(X_train, y_train)
```

### 隨機森林概述

隨機森林是基於決策樹的整合學習演算法，所謂整合學習就是透過組合多個模型的預測結果來提高整體模型的效能，其核心思想就是多個模型的組合往往比單個模型更有效，透過不同的模型捕捉到資料的不同特徵，從而降低模型的過擬合風險（提升模型的泛化能力）。隨機森林透過構建多個決策樹並將它們的預測結果進行投票（分類）或平均（迴歸），來提高模型的準確性和魯棒性，如下圖所示。

<img src="res/03_random_forest.png" style="zoom:45%;">

隨機森林的基本工作流程如下：

1. Bootstrap 抽樣：從原始訓練資料集中隨機抽取若干個樣本（有放回抽樣），形成多個不同的子集，每個子集用於訓練一棵決策樹。
2. 構建決策樹：對於每棵樹在進行結點分裂時，不必考慮所有特徵，而是隨機選擇一部分特徵實現資料分裂。透過這種方式，隨機森林增加了模型的多樣性，減少了樹之間的相關性。
3. 整合學習：對於分類任務，隨機森林透過投票的方式（即多數表決）來決定最終分類的結果；對於迴歸任務，通常可以對每棵樹的預測結果求均值作為最終的預測結果。

隨機森林的主要優點體現在：

1. 透過整合多個決策樹，能夠有效提高模型的準確性。
2. 由於引入了隨機性，隨機森林通常比單棵決策樹更不容易過擬合。
3. 隨機森林可以提供特徵的重要性評分，幫助理解模型。
4. 能夠處理大規模資料集和高維資料。

當然，因為要構建多個決策樹，隨機森林模型通常比較複雜，在訓練模型時對計算資源、記憶體資源和時間成本的消耗都是更大的。我們可以透過 scikit-learn 庫`ensemble`模組的`RandomForestClassifier`類來構造隨機森林模型，這裡我們仍然使用網格搜尋交叉驗證的方式來對超引數進行調優，程式碼如下所示。

```python
from sklearn.ensemble import RandomForestClassifier

gs = GridSearchCV(
    estimator=RandomForestClassifier(n_jobs=-1),
    param_grid={
        'n_estimators': [50, 100, 150],
        'criterion': ['gini', 'entropy'],
        'max_depth': np.arange(5, 10),
        'max_features': ['sqrt', 'log2'],
        'min_samples_leaf': np.arange(1, 11),
        'max_leaf_nodes': np.arange(5, 15)
    },
    cv=5
)
gs.fit(X_train, y_train)
```

> **提示**：上面的程式碼可能會執行非常長的時間。

隨機森林模型的超引數很多跟決策樹類似，需要說明的有以下幾個超引數：

1. `estimator`：森林中樹的數量，簡單的說就是用多少個決策樹來構造森林。更多的樹通常會帶來更好的預測效果，但也會增加計算成本。
2. `boostrap`：是否使用 Bootstrap 抽樣，預設值為`True`，表示在構建每棵樹時使用有放回抽樣。
3. `n_jobs`：用於並行執行的任務數，預設值為`None`，表示使用單個處理器核心；設定為`-1`，則可以使用所有可用的處理器核心。

### 總結

決策樹是簡單有效且易於理解的預測模型，適用於分類和迴歸任務，但容易發生過擬合且對噪聲資料敏感；隨機森林透過整合多個決策樹提高了泛化能力且對噪聲資料不敏感，適合解決複雜問題。在解決實際問題時，可以根據具體場景選擇合適的模型，透過調節相關超引數來獲得最佳效果，二者的對比如下表所示：

| **屬性**         | **決策樹**           | **隨機森林**       |
| ---------------- | -------------------- | ------------------ |
| **模型複雜度**   | 簡單                 | 較複雜             |
| **抗過擬合能力** | 差                   | 強                 |
| **計算效率**     | 高                   | 較低               |
| **結果穩定性**   | 易受單一資料變化影響 | 穩定               |
| **適用場景**     | 資料較少、簡單問題   | 資料較多、複雜問題 |
