## 機器學習實戰

本章我們透過經典的“泰坦尼克號生存預測”專案為大家講解機器學習的完整應用，該專案來自於全球知名的資料科學競賽平臺[Kaggle](https://www.kaggle.com/)。該平臺是資料科學、機器學習、人工智慧領域的重要社群之一，為資料科學家和演算法工程師提供了一個實踐、分享和競爭的空間。無論是新手還是經驗豐富的專家，Kaggle 都能為其提供豐富的資源和挑戰，透過平臺提供的資料集、競賽、課程等資源，使用者可以根據自身的需求提升相關的資料科學技能並與全球的資料科學家互動。

泰坦尼克號生存預測專案是 Kaggle 上最著名的入門級機器學習專案之一。該專案要求你根據乘客的基本資訊（如年齡、性別、艙位、登船地等）預測他們是否能夠在泰坦尼克號沉船事件中倖存下來，屬於典型的分類任務。我們可以在 Kaggle 網站的 Competitions 搜尋到名為“[*Titanic - Machine Learning from Disaster*](https://www.kaggle.com/competitions/titanic/)”的專案，點選進入可以看到專案的背景介紹並找到下載資料檔案`train.csv`和`test.csv`的連結，顯然前者是訓練集（包含特徵和標籤）而後者是測試集（只有特徵）。我們可以下載資料檔案到本地並訓練模型，然後再提交自己的預測結果；當然，如果願意也可以線上建立 Notebook 檔案並編寫和執行程式碼。

### 資料探索

我們首先載入訓練模型的資料，假設資料檔案儲存在當面路徑下名為`data`的目錄中，程式碼如下所示。

```python
import numpy as np
import pandas as pd

df = pd.read_csv('data/train.csv', index_col='PassengerId')
df.head(5)
```

輸出：

```
			Survived Pclass	Name			Sex		Age		SibSp	Parch	Ticket		Fare	Cabin	Embarked
PassengerId											
1			0		 3		Braund, ...	    male	22.0	1		0		A/5 ...		7.2500	NaN		S
2			1		 1		Cumings, ...	female	38.0	1		0		PC ...		71.2833	C85		C
3			1		 3		Heikkinen, ...  female	26.0	0		0		STON/O2 ...	7.9250	NaN		S
4			1		 1		Futrelle, ...   female	35.0	1		0		113803 ...	53.1000	C123	S
5			0		 3		Allen, ...	    male	35.0	0		0		373450 ...	8.0500	NaN		S
```

> **注意**：載入資料的時候，我們直接將 PassengerID （乘客編號）列處理成了行索引。

下面的資料字典對`DataFrame`每個列的含義進行了說明。

| 列名       | 含義             | 說明                                                |
| ---------- | ---------------- | --------------------------------------------------- |
| `Survived` | 是否倖存         | 目標類別（`1`表示倖存，`0`表示遇難）                |
| `Pclass`   | 艙位等級         | 整數，取值為`1`、`2`、`3`，表示三種不同的等級       |
| `Name`     | 乘客姓名         | 字串                                              |
| `Sex`      | 性別             | 字串，`male`為男性，`female`為女性                |
| `Age`      | 年齡             | 浮點數                                              |
| `SibSp`    | 平輩親屬登船人數 | 整數，包括兄弟、姐妹、配偶總共登船的人數            |
| `Parch`    | 父母小孩登船人數 | 整數                                                |
| `Ticket`   | 船票號           | 字串                                              |
| `Fare`     | 船票價格         | 浮點數                                              |
| `Cabin`    | 客艙號           | 字串                                              |
| `Embarked` | 登船港口         | 字串，`C`表示瑟堡，`Q`表示皇后鎮、`S`表示南安普頓 |

我們對資料進行視覺化操，透過統計圖表實訓對資料的初步探索，程式碼如下所示。

```python
import matplotlib.pyplot as plt

# 修改配置新增中文字型
plt.rcParams['font.sans-serif'].insert(0, 'SimHei')
plt.rcParams['axes.unicode_minus'] = False

# 定製畫布
plt.figure(figsize=(16, 12), dpi=200)

# 遇難和獲救人數分佈
plt.subplot(3, 4, 1)
ser = df.Survived.value_counts()
ser.plot(kind='bar', color=['#BE3144', '#3A7D44'])
plt.xticks(rotation=0)
plt.title('圖1.獲救情況分佈')
plt.ylabel('人數')
plt.xlabel('')
for i, v in enumerate(ser):
    plt.text(i, v, v, ha='center')

# 客艙等級人數分佈
plt.subplot(3, 4, 2)
ser = df.Pclass.value_counts().sort_index()
ser.plot(kind='bar', color=['#FA4032', '#FA812F', '#FAB12F'])
plt.xticks(rotation=0)
plt.ylabel('人數')
plt.xlabel('')
plt.title('圖2.客艙等級分佈')
for i, v in enumerate(ser):
    plt.text(i, v, v, ha='center')

# 性別人數分佈
plt.subplot(3, 4, 3)
ser = df.Sex.value_counts()
ser.plot(kind='bar', color=['#16404D', '#D84040'])
plt.xticks(rotation=0)
plt.ylabel('人數')
plt.xlabel('')
plt.title('圖3.性別分佈')
for i, v in enumerate(ser):
    plt.text(i, v, v, ha='center')

# 登船港口人數分佈
plt.subplot(3, 4, 4)
ser = df.Embarked.value_counts()
ser.plot(kind='bar', color=['#FA4032', '#FA812F', '#FAB12F'])
plt.xticks(rotation=0)
plt.ylabel('人數')
plt.xlabel('')
plt.title('圖4.登船港口分佈')
for i, v in enumerate(ser):
    plt.text(i, v, v, ha='center')

# 乘客年齡箱線圖
plt.subplot(3, 4, 5)
df.Age.plot(kind='box', showmeans=True, notch=True)
plt.title('圖5.乘客年齡情況')

# 船票價格箱線圖
plt.subplot(3, 4, 6)
df.Fare.plot(kind='box', showmeans=True, notch=True)
plt.title('圖6.船票價格情況')

# 不同客艙等級遇難和倖存人數分佈
plt.subplot(3, 4, (7, 8))
s0 = df[df.Survived == 0].Pclass.value_counts()
s1 = df[df.Survived == 1].Pclass.value_counts()
temp = pd.DataFrame({'遇難': s0, '倖存': s1})
pcts = temp.div(temp.sum(axis=1), axis=0)
temp.plot(ax=plt.gca(), kind='bar', stacked=True, color=['#BE3144', '#3A7D44'])
for i, idx in enumerate(temp.index):
    plt.text(i, temp.at[idx, '遇難'] // 2, f'{pcts.at[idx, "遇難"]:.2%}', ha='center', va='center')
    plt.text(i, temp.at[idx, '遇難'] + temp.at[idx, '倖存'] // 2, f'{pcts.at[idx, "倖存"]:.2%}', ha='center', va='center')
plt.xticks(rotation=0)
plt.xlabel('')
plt.title('圖7.不同客艙等級倖存情況')

# 不同性別遇難和倖存人數分佈
plt.subplot(3, 4, (9, 10))
s0 = df[df.Survived == 0].Sex.value_counts()
s1 = df[df.Survived == 1].Sex.value_counts()
temp = pd.DataFrame({'遇難': s0, '倖存': s1})
pcts = temp.div(temp.sum(axis=1), axis=0)
temp.plot(ax=plt.gca(), kind='bar', stacked=True, color=['#BE3144', '#3A7D44'])
for i, idx in enumerate(temp.index):
    plt.text(i, temp.at[idx, '遇難'] // 2, f'{pcts.at[idx, "遇難"]:.2%}', ha='center', va='center')
    plt.text(i, temp.at[idx, '遇難'] + temp.at[idx, '倖存'] // 2, f'{pcts.at[idx, "倖存"]:.2%}', ha='center', va='center')
plt.xticks(rotation=0)
plt.xlabel('')
plt.title('圖8.不同性別倖存情況')

# 不同登船港口遇難和倖存人數分佈
plt.subplot(3, 4, (11, 12))
s0 = df[df.Survived == 0].Embarked.value_counts()
s1 = df[df.Survived == 1].Embarked.value_counts()
temp = pd.DataFrame({'遇難': s0, '倖存': s1})
pcts = temp.div(temp.sum(axis=1), axis=0)
temp.plot(ax=plt.gca(), kind='bar', stacked=True, color=['#BE3144', '#3A7D44'])
for i, idx in enumerate(temp.index):
    plt.text(i, temp.at[idx, '遇難'] // 2, f'{pcts.at[idx, "遇難"]:.2%}', ha='center', va='center')
    plt.text(i, temp.at[idx, '遇難'] + temp.at[idx, '倖存'] // 2, f'{pcts.at[idx, "倖存"]:.2%}', ha='center', va='center')
plt.xticks(rotation=0)
plt.xlabel('')
plt.title('圖9.不同登船港口倖存情況')

plt.show()
```

輸出：

<img src="res/10_dataset_visualization.png" style="zoom:85%;">

從上面的統計圖表可以看出，一等艙的乘客有更高的存活率，女性乘客相較於男性乘客也有更高的存活率，在瑟堡登船的使用者存活率較其他兩地更高。如果願意，我們還可以嘗試對性別、艙位、年齡等維度進行交叉組合並繪製出對應的統計圖表，甚至還可以探索乘客姓名中的稱謂（`Mr`、`Miss`、`Mrs`、`Dr`、`Master`、`Major`等）跟乘客能否倖存是否存在某種關係，有興趣的讀者可以自行研究。

我們進一步檢視`DataFrame`物件相關資訊。

```python
df.info()
```

輸出：

```
<class 'pandas.core.frame.DataFrame'>
Index: 891 entries, 1 to 891
Data columns (total 11 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   Survived  891 non-null    int64  
 1   Pclass    891 non-null    int64  
 2   Name      891 non-null    object 
 3   Sex       891 non-null    object 
 4   Age       714 non-null    float64
 5   SibSp     891 non-null    int64  
 6   Parch     891 non-null    int64  
 7   Ticket    891 non-null    object 
 8   Fare      891 non-null    float64
 9   Cabin     204 non-null    object 
 10  Embarked  889 non-null    object 
dtypes: float64(2), int64(4), object(5)
memory usage: 83.5+ KB
```

可以看出，訓練集一共`891`條資料，其中年齡、客艙號、登船港口等列存在缺失值，乘客姓名、登船港口等是字串型別沒有辦法直接用於模型訓練，所以我們需要做一些準備工作。

### 特徵工程

特徵工程（Feature Engineering）是機器學習中的一個重要環節，透過對原始資料的處理、轉換與選擇，提取出能夠幫助模型更好理解資料的特徵，最終提升模型的預測效能。特徵工程的主要步驟包括：

1. **資料清洗**：處理資料中的缺失值、重複值、異常值等。

2. **特徵轉換**：將原始資料轉化為更適合模型輸入的形式。常見的特徵轉換方法有：

    - 標準化和歸一化：標準化將資料轉換為均值為`0`、標準差為`1`的分佈；歸一化將資料縮放到特定的範圍（如：$\small{[0, 1]}$），確保各特徵在同一數量級，對應的公式如下所示：
        $$
        X^{\prime} = \frac{X - \mu}{\sigma}
        $$

        $$
        X^{\prime} = \frac{X - X_{min}}{X_{max} - X_{min}}
        $$

        其中，$\small{\mu}$表示資料的均值，$\small{\sigma}$表示資料的標準差，$\small{X_{min}}$表示資料中的最小值，$\small{X_{max}}$表示資料中的最大值。Scikit-learn 庫`preprocessing`模組中的`StandardScaler`和`MinMaxScaler`可以實現資料的標準化和歸一化。

    - 類別編碼：對於類別變數（如：性別、職業、地區等），常用的編碼方式有獨熱編碼（One-Hot Encoding）和標籤編碼（Label Encoding）。Scikit-learn 庫`preprocessing`模組中的`OneHotEncoder`和`LabelEncoder`可以實現資料的獨熱編碼和標籤編碼，當然也可以透過 pandas 庫的`get_dummies`函式來處理`DataFrame`中的類別變數。

    - 對數變換：對數變換可以幫助處理具有偏態分佈的特徵，使其更接近正態分佈。

3. **特徵選擇**：從大量的特徵中篩選出對模型預測有用的特徵，去除冗餘或無關的特徵，減少維度並提高模型的效能。常用的方法有：

    - **過濾法**：基於統計檢驗方法（如：卡方檢驗、皮爾遜相關係數等）選擇最具區分度的特徵。
    - **包裹法**：透過模型的效能來評估特徵子集的效果（如：遞迴特徵消除法）。
    - **嵌入法**：透過訓練模型並從模型中提取特徵的重要性（如：決策樹、L1正則化等），從而進行特徵選擇。

4. **特徵降維**：透過某種數學變換將資料從高維空間對映到低維空間，以便保留資料的主要資訊，簡化模型，降維後的新特徵是原始特徵的某種組合。降維可以減少模型訓練的運算量，減少不必要的噪聲和冗餘特徵來降低複雜性，從而提高模型的泛化能力。特徵降維常用的方法有：

    - **PCA**（主成分分析）：PCA 的目標是找出資料中最重要的特徵（主成分），這些主成分能夠保留資料的大部分方差。PCA 透過線性變換將資料投影到新的座標軸上，使得投影后的資料方差最大化。換句話說，PCA透過對資料的特徵進行“旋轉”，將其轉換為一個新的座標系，新座標系的每個軸（即主成分）按資料的方差大小排列，前幾個主成分通常就能夠解釋資料的大部分變異。Scikit-learn 庫`decomposition`模組的`PCA`類可以幫助我們完成主成分分析。
    - **LDA**（線性判別分析）：LDA 的主要目標是透過尋找最佳的投影方向來最大化不同類別之間的可分性，並同時最小化同一類別內資料點的散佈。LDA在特徵空間中找到一個低維的空間，使得類別之間的差異儘可能大，而同一類別內的資料儘量聚集。簡單的說，LDA 的核心思想是**最大化類間散度，最小化類內散度**。Scikit-learn 庫`discriminant_analysis`模組的`LinearDiscriminantAnalysis`類可以幫助我們完成線性判別分析。
    - **t-SNE**（t分佈隨機鄰居嵌入）：t-SNE 是一種非線性的降維方法，主要用於資料視覺化，特別適用於高維資料（如影象、文字等）的降維。t-SNE的目標是將高維空間中的相似點（鄰近點）對映到低維空間中的相近位置，同時儘量避免不同類別之間的重疊。Scikit-learn 庫`manifold`模組的`TSNE`類可以幫助我們完成t分佈隨機鄰居嵌入。

5. **特徵構造**：透過原始特徵組合、變換或派生出新的特徵。

特徵工程不僅是提高模型準確性的關鍵因素，也是機器學習中一個需要持續最佳化的過程。透過對資料進行精心的清洗、轉換、選擇和構造，可以為模型提供更有價值的資訊，提高模型的表現。儘管特徵工程看似複雜，但它的效果在很多情況下超出了模型本身的改進，因此在實際應用中，我們應投入足夠的精力和時間來進行特徵工程的最佳化。

對於我們載入的資料集，我們可以將年齡欄位的缺失值處理為中位數，將登船港口的缺失值處理為眾數，客艙號欄位的缺失值比較多，一種處理方式是直接刪除這個列，另一種處理方式是二值化，有客艙號的記為`1`，沒有客艙號的記為`0`，程式碼如下所示。

```python
df['Age'] = df.Age.fillna(df.Age.median())
df['Embarked'] = df.Embarked.fillna(df.Embarked.mode()[0])
df['Cabin'] = df.Cabin.replace(r'.+', '1', regex=True).replace(np.nan, 0).astype('i8')
```

處理完缺失值後，我們對年齡和船票價格兩個欄位進行特徵縮放，透過`StandardScaler`實現標準化，程式碼如下所示。

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df[['Fare', 'Age']] = scaler.fit_transform(df[['Fare', 'Age']])
```

接下來，還需要對性別和登船港口兩個欄位進行獨熱編碼處理，可以使用 pandas 庫中的`get_dummies`函式或 scikit-learn 庫的`OneHotEncoder`處理，兩者處理的結果類似，程式碼如下所示。

```python
df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)
```

我們繼續處理乘客姓名欄位，根據姓名中的稱謂衍生出一個新的特徵；此外，對於`SibSp`和`Parch`兩個欄位，我們可以將其衍生為家庭成員數量的新特徵，程式碼如下所示。

```python
title_mapping = {
    'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Dr': 4, 'Rev': 5, 'Col': 6, 'Major': 7, 
    'Mlle': 8, 'Ms': 9, 'Lady': 10, 'Sir': 11, 'Jonkheer': 12, 'Don': 13, 'Dona': 14, 'Countess': 15
}
df['Title'] = df['Name'].map(
    lambda x: x.split(',')[1].split('.')[0].strip()
).map(title_mapping).fillna(-1)
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
```

> **說明**：我們將每個稱謂對映到一個數字，這裡不必過於糾結稱謂跟數字的對應關係，而且有些稱謂在我們的訓練集中並沒有出現。對於那些未知的稱謂，我們統一處理為`-1`。

完成上面的操作之後，我們可以把不必要的欄位全部刪除掉，資料的準備工作就基本完成了。

```python
df.drop(columns=['Name', 'SibSp', 'Parch', 'Ticket'], inplace=True)
```

### 模型訓練

首先我們將資料集劃分為訓練集和驗證集，注意這裡不是劃分訓練集和測試集，因為我們最終的測試集是官方提供的`test.csv`檔案，這裡我們把`train.csv`中 90% 的資料劃分出來訓練模型，用剩下的 10% 對模型進行驗證。下面的程式碼中我們將驗證集的特徵和標籤分別命名為`X_valid`和`y_valid`。

```python
from sklearn.model_selection import train_test_split

X, y = df.drop(columns='Survived'), df.Survived
X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.9, random_state=3)
```

我們先嚐試邏輯迴歸模型，程式碼如下所示。

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

model = LogisticRegression(penalty='l1', tol=1e-6, solver='liblinear')
model.fit(X_train, y_train)
y_pred = model.predict(X_valid)
print(classification_report(y_valid, y_pred))
```

輸出：

```
              precision    recall  f1-score   support

           0       0.88      0.80      0.84        56
           1       0.72      0.82      0.77        34

    accuracy                           0.81        90
   macro avg       0.80      0.81      0.80        90
weighted avg       0.82      0.81      0.81        90
```

我們再試一試大殺器 XGBoost，程式碼如下所示。

```python
import xgboost as xgb

# 將資料處理成資料集格式DMatrix格式
dm_train = xgb.DMatrix(X_train, y_train)
dm_valid = xgb.DMatrix(X_valid)

# 設定模型引數
params = {
    'booster': 'gbtree',             # 用於訓練的基學習器型別
    'objective': 'binary:logistic',  # 指定模型的損失函式
    'gamma': 0.1,                    # 控制每次分裂的最小損失函式減少量
    'max_depth': 10,                 # 決策樹最大深度
    'lambda': 0.5,                   # L2正則化權重
    'subsample': 0.8,                # 控制每棵樹訓練時隨機選取的樣本比例
    'colsample_bytree': 0.8,         # 用於控制每棵樹或每個節點的特徵選擇比例
    'eta': 0.05,                     # 學習率
    'seed': 3,                       # 設定隨機數生成器的種子
    'nthread': 16,                   # 指定了訓練時並行使用的執行緒數
}

model = xgb.train(params, dm_train, num_boost_round=200)
y_pred = model.predict(dm_valid)
# 將預測的機率轉換為類別標籤
y_pred_label = (y_pred > 0.5).astype('i8')
print(classification_report(y_valid, y_pred_label))
```

輸出：

```
              precision    recall  f1-score   support

           0       0.89      0.91      0.90        56
           1       0.85      0.82      0.84        34

    accuracy                           0.88        90
   macro avg       0.87      0.87      0.87        90
weighted avg       0.88      0.88      0.88        90
```

當然，在條件允許的情況下，**強烈建議**大家透過網格搜尋交叉驗證的方式對模型的超引數進行調優，同時我們還可以繪製**學習曲線**（learning curve）來輔助判定是否存在過擬合和欠擬合問題。學習曲線包含兩條曲線，一條反映訓練誤差，一條反映驗證誤差。欠擬合通常表現為模型在訓練集和驗證集上的誤差都較大，並且隨著訓練資料增加，訓練誤差和驗證誤差的差距較小，說明我們的模型過於簡單，無法捕捉到資料中的模式。過擬合通常表現為模型在訓練集上表現得很好（訓練誤差較低），但在驗證集上表現較差（驗證誤差較高），且隨著訓練資料增加，訓練誤差持續降低而驗證誤差開始上升時，說明我們的模型過於複雜。Scikit-learn 庫`model_selection`模組的`learning_curve`函式可以幫助我們繪製學習曲線，有興趣的讀者可以自行研究。

### 模型評估

接下來我們載入真正的測試資料`test.csv`，透過前面訓練好的模型來做出預測。我們可以將預測的結果儲存成一個 CSV 檔案，該檔案共有兩列，一列是 PassengerID，一列是我們預測的結果。我們將該檔案提交到 Kaggle 平臺，可以獲得最終模型的準確率評分。

```python
test = pd.read_csv('data/test.csv', index_col='PassengerId')
# 處理缺失值
test['Age'] = test.Age.fillna(test.Age.median())
test['Fare'] = test.Fare.fillna(test.Fare.median())
test['Embarked'] = test.Embarked.fillna(test.Embarked.mode()[0])
test['Cabin'] = test.Cabin.replace(r'.+', '1', regex=True).replace(np.nan, 0).astype('i8')
# 特徵縮放
test[['Fare', 'Age']] = scaler.fit_transform(test[['Fare', 'Age']])
# 處理類別
test = pd.get_dummies(test, columns=['Sex', 'Embarked'], drop_first=True)
# 特徵構造
test['Title'] = test['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip()).map(title_mapping).fillna(-1)
test['FamilySize'] = test['SibSp'] + test['Parch'] + 1
# 刪除多餘特徵
test.drop(columns=['Name', 'Ticket', 'SibSp', 'Parch'], inplace=True)

# 使用邏輯迴歸模型
passenger_id, X_test = test.index, test
# 使用XGBoost模型
# passenger_id, X_test = test.index, xgb.DMatrix(test)

y_test_pred = model.predict(X_test)
# XGoost模型 - 將預測的機率轉換成類別標籤
# y_test_pred = (model.predict(X_test) > 0.5).astype('i8')

# 生成提交檔案
result = pd.DataFrame({
    'PassengerId': passenger_id,
    'Survived': y_test_pred
})
result.to_csv('submission.csv', index=False)
```

大家可以試一試，不同的模型預測結果是否存在較大的差異，哪個是你心目中最理想的模型，然後大家可以透過實驗驗證我們之前提到的一個觀點，看看特徵工程是不是對預測的結果有重要的影響。下面是我對特徵工程的一些小建議（僅供參考），大家可以自行實踐，看看能否透過這些操作提升模型的效能。

1. 對年齡（Age）欄位進行離散化（分箱），將年齡特別小的（兒童）單獨處理（如增加一個布林欄位）。
2. 對客艙號（Cabin）欄位進行更細緻的處理，提取字首字母和後續數字作為特徵，而不是簡單的二值化。
3. 對比較重要的兩個特徵艙位等級（Pclass）和性別（Sex）進行特徵組合。
4. 對名字（Name）中有 Mrs 且父母小孩登船人數大於1的（可能是一位母親）單獨處理（如增加一個布林欄位）。
5. 刪除掉登船港口（Embarked）欄位。

### 模型部署

我們可以將訓練好的模型部署到工程化專案中，首先需要對模型進行序列化處理，程式碼如下所示。

```python
import joblib

joblib.dump(model, 'model.pkl')
```

> **說明**：此處我們預設上面程式碼中的`model`物件是之前透過 XGBoost 訓練的模型。 

`joblib`模組的`dump`函式對模型進行了 Pickle 序列化（Python私有序列化協議），在需要使用模型的地方，我們可以透過`load`函式實現反序列化，這樣就可以將模型載入到應用程式中並完成預測，程式碼如下所示。

```python
import joblib

model = joblib.load('model.pkl')
model.predict(X_test)
```

> **說明**：此處我們預設上面程式碼中的`X_test`已經處理為 XGBoost 中`DMatrix`物件。

如果我們希望將模型部署成 Web 服務，可以利用 Flask、FastAPI 這樣的 Web 框架來建立 API 介面。下面，我們以 Flask 框架為例，用簡單的程式碼為大家演示這個過程。

```python
from flask import Flask
from flask import jsonify
from flask import request

import joblib
import pandas as pd
import xgboost as xgb

app = Flask(__name__)


@app.route('/predict', methods=['POST'])
def predict():
    query_df = pd.DataFrame(request.json)
    model = joblib.load('model.pkl')
    y_pred = (model.predict(xgb.DMatrix(query_df)) > 0.5).tolist()
    return jsonify({'message': 'OK', 'result': y_pred})


if __name__ == '__main__':
    app.run(debug=True)
```

> **說明**：作為工程化的專案，肯定不能在請求到來時才載入模型，因為這樣會嚴重影響 Web 服務的效能。可以在專案啟動之後的適當時機提前載入需要的模型，與此同時還要根據實際需求做好模型資源佔用的監控。

執行上面的程式碼，預設會在本機 5000 埠執行一個 Web 伺服器，下面我們透過 API 測試工具模擬傳送一個 HTTP 請求給伺服器，看看我們的模型能否運轉起來給出預測的結果。

<img src="res/10_test_web_api.png" style="zoom:40%;">

除了把模型部署成 Web 服務之外，我們還可以將模型部署到定時任務中，透過時間自動激發對模型的呼叫。此外，一些邊緣裝置支援我們將訓練好的模型以某種特殊的格式嵌入進去，有興趣的讀者可以自行探索。
