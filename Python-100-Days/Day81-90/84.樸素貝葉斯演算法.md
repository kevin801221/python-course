## 樸素貝葉斯演算法

我們繼續為大家介紹解決分類任務的演算法，本章介紹一種機率模型貝葉斯分類器。貝葉斯分類器是一類分類演算法的總稱，這類演算法均以貝葉斯定理為基礎，因而統稱為貝葉斯分類器。在介紹貝葉斯定理之前，我們先講一個故事：從 2015 年到 2020 年期間，某位李姓女士憑藉自己對航班是否會延誤的分析，購買了大約 900 次飛機延誤險並獲得延誤賠償，累計獲得理賠金高達 300 多萬元，真可謂“航班延誤，發家致富”。當然，這套騷操作本身不是我們探討的重點，我們的問題是：李女士是怎麼決定要不要購買延誤險的呢？航班延誤最主要的原因就是天氣（包括起飛地和降落地的天氣）、機場（起飛機場和降落機場）和航司，由於李女士有過航空服務類工作的經歷，有獲得機場和航司相關資料的途徑（天氣資料相對更容易獲取），集齊相關的資料再利用貝葉斯定理，她可以能夠計算出當前航班延誤的機率並決定是否購買延誤險。接下來，李女士透過虛構不同身份購票並大量投保（每個身份購買 30 到 40 份延誤險），這樣一旦航班延誤，她就可以向保險公司進行索賠。那麼，我們要探討的就是貝葉斯定理是如何利用現有資料計算出航班延誤的機率。

### 貝葉斯定理

貝葉斯定理是機率論中的一個重要定理，它描述瞭如何從主觀經驗或已知事實出發，透過收集到的樣本資料（證據）來更新對事件發生機率的認知（信念）。貝葉斯定理的數學表示式為：

$$
P(A \vert B) = \frac{P(B \vert A)}{P(B)} \cdot P(A)
$$

其中， $\small{P(A)}$ 是事件 $\small{A}$ 發生的**先驗機率**，我們可以理解為已知事實或主觀經驗（**主觀機率**）； $\small{P(B \vert A)}$ 是在事件 $\small{A}$ 發生的條件下事件 $\small{B}$ 發生的 條件機率，通常也稱之為**似然性**（likelihood）， $\small{P(B)}$ 是事件 $\small{B}$ 發生的（全）機率，這兩個機率可以透過我們收集到的樣本資料（證據）獲得； $\small{P(A \vert B)}$ 是在事件 $\small{B}$ 發生的條件下事件 $\small{A}$ 發生的條件機率，即收集到樣本資料後對事件 $\small{A}$ 發生機率的重新認知，稱之為**後驗機率**。貝葉斯定理告訴我們一個重要的事實：可以從已知的事實或主觀經驗出發，透過收集到的證據來更新我們對某個事件發生機率的認知，簡單的說就是**可以透過已知的事實和收集的證據來推斷出未知的真相**。

回到上面李女士購買飛機延誤險的例子，假設本次航班是從成都雙流國際機場飛往北京首都國際機場，執飛的航空公司是四川航空，起飛地天氣為雨天（小雨），溫度為8°C，東北風2級，降落地天氣為晴天，溫度4°C，西北風2級。為了更簡單的讓大家理解貝葉斯定理，我們對這裡的條件稍作簡化，只保留天氣中的降水資訊，暫不考慮溫度和風速等其他因素，對應到上面的貝葉斯定理有：

$$
P(延誤 \vert 起飛機場=雙流,到達機場=首都,起飛天氣=小雨,降落天氣=晴天,執飛航司=川航) = \\\\
\frac{P(起飛機場=雙流,到達機場=首都,起飛天氣=小雨,降落天氣=晴天,執飛航司=川航 \vert 延誤)}{P(起飛機場=雙流,到達機場=首都,起飛天氣=小雨,降落天氣=晴天,執飛航司=川航)} \cdot P(延誤)
$$

上面公式等號左邊就是李女士想知道的當前航班延誤的機率，等號右邊的部分其實就是歷史資料和當前資訊，計算這個機率的關鍵在於計算出似然性，即 $\small{P(起飛機場=雙流,到達機場=首都,起飛天氣=小雨,降落天氣=晴天,執飛航司=川航 \vert 延誤)}$ 到底是多少，那麼這個條件機率又該如何計算呢？

### 樸素貝葉斯

樸素貝葉斯演算法是基於貝葉斯定理和特徵條件獨立性假設的分類演算法，因其簡單高效而受到廣泛應用。樸素貝葉斯演算法的關鍵在於“樸素”二字，就是剛才我們說到特徵條件獨立性假設，條件獨立性假設是說用於分類的特徵在類確定的條件下都是獨立的，該假設使得樸素貝葉斯的學習成為可能。

假設我們有一個特徵集合 $\small{X = \{x_1, x_2, \ldots, x_n\}}$ 和一個類別 $\small{C}$ ，樸素貝葉斯演算法假設：

$$
P(X \vert C) = P(x_1 \vert C) \cdot P(x_2 \vert C) \cdot \ldots \cdot P(x_n \vert C)
$$

這個假設大大簡化了計算複雜性，使得我們可以只計算每個特徵在給定類別下的機率，而不需要考慮特徵之間的相互作用，對應到上面購買飛機延誤險的例子，我們可以用下面的方式來計算似然性：

$$
P(起飛機場=雙流,到達機場=首都,起飛天氣=小雨,降落天氣=晴天,執飛航司=川航 \vert 延誤) = \\\\
P(起飛機場=雙流 \vert 延誤) \times P(到達機場=首都 \vert 延誤) \times P(起飛天氣=小雨 \vert 延誤) \times P(降落天氣=晴天 \vert 延誤) \times P(執飛航司=川航 \vert 延誤)
$$

### 演算法原理

#### 訓練階段

在訓練階段，樸素貝葉斯演算法首先需要計算每個類別的先驗機率和每個特徵在各個類別下的條件機率。

1. **計算先驗機率**：

$$
P(C_{i}) = \frac{n_{i}}{n}
$$

其中， $\small{C_{i}}$ 表示類別， $\small{n_{i}}$ 是類別 $\small{C_{i}}$ 的樣本數量， $\small{n}$ 是總的樣本容量。

2. **計算條件機率**：

$$
P(x_{j} \vert C_{i}) = \frac{n_{i,j}}{n_{i}}
$$

其中， $\small{n_{i,j}}$ 是在類別 $\small{C_{i}}$ 中，特徵 $\small{x_{j}}$ 出現的次數。

#### 預測階段

在預測階段，給定一個待分類樣本 $\small{X}$ ，樸素貝葉斯演算法透過以下步驟來計算其屬於每個類別的後驗機率：

$$
P(C_{i} \vert X) = \frac{P(X \vert C_{i})}{P(X)} \cdot P(C_{i})
$$

上面的公式中， $\small{P(X)}$ 對應到每個類別都是一個常量，可以忽略掉它，再結合獨立性假設有：

$$
P(C_{i} \vert X) \propto P(C_{i}) \cdot P(x_1 \vert C_{i}) \cdot P(x_2 \vert C_{i}) \cdot \ldots \cdot P(x_n \vert C_{i})
$$

這樣，我們可以選擇具有最高後驗機率的類別作為預測結果。

#### 程式碼實現

我們還是以鳶尾花資料集為例，按照上面的講解的原理，用 NumPy 來實現一個樸素貝葉斯分類器，我們還是從載入資料開始，程式碼如下所示。

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=3)
```

訓練階段我們要獲得類別標籤和對應的先驗機率，此外還要計算出似然性，似然性的計算用到了上面提到的“樸素”假設，我們對鳶尾花連續的特徵值進行了簡單的離散化處理（大家先不考慮這種處理方式是否合理），程式碼如下所示。

```python
import numpy as np
import pandas as pd


def naive_bayes_fit(X, y):
    """
    :param X: 樣本特徵
    :param Y: 樣本標籤
    :returns: 二元組 - (先驗機率, 似然性)
    """
    # 計算先驗機率
    clazz_labels, clazz_counts = np.unique(y, return_counts=True)
    prior_probs = pd.Series({k: v / y.size for k, v in zip(clazz_labels, clazz_counts)})
    # 複製陣列建立副本
    X = np.copy(X)
    # 儲存似然性計算結果的字典
    likelihoods = {}
    for j in range(X.shape[1]):  # 對特徵的迴圈
        # 對特徵進行等寬分箱（離散化處理）
        X[:, j] = pd.cut(X[:, j], bins=5, labels=np.arange(1, 6))
        for i in prior_probs.index:
            # 按標籤類別拆分資料並統計每個特徵值出現的頻次
            x_prime = X[y == i, j]
            x_values, x_counts = np.unique(x_prime, return_counts=True)
            for k, value in enumerate(x_values):  # 對不同特徵值的迴圈
                # 計算似然性並儲存在字典中（字典的鍵是一個三元組 - (標籤, 特徵序號, 特徵值)）
                likelihoods[(i, j, value)] = x_counts[k] / x_prime.size
    return prior_probs, likelihoods
```

呼叫上面的函式，我們可以得到一個二元組，解包之後分別是類別標籤 $\small{C_{i}}$ 對應的先驗機率和在類別 $\small{C_{i}}$ 中，第 $\small{j}$ 個特徵取到某個值`value`（上面的程式碼中，我們用 pandas 的`cut`函式對特徵值分箱，`value`的取值為`1` 到`5`）的似然性，前者是一個`Series`物件，後者是一個`dict`物件，如下所示：

```python
p_ci, p_x_ci = naive_bayes_fit(X_train, y_train)
print('先驗機率: ', p_ci, sep='\n')
print('似然性: ', p_x_ci, sep='\n')
```

輸出：

```
先驗機率: 
0    0.333333
1    0.333333
2    0.333333
dtype: float64
似然性: 
{(0, 0, 1.0): 0.525, (0, 0, 2.0): 0.45, (0, 0, 3.0): 0.025, (1, 0, 1.0): 0.05, (1, 0, 2.0): 0.375, (1, 0, 3.0): 0.425, (1, 0, 4.0): 0.15, (2, 0, 1.0): 0.025, (2, 0, 2.0): 0.025, (2, 0, 3.0): 0.45, (2, 0, 4.0): 0.3, (2, 0, 5.0): 0.2, (0, 1, 1.0): 0.025, (0, 1, 3.0): 0.325, (0, 1, 4.0): 0.45, (0, 1, 5.0): 0.2, (1, 1, 1.0): 0.175, (1, 1, 2.0): 0.325, (1, 1, 3.0): 0.475, (1, 1, 4.0): 0.025, (2, 1, 1.0): 0.025, (2, 1, 2.0): 0.35, (2, 1, 3.0): 0.525, (2, 1, 4.0): 0.05, (2, 1, 5.0): 0.05, (0, 2, 1.0): 1.0, (1, 2, 2.0): 0.025, (1, 2, 3.0): 0.525, (1, 2, 4.0): 0.45, (2, 2, 4.0): 0.525, (2, 2, 5.0): 0.475, (0, 3, 1.0): 0.975, (0, 3, 2.0): 0.025, (1, 3, 2.0): 0.125, (1, 3, 3.0): 0.75, (1, 3, 4.0): 0.125, (2, 3, 3.0): 0.05, (2, 3, 4.0): 0.525, (2, 3, 5.0): 0.425}
```

> **說明**：字典中的第一個元素`(0, 0, 1.0): 0.525`表示標籤為`0`，第`0`個特徵（花萼長度）取值為`1.0`的似然性為`0.525`；最後一個元素`(2, 3, 5.0): 0.425`表示標籤為`2`，第`3`個特徵（花瓣寬度）取值為`5.0`的似然性為`0.425`。

預測階段我們利用上面函式得到的先驗機率和似然性計算後驗機率，然後根據後驗機率的最大值為樣本賦予預測的類別標籤。

```python
def naive_bayes_predict(X, p_ci, p_x_ci):
    """
    樸素貝葉斯分類器預測
    :param X: 樣本特徵
    :param p_ci: 先驗機率
    :param p_x_ci: 似然性
    :return: 預測的標籤
    """
    # 對特徵進行等寬分箱（離散化處理）
    X = np.copy(X)
    for j in range(X.shape[1]):
        X[:, j] = pd.cut(X[:, j], bins=5, labels=np.arange(1, 6))
    # 儲存每個樣本對應每個類別後驗機率的二維陣列
    results = np.zeros((X.shape[0], p_ci.size))
    clazz_labels = p_ci.index.values
    for k in range(X.shape[0]):
        for i, label in enumerate(clazz_labels):
            # 獲得先驗機率（訓練的結果）
            prob = p_ci.loc[label]
            # 計算獲得特徵資料後的後驗機率
            for j in range(X.shape[1]):
                # 如果沒有對應的似然性就取值為0
                prob *= p_x_ci.get((i, j, X[k, j]), 0)
            results[k, i] = prob
    # 根據每個樣本對應類別最大的機率選擇預測標籤
    return clazz_labels[results.argmax(axis=1)]
```

將上面的函式作用於測試集進行預測，比較預測值和真實值，如下所示。

```python
y_pred = naive_bayes_predict(X_test, p_ci, p_x_ci)
y_pred == y_test
```

輸出：

```
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
       False,  True,  True,  True,  True,  True,  True,  True, False,
        True,  True,  True])
```

上面兩個函式希望能幫助大家理解樸素貝葉斯的工作原理，實際工作中我們還是推薦大家使用 scikit-learn 庫的`navie_bayes`模組封裝的類來建立樸素貝葉斯模型，該模組下有五個樸素貝葉斯演算法的變體，每種變體針對不同型別的資料和特徵分佈，對應的五種樸素貝葉斯分類器分別是：`BernoulliNB`、`CategoricalNB`、`ComplementNB`、`GaussianNB`和`MultinomialNB`，它們之間的差別如下表所示：

| 分類器          | 特徵型別 | 主要假設                               |
| --------------- | -------- | ------------------------------------ |
| `BernoulliNB`   | 二元特徵 | 特徵服從 Bernoulli 分佈                |
| `CategoricalNB` | 類別特徵 | 特徵服從多項式分佈，常用於處理類別資料   | 
| `ComplementNB`  | 計數特徵 | 利用補集機率，常用於處理不平衡資料集    |
| `GaussianNB`    | 連續特徵 | 特徵服從高斯分佈                      |
| `MultinomialNB` | 計數特徵 | 特徵服從多項式分佈，常用於文字分類      |

對於鳶尾花資料集，由於其特徵是連續值，我們可以用`GaussianNB`來建立模型，程式碼如下所示。

```python
from sklearn.naive_bayes import GaussianNB

model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

我們看看模型評估的結果。

```python
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))
```

輸出：

```
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        10
           1       0.91      1.00      0.95        10
           2       1.00      0.90      0.95        10

    accuracy                           0.97        30
   macro avg       0.97      0.97      0.97        30
weighted avg       0.97      0.97      0.97        30
```

如果想看看樸素貝葉斯模型給每個樣本對應到每個標籤給出的機率值，可以使用下面的程式碼。

```python
model.predict_proba(X_test).round(2)
```

輸出：

```
array([[1.  , 0.  , 0.  ],
       [1.  , 0.  , 0.  ],
       [1.  , 0.  , 0.  ],
       [1.  , 0.  , 0.  ],
       [1.  , 0.  , 0.  ],
       [0.  , 0.  , 1.  ],
       [0.  , 1.  , 0.  ],
       [1.  , 0.  , 0.  ],
       [0.  , 0.  , 1.  ],
       [0.  , 0.98, 0.02],
       [0.  , 1.  , 0.  ],
       [1.  , 0.  , 0.  ],
       [0.  , 1.  , 0.  ],
       [0.  , 1.  , 0.  ],
       [0.  , 0.  , 1.  ],
       [1.  , 0.  , 0.  ],
       [0.  , 0.93, 0.07],
       [0.  , 0.  , 1.  ],
       [0.  , 0.02, 0.98],
       [1.  , 0.  , 0.  ],
       [0.  , 0.22, 0.78],
       [0.  , 0.  , 1.  ],
       [0.  , 0.  , 1.  ],
       [0.  , 0.92, 0.08],
       [1.  , 0.  , 0.  ],
       [0.  , 0.  , 1.  ],
       [0.  , 0.54, 0.46],
       [0.  , 1.  , 0.  ],
       [0.  , 1.  , 0.  ],
       [0.  , 0.81, 0.19]])
```

### 演算法優缺點

樸素貝葉斯演算法的優缺包括：

1. **邏輯簡單容易實現，適合大規模資料集**。
2. **運算開銷較小**。預測需要用到的機率在訓練階段都已經準好了，當新資料來了之後，只需要獲取對應的機率值並進行簡單的運算就能獲得預測的結果。
3. **受噪聲和無關屬性影響小**。

當然，由於做了“特徵相互獨立”這個假設，樸素貝葉斯演算法的缺點也相當明顯，因為在實際應用中，特徵之間很難做到完全獨立，尤其是維度很高的資料，如果特徵之間的相關性較大，那麼分類的效果就會變得很差。為了解決這個問題，在樸素貝葉斯演算法的基礎上又衍生出了一些新的方法，包括：半樸素貝葉斯（One Dependent Estimator）、AODE（Averaged One Dependent Estimator）、K 依賴樸素貝葉斯、樸素貝葉斯網路、高斯混合樸素貝葉斯等，有興趣的讀者可以自行了解。

### 總結

樸素貝葉斯演算法在多個領域有廣泛應用，以下是一些常見的應用場景：

- **文字分類**：如垃圾郵件檢測、情感分析等。
- **推薦系統**：根據使用者行為和喜好進行個性化推薦。
- **醫藥診斷**：根據症狀預測疾病。
