## 迴歸模型

迴歸模型是機器學習和統計學中的一種基本模型，用於預測連續型輸出變數。簡單的說，給定一組輸入變數（自變數）和對應的輸出變數（因變數），迴歸模型旨在找到輸入變數和輸出變數之間的對映關係。迴歸模型的形式可能比較簡單，但它確包含了機器學習中最主要的建模思想。通常，我們建立迴歸模型主要有兩個目標：

1. **描述資料之間的關係**。我們之前講過**機器學習的關鍵就是要透過歷史資料掌握如何從特徵對映到目標值**，這個過程不需要我們事先設定任何的規則，而是讓機器透過對歷史資料的學習來獲得。迴歸模型可以幫助我們透過模型表達輸入和輸出之間的關係。
2. **對未知資料做出預測**。透過學習到的對映關係，模型可以對新的輸入資料進行預測。

迴歸模型的應用非常廣泛，我們為大家舉幾個具體的例子：

1. **零售行業**。全球最大的電商平臺亞馬遜（Amazon）會根據歷史銷量、商品屬性（價格、折扣、品牌、類別等）、時間特徵（季節、工作日、節假日等）、外部因素（天氣、社媒等）等特徵建立迴歸模型預測未來一段時間內不同商品的需求量。在促銷活動期間，會使用多元迴歸結合互動項來分析促銷對銷量的影響。
2. **汽車行業**。為了最佳化電池的充電策略，延長電池的使用壽命，為電車使用者提供更準確的電量預警，特斯拉（Tesla）使用迴歸模型，透過電池充放電次數、環境溫度、放電深度、電池物理引數等建立迴歸模型，預測電池的剩餘壽命。
3. **房地產行業**。美國最大的線上房地產平臺 Zillow 曾經使用迴歸模型幫助使用者評估房屋的價值，透過房屋的面積、房齡、地理位置、房屋型別、社群安全等級、學校評分等對房屋的市場價格做出預測。

### 迴歸模型的分類

根據模型的複雜程度和假設，迴歸模型可以分為以下幾類：

1. **線性迴歸**（Linear Regression）：假設輸入變數和輸出變數之間是線性關係。

一元線性迴歸：建立一個因變數與單個自變數之間線性關係的模型。

$$
y = \beta_0 + \beta_1 x + \varepsilon
$$

其中， $\small{y}$ 是目標變數（因變數）， $\small{x}$ 是輸入變數（自變數）， $\small{\beta_{0}}$ 是截距，表示 $\small{x=0}$ 時的預測值， $\small{\beta_{1}}$ 是迴歸係數（斜率），表示輸入變數對輸出變數影響的大小， $\small{\varepsilon}$ 是誤差項，用於表示資料中的隨機噪聲或無法解釋的部分。

多元線性迴歸：建立一個因變數與多個自變數之間線性關係的模型。

$$
y = \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \cdots + \beta_{n} x_{n} + \varepsilon
$$

上面的公式也可以用向量的形式簡化表示為：

$$
y = \mathbf{x}^{T} \mathbf{\beta} + \varepsilon
$$

其中， $\small{\mathbf{x} = [1, x_{1}, x_{2}, \dots, x_{n}]^{T}}$ 是包含截距的輸入向量， $\small{\mathbf{\beta} = [\beta_{0}, \beta_{1}, \beta_{2}, \dots, \beta_{n}]^{T}}$ 是模型引數（包括截距 $\small{\beta_{0}}$ 和迴歸係數 $\small{\beta_{1}, \beta_{2}, \cdots, \beta_{n}}$ ）， $\small{\varepsilon}$ 是誤差項。

2. **多項式迴歸**（Polynomial Regression）：引入高階特徵，使模型能擬合更復雜的非線性關係，屬於線性模型的擴充套件，因為因為它對引數 $\small{\beta}$ 的求解仍然是線性形式，如下面所示的二次關係：

$$
y = \beta_{0} + \beta_{1} x + \beta_{2} x^{2} + \varepsilon
$$

3. **非線性迴歸**（Nonlinear Regression）：非線性迴歸完全放棄了線性假設，模型形式可以是任意非線性函式。

4. **嶺迴歸**（Ridge Regression）、**套索迴歸**（Lasso Regression）、**彈性網路迴歸**（Elastic Net Regression）：線上性迴歸基礎上加入正則化項，用於處理過擬合、多重共線性和特徵篩選問題。

5. **邏輯迴歸**（Logistic Regression）：邏輯迴歸雖然名字中帶“迴歸”，但實際上是用於分類問題的模型。它透過 Sigmoid 函式將線性組合的輸入值對映到區間 $\small{(0, 1)}$ ，表示分類機率，適用於二分類問題；也可以擴充套件為 Softmax 迴歸，解決多分類問題。

$$
P(y=1 \vert x) = \frac{1}{1 + e^{-(\beta_{0} + \beta_{1} x_{1} + \cdots + \beta_{n} x_{n})}}
$$

### 迴歸係數的計算

建立迴歸模型的關鍵是找到最佳的迴歸係數 $\small{\mathbf{\beta}}$ ，所謂最佳迴歸係數是指讓模型對資料的擬合效果達到最好的模型引數，即能夠最小化模型的預測值 $\small{\hat{y_{i}}}$ 與實際觀測值 $\small{y_{i}}$ 之間差異的模型引數。為此，我們先定義如下所示的損失函式。

$$
L(\mathbf{\beta}) = \sum_{i=1}^{m}(y_{i} - \hat{y_{i}})^{2}
$$

其中， $\small{m}$ 表示樣本容量，代入迴歸模型，有：

$$
L(\mathbf{\beta}) = \sum_{i=1}^{m}(y_{i} - \mathbf{x}_{i}^{T}\mathbf{\beta})^{2}
$$

如果用矩陣形式表示，有：

$$
L(\mathbf{\beta}) = (\mathbf{y} - \mathbf{X\beta})^{T}(\mathbf{y} - \mathbf{X\beta})
$$

其中， $\mathbf{y}$ 是目標值的向量，大小為 $\small{m \times 1}$， $\mathbf{X}$ 是特徵矩陣，大小為 $\small{m \times n}$， $\small{\mathbf{\beta}}$ 是迴歸係數的向量，大小為 $\small{n \times 1}$。

透過最小化損失函式 $\small{L(\mathbf{\beta})}$ ，我們可以得到線性迴歸模型的解析解。對 $\small{L(\mathbf{\beta})}$ 求導並令其為 0，有：

$$
\frac{\partial{L(\mathbf{\beta})}}{\partial{\mathbf{\beta}}} = -2\mathbf{X}^{T}(\mathbf{y} - \mathbf{X\beta}) = 0
$$

整理後得到：

$$
\mathbf{\beta} = (\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}
$$

對於矩陣 $\small{\mathbf{X}^{T}\mathbf{X}}$ 不滿秩的情況，可以透過新增正則化項的方式使得矩陣可逆，如下所示，這個就是線性迴歸的解析解。

$$
\mathbf{\beta} = (\mathbf{X}^{T}\mathbf{X} + \mathbf{\lambda \mit{I}})^{-1}\mathbf{X}^{T}\mathbf{y}
$$

> **說明**：如果你對這裡提到的正則化不理解可以先放放，後面我們再來討論這個問題。

上述方法適用於小規模資料集，當資料體量不大（樣本和特徵數量較少）時，計算效率是沒有問題的。對於大規模資料集或更為複雜的最佳化問題，我們可以使用**梯度下降法**，透過迭代更新引數來逐步逼近最優解。梯度下降法的目標也是最小化損失函式，該方法透過計算梯度方向進行引數更新。梯度是一個向量，包含了目標函式在各個引數方向上的偏導數。對於上面的損失函式 $\small{L(\mathbf{\beta})}$ ，梯度可以表示為：

$$
\nabla L(\mathbf{\beta}) = \left[ \frac{\partial{L}}{\partial{\beta_{1}}},  \frac{\partial{L}}{\partial{\beta_{2}}}, \cdots,  \frac{\partial{L}}{\partial{\beta_{n}}} \right]
$$

梯度下降法透過以下更新規則來更新引數 $\small{\mathbf{\beta}}$ ：

$$
\mathbf{\beta}^{\prime} = \mathbf{\beta} - \alpha \nabla L(\mathbf{\beta}) \\\\
\mathbf{\beta} = \mathbf{\beta^{\prime}}
$$

其中， $\small{\alpha}$ 是學習率（step size），通常是一個較小的正數，用於控制每次更新的幅度。如果學習率 $\small{\alpha}$ 選擇得當，梯度下降法將收斂到目標函式的區域性最小值。如果學習率過大，可能導致震盪不收斂；如果學習率過小，則收斂的速度緩慢，需要更多次的迭代。

### 新資料集介紹

之前介紹的鳶尾花資料集並不適合講解迴歸模型，為此我們引入另一個經典的汽車 MPG 資料集。汽車 MPG 資料集最初由美國汽車協會提供，我們可以透過該資料集預測車輛的燃油效率，即每加侖燃料行駛的里程（Miles Per Gallon, MPG）。需要注意的是，scikit-learn 庫沒有內建該資料集，我們可以直接從 [UCI 機器學習倉庫](https://archive.ics.uci.edu/dataset/9/auto+mpg) 網站下載資料集，也可以透過執行下面的程式碼聯網載入該資料集。

```python
import ssl
import pandas as pd

ssl._create_default_https_context = ssl._create_unverified_context
df = pd.read_csv('https://archive.ics.uci.edu/static/public/9/data.csv')
df.info()
```

輸出：

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 398 entries, 0 to 397
Data columns (total 9 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   car_name      398 non-null    object 
 1   cylinders     398 non-null    int64  
 2   displacement  398 non-null    float64
 3   horsepower    392 non-null    float64
 4   weight        398 non-null    int64  
 5   acceleration  398 non-null    float64
 6   model_year    398 non-null    int64  
 7   origin        398 non-null    int64  
 8   mpg           398 non-null    float64
dtypes: float64(4), int64(4), object(1)
memory usage: 28.1+ KB
```

根據上面的輸出，我們簡單介紹下資料集的九個屬性，前面八個都是輸入變數（第一個暫不使用），最後一個是輸出變數，具體如下表所示。

| 屬性名稱       | 描述                                                         |
| -------------- | ------------------------------------------------------------ |
| *car_name*     | 汽車的名稱，字串，這個屬性對建模暫時沒有幫助               |
| *cylinders*    | 氣缸數量，整數                                               |
| *displacement* | 發動機排量（立方英寸），浮點數                               |
| *horsepower*   | 馬力，浮點數，有空值需要提前處理                             |
| *weight*       | 汽車重量（磅），整數                                         |
| *acceleration* | 加速（0 - 60 mph所需時間），浮點數                           |
| *model_year*   | 模型年份（1970年 - 1982年），這裡用的是兩位的年份            |
| *origin*       | 汽車來源（1 = 美國, 2 = 歐洲, 3 = 日本），這裡的`1`、`2`、`3`應該視為三種類別而不是整數 |
| *mpg*          | 車輛的燃油效率，每加侖行駛的里程（目標變數）                 |

我們先刪除`car_name`這個暫時用不上的屬性，然後使用`DataFrame`物件的`corr`方法檢查輸入變數（特徵）與輸出變數（目標值）之間是否存在相關性。透過相關性分析我們可以選擇相關性強的特徵，剔除掉那些與目標值相關性較弱的特徵，這有助於減少模型的複雜性和過擬合的風險。在多元迴歸中，多重共線性（即輸入變數之間高度相關）可能會影響迴歸係數的估計，導致模型不穩定。可以透過計算特徵之間的相關性和方差膨脹因子（VIF）等來檢測共線性問題。

```python
# 刪除指定的列
df.drop(columns=['car_name'], inplace=True)
# 計算相關係數矩陣
df.corr()
```

> **說明**：`DataFrame`物件的`corr`方法預設計算皮爾遜相關係數，皮爾遜相關係數適合來自於正態總體的連續值，對於等級資料之間相關性的判定，可以透過修改`method`引數為`spearmean`或`kendall`來計算斯皮爾曼秩相關或肯德爾係數。當然，連續值也可以透過分箱操作處理成等級資料，然後再進行相關性的判定。

在使用該資料集建模之前，我們需要做一些準備工作，首先處理掉`horsepower`欄位的空值，然後將`origin`欄位處理成**獨熱編碼**（One-Hot Encoding）。獨熱編碼是一種用於處理分類變數的常見編碼方式，通常分類資料（如性別、顏色、季節等）無法直接輸入機器學習模型進行訓練，因為大多數演算法只能處理數值資料。獨熱編碼透過將每個分類變數轉換為若干個新的二元特徵（`0`或` 1`）來表示，從而使得這些變數可以輸入到機器學習模型中。假設我們有一個叫“顏色”的特徵列，可能的取值有`紅色`、`綠色`和`藍色`，我們可以將其透過獨熱編碼轉換成三個二元特徵，如下表所示：

| 紅色  | 綠色 | 藍色 |
| :--: | :--: | :--: |
| 1    | 0    | 0    |
| 0    | 1    | 0    |
| 0    | 0    | 1    |
| 0    | 1    | 0    |
| 1    | 0    | 0    |

> **說明**：我們也可以只保留綠色和藍色兩個列，如果兩個列的取值都為`0`，那麼說明我們的顏色是紅色。

獨熱編碼方法簡單直觀，容易理解和實現。對於無序類別獨熱編碼是非常有效的，因為它不會引入任何虛假的順序關係，處理後的資料型別是數值型的，很多機器學習演算法都能很好的處理。當然，如果類別特徵有大量不同的類別取值，獨熱編碼會生成大量的新特徵，可能導致資料的維度大幅增加，從而影響計算效能和儲存效率，尤其是在資料中有很多稀疏類別時。

下面的程式碼實現了對資料的預處理。

```python
# 刪除有缺失值的樣本
df.dropna(inplace=True)
# 將origin欄位處理為類別型別
df['origin'] = df['origin'].astype('category') 
# 將origin欄位處理為獨熱編碼
df = pd.get_dummies(df, columns=['origin'], drop_first=True)
df
```

輸出：

```
     cylinders  displacement  horsepower  weight  ...  model_year   mpg  origin_2  origin_3
0            8         307.0       130.0    3504  ...          70  18.0     False     False
1            8         350.0       165.0    3693  ...          70  15.0     False     False
2            8         318.0       150.0    3436  ...          70  18.0     False     False
3            8         304.0       150.0    3433  ...          70  16.0     False     False
4            8         302.0       140.0    3449  ...          70  17.0     False     False
..         ...           ...         ...     ...  ...         ...   ...       ...       ...
393          4         140.0        86.0    2790  ...          82  27.0     False     False
394          4          97.0        52.0    2130  ...          82  44.0      True     False
395          4         135.0        84.0    2295  ...          82  32.0     False     False
396          4         120.0        79.0    2625  ...          82  28.0     False     False
397          4         119.0        82.0    2720  ...          82  31.0     False     False

[392 rows x 9 columns]
```

> **說明**：上面呼叫 pandas 的`get_dummies`函式將`origin`列處理成了獨熱編碼，由於將`drop_first`引數設定為`True`，所以原來的取值`1`、`2`、`3`只保留了兩個列，分別叫`origin_2`和`origin_3`。Scikit-learn 庫中`preprocessing`模組的`OneHotEncoder`也支援將類別特徵處理成獨熱編碼。

接下來，我們還是將資料集拆分為訓練集和測試集，程式碼如下所示。

```python
from sklearn.model_selection import train_test_split

X, y = df.drop(columns='mpg').values, df['mpg'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=3)
```

### 線性迴歸程式碼實現

我們首先使用 scikit-learn 庫`linear_model`模組的`LinearRegression`來建立線性迴歸模型，`LinearRegression`使用最小二乘法計算迴歸模型的引數，程式碼如下所示。

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

如果想檢視線性迴歸模型的引數（迴歸係數和截距），可以透過下面的程式碼來實現。

```python
print('迴歸係數:', model.coef_)
print('截距:', model.intercept_)
```

輸出：

```
迴歸係數: [-0.70865621  0.03138774 -0.03034065 -0.0064137   0.06224274  0.82866534
  3.20888265  3.68252848]
截距: -21.685482718950933
```

### 迴歸模型的評估

迴歸模型的預測效果到底如何，我們可以透過下面的指標對其進行評估。

1. 均方誤差（Mean Squared Error, MSE）。MSE 是迴歸模型最常用的評估指標之一，定義為預測值與真實值誤差的平方平均值。

$$
\text{MSE} = \frac{1}{m} \sum_{i=1}^{m}(y_{i} - \hat{y_{i}})^{2}
$$

2. 均方根誤差（Root Mean Squared Error, RMSE）。RMSE 是 MSE 的平方根形式，用於更直觀地衡量誤差的實際尺度（單位與目標變數一致）。

$$
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{m} \sum_{i=1}^{m}(y_{i} - \hat{y_{i}})^{2}}
$$

3. 平均絕對誤差（Mean Absolute Error, MAE）。MAE 是另一個常用的誤差度量指標，定義為預測值與真實值誤差的絕對值平均值。

$$
\text{MAE} = \frac{1}{m} \sum_{i=1}^{m} \lvert y_{i} - \hat{y_{i}} \rvert
$$

4. 決定係數（R-Squared, $\small{R^{2}}$）。 $\small{R^{2}}$ 是一個相對指標，用於衡量模型對資料的擬合程度，其值越接近 1 越好。 $\small{R^{2}}$ 的計算公式為：

$$
R^{2} = 1 - \frac{SS_{res}}{{SS}_{tot}}
$$

其中，  $\small{SS_{res} = \sum_{i=1}^{m}(y_{i} - \hat{y_{i}})^{2}}$ 為殘差平方和； $\small{SS_{tot} = \sum_{i=1}^{m} (y_{i} - \bar{y})^{2}}$ 為總平方和，如下圖所示。

<img src="res/05_regression_r2.png" style="zoom:40%;">

上圖左邊紅色正方形的面積之和就是總平方和，右邊藍色正方形的面積之和就是殘差平方和，很顯然，模型擬合的效果越好，殘差平方和除以總平方和的值就越接近 0， $\small{R^{2}}$ 的值就越接近 1。通常 $\small{R^{2} \ge 0.8}$ 時，我們認為模型的擬合效果已經很不錯了。

可以使用 scikit-learn 中封裝好的函式計算出均方誤差、平均絕對誤差和 $\small{R^{2}}$ 的值，程式碼如下所示。

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'均方誤差: {mse:.4f}')
print(f'平均絕對誤差: {mae:.4f}')
print(f'決定係數: {r2:.4f}')
```

輸出：

```
均方誤差: 13.1215
平均絕對誤差: 2.8571
決定係數: 0.7848
```

### 引入正則化項

嶺迴歸是線上性迴歸的基礎上引入 $\small{L2}$ 正則化項，目的是防止模型過擬合，尤其是當特徵數較多或特徵之間存在共線性時。嶺迴歸的損失函式如下所示：

$$
L(\beta) = \sum_{i=1}^{m}{(y_{i} - \hat{y_{i}})^{2}} + \lambda \cdot \sum_{j=1}^{n}{\beta_{j}^{2}}
$$

其中， $\small{L2}$ 正則化項 $\small{\lambda \sum_{j=1}^{n} \beta_{j}^{2}}$ 會懲罰較大的迴歸係數，相當於縮小了迴歸係數的大小，但不會使係數為 0（即不會進行特徵選擇）。可以透過 scikit-learn 庫`linear_model`模組的`Ridge`類實現嶺迴歸，程式碼如下所示。

```python
from sklearn.linear_model import Ridge

model = Ridge()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print('迴歸係數:', model.coef_)
print('截距:', model.intercept_)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'均方誤差: {mse:.4f}')
print(f'決定係數: {r2:.4f}')
```

輸出：

```
迴歸係數: [-0.68868217  0.03023126 -0.0291811  -0.00642523  0.06312298  0.82583962
  3.04105754  3.49988826]
截距: -21.390402697674855
均方誤差: 12.9604
決定係數: 0.7874
```

套索迴歸引入 $\small{L1}$ 正則化項，不僅防止過擬合，還具有特徵選擇的功，特別適用於高維資料。套索迴歸的損失函式如下所示：

$$
L(\mathbf{\beta}) = \sum_{i=1}^{m}{(y_{i} - \hat{y_i})^{2}} + \lambda \cdot \sum_{j=1}^{n}{\lvert \beta_{j} \rvert}
$$

其中， $\small{L1}$ 正則化項 $\small{\lambda \sum_{j=1}^{n} \lvert \beta_{j} \rvert}$ 會將某些不重要的迴歸係數縮減為 0，從而實現特徵選擇。可以透過 scikit-learn 庫`linear_model`模組的`Lasso`類實現套索迴歸，程式碼如下所示。

```python
from sklearn.linear_model import Lasso

model = Lasso()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print('迴歸係數:', model.coef_)
print('截距:', model.intercept_)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'均方誤差: {mse:.4f}')
print(f'決定係數: {r2:.4f}')
```

輸出：

```
迴歸係數: [-0.00000000e+00  4.46821248e-04 -1.22830326e-02 -6.29725191e-03
  0.00000000e+00  6.91590631e-01  0.00000000e+00  0.00000000e+00]
截距: -9.109888229245005
均方誤差: 11.1035
決定係數: 0.8179
```

> **注意**：上面程式碼執行結果中的迴歸係數，有四個特徵的迴歸係數被設定為 0，相當於從 8 個特徵中選出了 4 個重要的特徵。模型的擬合效果是優於之間的迴歸模型的，這一點從均方誤差和決定係數可以看出。

彈性網路迴歸結合了嶺迴歸和套索迴歸的優點，透過同時引入 $\small{L1}$ 和 $\small{L2}$ 正則化項，適用於高維資料且特徵之間存在相關的情況，其損失函式如下所示：

$$
L(\mathbf{\beta}) = \sum_{i=1}^{m}{(y_{i} - \hat{y_i})^{2}} + \alpha \cdot \lambda \sum_{j=1}^{n}{\lvert \beta_{j} \rvert} + (1 - \alpha) \cdot \lambda \cdot \sum_{j=1}^{n}{\beta_{j}^{2}}
$$

其中， $\small{\alpha}$ 是控制 $\small{L1}$ 和 $\small{L2}$ 正則化的權重比例。

### 線性迴歸另一種實現

上面我們提到過，除了最小二乘法我們還可以使用梯度下降法來求解迴歸模型的引數，scikit-learn 庫`linear_model`模組的`SGDRegressor`就使用了這種方法，SGD 就是 Stochastic Gradient Descent 的縮寫。隨機梯度下降每次迭代只使用一個隨機樣本來計算梯度，計算速度快，適合大規模資料集，而且可以跳出區域性最優解。需要注意的是它的學習率，如果學習率設定得不合理，它的收斂性可能會發生波動，通常需要使用學習率衰減策略來促進收斂。此外，隨機梯度下降對特徵的尺度非常敏感，通常需要在訓練之前對特徵進行標準化或歸一化處理，完整的程式碼如下所示。

```python
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler

# 對特徵進行選擇和標準化處理
scaler = StandardScaler()
scaled_X = scaler.fit_transform(X[:, [1, 2, 3, 5]])
# 重新拆分訓練集和測試集
X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, train_size=0.8, random_state=3)

# 模型的建立、訓練和預測
model = SGDRegressor()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print('迴歸係數:', model.coef_)
print('截距:', model.intercept_)

# 模型評估
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'均方誤差: {mse:.4f}')
print(f'決定係數: {r2:.4f}')
```

輸出：

```
迴歸係數: [-0.25027084 -0.41349219 -4.9559786   2.83009217]
截距: [23.48707219]
均方誤差: 11.3853
決定係數: 0.8133
```

這裡，我們還需要強調一下`SGDRegressor`建構函式幾個重要的引數，也是迴歸模型比較重要的超引數，如下所示：

1. `loss`：指定最佳化目標（損失函式），預設值為`'squared_error'`（最小二乘法），其他可以選擇的值有：`'huber'`、`'epsilon_insensitive'` 和 `'squared_epsilon_insensitive'`，其中`'huber'`適用於對異常值更魯棒的迴歸模型。

2. `penalty`：指定正則化方法，用於防止過擬合，預設為`'l2'`（L2 正則化），其他可以選擇的值有：`'l1'`（L1正則化）、`'elasticnet'`（彈性網路，L1 和 L2 的組合）、`None`（不使用正則化）。

3. `alpha`：正則化強度的係數，控制正則化項的權重，預設值為`0.0001`。較大的 `alpha` 值會加重正則化的影響，從而限制模型複雜度；較小的值會讓模型更關注訓練資料的擬合。
4. `l1_ratio`：當 `penalty='elasticnet'` 時，控制 L1 和 L2 正則化之間的權重，預設值為`0.15`，取值範圍為`[0, 1]`（`0` 表示完全使用 L2，`1` 表示完全使用 L1）。
5. `tol`：最佳化演算法的容差，即判斷收斂的閾值，預設值為`1e-3`。當目標函式的改變數小於 `tol` 時，訓練會提前終止；如果希望訓練更加精確，可以適當降低 `tol`。
6. `learning_rate`：指定學習率的調節策略，預設值為`'constant'`，表示使用固定學習率，具體的值由 `eta0` 指定；其他可選項包括：
    - `'optimal'`：基於公式`eta = 1.0 / (alpha * (t + t0))`自動調整。
    - `'invscaling'`：按 `eta = eta0 / pow(t, power_t)` 縮放學習率。
    - `'adaptive'`：動態調整，誤差減少時保持當前學習率，否則減小學習率。

7. `eta0`：初始學習率，預設值為`0.01`，當 `learning_rate='constant'` 或其他策略使用時，`eta0` 決定了初始更新步長。
8. `power_t`：當 `learning_rate='invscaling'` 時，控制學習率衰減速度，預設值為`0.25`。較小的值會讓學習率下降得更慢，從而更長時間地關注全域性最佳化。
9. `early_stopping`：是否啟用早停機制，預設值為`False`。如果設定為 `True`，模型會根據驗證集效能自動停止訓練，防止過擬合。
10. `validation_fraction`：指定用作驗證集的訓練資料比例，預設值為`0.1`。當 `early_stopping=True` 時，該引數會起作用。
11. `max_iter`：訓練的最大迭代次數，預設值為`1000`。當資料較大或學習率較小時，可能需要增加迭代次數以保證收斂。
12. `shuffle`：是否在每個迭代輪次開始時打亂訓練資料，預設值為`True`，表示打亂資料。打亂資料有助於提高模型的泛化能力。
13. `warm_start`：是否使用上次訓練的引數繼續訓練，預設值為`False`。當設定為 `True` 時，可以在已有模型的基礎上進一步最佳化。
14. `verbose`：控制訓練過程的日誌輸出，預設值為`0`，可以設定為更高值以觀察訓練進度。

### 多項式迴歸

有的時候，我們關心的自變數和因變數之間並不是簡單的線性關係，例如廣告投入與銷售額增長的關係、裝置的使用時間與故障率之間的關係等。除此以外，如果資料中存在明顯的拐點或者要透過簡單的公式來近似某些複雜的現象，線性迴歸模型可能並不能滿足這樣的需求，這個時候我們就需要建立多項式迴歸模型。

下面我們用一個簡單的例子對多項式迴歸加以說明，我們先生成一組資料點並繪製出對應的散點圖，程式碼如下所示。

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0, 6, 150)
y = x ** 2 - 4 * x + 3 + np.random.normal(1, 1, 150)
plt.scatter(x, y)
plt.show()
```

輸出：

<img src="res/05_polynomial_scatter.png" style="zoom:50%;">

顯然，這樣的一組資料點是很難透過線性模型進行擬合的，下面的程式碼可以證明這一點。

```Python
x_ = x.reshape(-1, 1)

model = LinearRegression()
model.fit(x_, y)
a, b = model.coef_[0], model.intercept_
y_pred = a * x + b
plt.scatter(x, y)
plt.plot(x, y_pred, color='r')
plt.show()
```

輸出：

<img src="res/05_polynomial_line_fit.png" style="zoom:50%;">

很顯然，這是一個欠擬合的結果。我們再看看 $\small{R^{2}}$ 的值：

```python
r2 = r2_score(y, y_pred)
print(f'決定係數: {r2:.4f}')
```

輸出：

```
決定係數: 0.5933
```

Scikit-learn 庫`preprocessing`模組中的`PolynomialFeatures`類可以將原始特徵擴充套件為多項式特徵，從而將線性模型轉換為具有高次項的模型。建立`PolynomialFeatures`物件時有幾個重要的引數：

1. `degree`：設定多項式的最高次項。例如，`degree=3` 會生成包含一次項、二次項和三次項的特徵。
2. `interaction_only`：預設值為`False`，如果設定為`True`，則只生成互動項（如 $\small{x_{1}x_{2}}$ ），不生成單獨的高次項（如 $\small{x_{1}^{2}}$ 、 $\small{x_{2}^{2}}$ ）。
3. `include_bias`：預設值為`True`，表示包括常數項（通常為 1），設定為`False`則不包括常數項。

下面我們透過程式碼來演示如何透過`PolynomialFeatures`類進行特徵預處理，實現多項式迴歸。

```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
x_ = poly.fit_transform(x_)

model = LinearRegression()
model.fit(x_, y)
y_pred = model.predict(x_)
r2 = r2_score(y, y_pred)
print(f'決定係數: {r2:.4f}')
```

輸出：

```
決定係數: 0.9497
```

透過特徵預處理引入了高次項之後，模型擬合的效果得到了明顯的改善。如果希望只對一部分特徵新增高次項，可以使用 scikit-learn 庫`compose`模組的`ColumnTransformer`來定義處理規則，有興趣的讀者可以自行研究。需要注意的是，多項式迴歸時隨著高次項的引入，模型發生過擬合的風險增加會大大增加，尤其在資料量較小時；另一方面，高次項的值可能會導致特徵範圍變得很大，要考慮對特徵進行標準化處理。

### 邏輯迴歸

邏輯迴歸儘管名字中含有“迴歸”，但邏輯迴歸實際上是一種分類演算法，用於處理二分類問題，例如電子郵件是不是垃圾郵件、使用者是否會點選廣告、信用卡客戶是否存在違約風險等。邏輯迴歸的核心思想是透過 Sigmoid 函式將線性迴歸的輸出對映到$\small{(0, 1)}$區間，作為對分類機率的預測。Sigmoid 函式的曲線如下圖所示。

<img src="res/05_sigmoid_function.png" style="zoom:58%;">

下面，我們用 scikit-learn 庫`datasets`模組提供的`make_classification`函式生成一組模擬資料， 透過邏輯迴歸來構建分類預測模型，程式碼如下所示。

```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# 生成1000條樣本資料，每個樣本包含6個特徵
X, y = make_classification(n_samples=1000, n_features=6, random_state=3)
# 將1000條樣本資料拆分為訓練集和測試集
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=3)

# 建立和訓練邏輯迴歸模型
model = LogisticRegression()
model.fit(X_train, y_train)

# 對測試集進行預測並評估
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

輸出：

```
              precision    recall  f1-score   support

           0       0.95      0.93      0.94       104
           1       0.93      0.95      0.94        96

    accuracy                           0.94       200
   macro avg       0.94      0.94      0.94       200
weighted avg       0.94      0.94      0.94       200
```

這裡，我們再強調一下`LogisticRegression`建構函式幾個重要的引數，也是邏輯迴歸模型比較重要的超引數，如下所示：

1. `penalty`：指定正則化型別，用於控制模型複雜度，防止過擬合，預設值為`l2`。
2. `C`：正則化強度的倒數，預設值為`1.0`。較小的 `C` 值會加強正則化（更多限制模型複雜度），較大的 `C` 值會減弱正則化（更注重擬合訓練資料）。
3. `solver`：指定最佳化演算法，預設值為`lbfgs`，可選值包括：
    - `'newton-cg'`、`'lbfgs'`、`'sag'`、`'saga'`：支援 L2 和無正則化。
    - `'liblinear'`：支援 L1 和 L2 正則化，適用於小型資料集。
    - `'saga'`：支援 L1、L2 和 ElasticNet，適用於大規模資料。
4. `multi_class`：指定多分類問題的處理方式，預設值為`'auto'`，根據資料選擇 `'ovr'` 或 `'multinomial'`，前者表示一對多策略，適合二分類或多分類的基礎情況，後者表示多項式迴歸策略，適用於多分類問題，需與 `'lbfgs'`、`'sag'` 或 `'saga'` 搭配使用。
5. `fit_intercept`：是否計算截距（偏置項），預設值為`True`。
6. `class_weight`：類別權重，處理類別不平衡問題，預設值為`None`，設定為`'balanced'`可以根據類別頻率自動調整權重。

> **說明**：邏輯迴歸有些超引數跟我們之前講的`SGDRegressor`是類似的，此處不再進行贅述。

### 總結

迴歸模型是一種統計分析方法，用於建立自變數與因變數之間的關係。它透過擬合資料來預測目標變數的值，在經濟學、工程、醫學等領域有著廣泛的應用，可以幫助決策者進行資料驅動的預測和分析。

