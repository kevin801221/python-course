## 神經網路模型

在人類的大腦中，神經元（neuron）是負責資訊傳遞和處理的單元，神經元透過化學訊號和電訊號進行交流，這是人類記憶、感覺、運動等功能的基礎。神經元包含了軸突（axon）和樹突（dendrite），樹突負責接收訊號，軸突負責傳送訊號，此外細胞體也是神經元的重要部分，起到整合和傳遞資訊的作用。神經元之間的連線透過突觸（synapse）傳遞化學訊號或電訊號來實現，一個神經元可能會與成千上萬個神經元連線，構成錯綜複雜的神經網路。人在剛出生時，大腦中有約 860 億神經元，大部分神經元是不會再生的，所以這個數字會隨著年齡的增長而略為減少。新生兒的大腦擁有數量極其龐大的突觸連線，為未來的學習和適應奠定基礎。

很多科普文章都宣傳神經網路模型是模擬人腦神經元的計算模型，透過多層神經元連線來完成複雜的非線性對映，但是沒有證據表明大腦的學習機制與神經網路模型機制相同。雖然神經網路這個術語來自於神經生物學，深度學習中的一些概念也是從人類對大腦的理解中汲取靈感而形成的，但是對新手來說，如果認為神經網路與神經生物學存在某種聯絡，只會讓人變得更加困惑。

### 基本構成

神經網路一般由多個層組成，通常包括輸入層、隱藏層和輸出層。每層中的神經元與前一層的神經元相連線，透過權重調整和啟用函式的作用，逐層傳遞和處理資料，最終完成複雜資料的特徵提取和學習。

1. **輸入層**：輸入層神經元接收輸入資料，每個神經元通常對應一個輸入特徵。輸入層的任務是將輸入資料傳遞給下一層的神經元。
2. **隱藏層**：隱藏層是網路的核心部分，負責對資料進行特徵提取和處理。隱藏層可以有一層或多層，每層的神經元透過權重和啟用函式來進行資料處理。深度神經網路通常會包含多層隱藏層，這樣能夠提取資料中的高階特徵。
3. **輸出層**：輸出層負責將隱藏層的結果轉換為最終輸出。輸出層的啟用函式通常與問題型別有關，例如二分類任務可能用 Sigmoid 函式以獲得機率，而回歸任務則可能直接輸出數值。
4. **神經元**：每個神經元都接收來自前一層神經元的輸入，進行加權求和，再透過啟用函式計算輸出。

<img src="res/08_neural-network-diagram.webp" style="zoom:80%;">

神經網路的訓練過程通常透過反向傳播演算法（Backpropagation）和梯度下降法來最佳化權重，以減少預測誤差。深度神經網路透過多層非線性轉換能夠學習到更加複雜的特徵表示，因此在許多工中表現優異。

### 工作原理

我們先設計一個最簡單的神經網路模型，它只有一層輸入和一層輸出，該模型會根據輸入（ $\small{X}$ ）去預測輸出（ $\small{y}$ ），而輸出又滿足下面的公式：

$$
y = aX_{1} + bX_{2} + cX_{3} + d
$$

其中， $\small{a, b, c, d}$ 都是模型的引數（權重和偏置），那麼我們的神經網路模型應該是如下圖所示的結構。

<img src="res/08_one_layer_nn.png" style="zoom:55%;">

大家可能已經發現，這個最簡單的神經網路模型跟線性迴歸根本沒有區別，但是我們在輸入層和輸出層中間再加上一個隱藏層又會怎樣呢？看看下面的圖，是不是已經有一點“網路”的味道了，加入的隱藏層可以處理非線性關係，而且隱藏層可以有多個，層與層之間連線的方式可以是全連線或部分連線，甚至可以出現環形結構。上面我們也說過，引入非線效能夠讓模型學習到更加複雜的特徵表示，這也是神經網路模型在很多工中表現優異的原因。

<img src="res/08_multi_layer_nn.png" style="zoom:58%;">

接下來，我們看看神經網路的工作原理。每個神經元的計算過程可以表示為：

$$
y = f \left( \sum_{i=1}^{n}w_{i}x_{i} + b \right)
$$

其中， $\small{x_{1}, x_{2}, \dots, x_{n}}$ 是輸入特徵， $\small{w_1, w_2, \dots, w_n}$ 是與輸入對應的權重； $\small{b}$ 是偏置項； $\small{f}$ 是啟用函式，通常是非線性函式，如 Sigmoid、ReLU、Tanh 等。啟用函式一個方面是引入非線性變換，增加了神經網路的表達能力，使得它可以模擬任意複雜的函式關係；另一方面，在訓練神經網路時會使用反向傳播來更新權重，啟用函式的非線性屬性使得梯度傳遞更有效，讓神經網路收斂得更快，提高訓練效率。下面我們對常用的啟用函式做一個簡要的介紹。

1. Sigmoid 函式
  
$$
f(x) = \frac{1}{1 + e^{-x}}
$$

<img src="res/08_sigmoid_function.png" style="zoom:62%;">
    
- **特點**：Sigmoid 函式將輸入值對映到 $\small{(0, 1)}$ 的範圍內，呈現出平滑的 S 型曲線。
- **優點**：特別適用於機率預測，因為輸出在 $\small{(0, 1)}$ 之間，可以理解為機率值。
- **缺點**：對於較大的正值或負值，梯度會變得很小，導致梯度消失問題，從而影響深層網路的訓練。除此以外，由於輸出非零中心，這會導致梯度更新不對稱，可能使得收斂變慢。

2. Tanh 函式（雙曲正切函式）

$$
f(x) = tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
$$

<img src="res/08_tanh_function.png" style="zoom:62%;" />
    
- **特點**：Tanh 函式將輸入對映到 $\small{(-1, 1)}$ 的範圍內，也是 S 型曲線，但中心對稱。
- **優點**：與 Sigmoid 類似，但輸出在 $\small{(-1, 1)}$ 之間，這樣的零中心輸出使得梯度更新更對稱，更適合用於深層網路。
- **缺點**：在極值附近，梯度仍會趨向於零，導致梯度消失問題。

3. ReLU 函式（Rectified Linear Unit）

$$
f(x) = max(0, x)
$$

- **特點**：ReLU 將輸入小於零的部分設為零，而大於零的部分保持不變，因此其輸出範圍是 $\small{[0, +\infty]}$ 。
- **優點**：計算簡單，有效避免了梯度消失問題，因此被廣泛應用於深層網路。能夠保持稀疏性，許多神經元的輸出為零，有利於網路簡化計算。
- **缺點**：當輸入為負數時，ReLU 的梯度為零。若輸入長期為負數，神經元可能“死亡”並停止更新。

4. Leaky ReLU 函式

$$
f(x) = \begin{cases} x & (x \gt 0) \\\\ {\alpha}x & (x \le 0)\end{cases}
$$

- **特點**：Leaky ReLU 是對 ReLU 的改進，它為輸入小於零的部分引入了一個小的負斜率（通常取值 $\small{\alpha = 0.01}$ ），使得梯度不為零。
- **優點**：透過允許負值的輸出，避免了死神經元問題，使得網路更健壯。
- **缺點**：雖然 Leaky ReLU 能緩解死神經元問題，但其負值斜率的選擇對網路效能會有一些影響，且對模型的非線性表示能力沒有顯著提升。

在一個包含多個層的神經網路中，資訊會一層一層的進行傳遞。假設第 $\small{l}$ 層的輸出是 $\small{\mathbf{a}^{[l]}}$  ，按照上面神經元計算公式，有：

$$
\mathbf{a}^{[l]} = f \left( \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]} \right)
$$

其中， $\small{\mathbf{W}^{[l]}}$ 是第 $\small{l}$ 層的權重矩陣， $\small{\mathbf{a}^{[l-1]}}$ 是是第 $\small{l - 1}$ 層的輸出， $\small{\mathbf{b}^{[l]}}$ 是第 $\small{l}$ 層的偏置項， $\small{f}$ 是啟用函式。神經網路最終的輸出是透過最後一層的啟用函式得到的，這個過程叫做前向傳播（forward-propagation）。

對於神經網路模型來說，還有一個極其重要的操作就是透過計算損失函式相對於每個權重和偏置的梯度來更新神經網路的引數（權重和偏置），這一過程通常稱為反向傳播（back-propagation）。反向傳播有兩個要點，一個是損失函式，一個是梯度下降法，前者用於衡量預測值與真實值之間的差距，常用的損失函式有均方誤差（迴歸任務）和交叉熵損失函式（分類任務），後者透過更新引數 $\small{\theta}$（權重和偏置)，使得損失函式最小化，即：

$$
\theta^{\prime} = \theta - \eta \nabla L(\theta) \\\\
\theta = \theta^{\prime}
$$

其中， $\small{\eta}$ 是學習率， $\small{\nabla L(\theta)}$ 是損失函式相對於引數的梯度，跟我們講解迴歸模型時使用的方法是一致的。

### 程式碼實現

根據上面講到的神經網路模型的原理，要自己寫一個簡單的神經網路模型也並不困難。當然，我們也可以用 scikit-learn 庫`neural_network` 模組來構建神經網路模型。下面，我們仍然以鳶尾花資料集為例，為大家展示神經網路模型的構建和預測效果。

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report

# 載入和劃分資料集
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=3)

# 建立多層感知機分類器模型
model = MLPClassifier(
    solver='lbfgs',            # 最佳化模型引數的求解器
    learning_rate='adaptive',  # 學習率的調節方式為自適應 
    activation='relu',         # 隱藏層中神經元的啟用函式 
    hidden_layer_sizes=(1, )   # 每一層神經元的數量
)
# 訓練和預測
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 檢視模型評估報告
print(classification_report(y_test, y_pred))
```

輸出：

```
              precision    recall  f1-score   support

           0       1.00      0.10      0.18        10
           1       0.00      0.00      0.00        10
           2       0.34      1.00      0.51        10

    accuracy                           0.37        30
   macro avg       0.45      0.37      0.23        30
weighted avg       0.45      0.37      0.23        30
```

> **注意**：由於建立`MLPClassifier`時沒有指定`random_state`引數，所以程式碼每次執行的結果可能並不相同。

模型的預測準確率只有`0.37`，大家對這個結果是不是感覺到非常失望，我們煞費苦心構建的模型預測效果竟然如此拉胯。別緊張，上面程式碼中我們建立神經網路模型時，`hidden_layer_sizes`引數設定的是`(1, )`，它表示我們的網路只有 1 個隱藏層，而且隱藏層只有 1 個神經元，這個神經元承受了太多（它真的，我哭死）。接下倆，我們需要增加隱藏層和神經元的數量，讓模型可以更好的學習特徵和目標之間的對映關係，這樣預測的效果就會好起來。下面，我們將`hidden_layer_sizes`引數調整為`(32, 32, 32)`，即模型有三個隱藏層，每層有 32 個神經元，再來看看程式碼執行的結果。

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report

# 載入和劃分資料集
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=3)

# 建立多層感知機分類器模型
model = MLPClassifier(
    solver='lbfgs',                  # 最佳化模型引數的求解器
    learning_rate='adaptive',        # 學習率的調節方式為自適應 
    activation='relu',               # 隱藏層中神經元的啟用函式 
    hidden_layer_sizes=(32, 32, 32)  # 每一層神經元的數量
)
# 訓練和預測
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 檢視模型評估報告
print(classification_report(y_test, y_pred))
```

> **說明**：大家可以試著執行上面的程式碼，看看有沒有獲得更好的結果。當然，模型準確率為 1 也未必就值得高興，因為你可能訓練了一個過擬合的模型。無論如何，大家可以試著重新設定`hidden_layer_sizes`引數，看看會得到怎樣的結果。

下面，我們還是對`MLPClassifier`幾個比較重要的超引數做一個說明。

1. `hidden_layer_sizes`：指定神經網路中每一層的神經元數目，元組型別，預設值為`(100, )`，表示只有一個隱藏層，包含 100 個神經元。該超引數可以改變網路的結構和容量，層數越多，神經元數目越多，模型的表示能力就越強，但也存在過擬合風險。
2. `activation`：隱藏層中神經元的啟用函式，預設值為`relu`，表示使用 ReLU 啟用函式。啟用函式決定了網路每一層的輸出形態，一般來說，`'relu'` 是訓練深度網路時的首選，因為它能夠緩解梯度消失問題，並且訓練速度較快，可選值包括：
    - `'identity'`：線性啟用函式，即 $\small{f(x) = x}$ ，通常不推薦使用。
    - `'logistic'` / `'tanh'` / `'relu'`：Sigmoid / 雙曲正切 / ReLU 啟用函式。
3. `solver`：用來最佳化模型引數的求解器（最佳化演算法）。常用的最佳化演算法有：
    - `'lbfgs'`：擬牛頓法（Limited-memory Broyden-Fletcher-Goldfarb-Shanno），這種方法計算複雜度較高，但對小資料集表現較好。
    - `'sgd'`：隨機梯度下降（Stochastic Gradient Descent），適用於大規模資料集，在訓練時會隨機選取小批次資料進行更新。
    - `'adam'`：自適應矩估計（Adaptive Moment Estimation），預設值，適用於大多數情況且計算效率較高。
4. `alpha`：L2 正則化項（也稱為權重衰減），控制神經網路的複雜度，預設值為`0.0001`。
5. `batch_size`：每次迭代所用的樣本批次大小，預設值為`'auto'`，根據樣本數量自動決定批次大小。較小的批次會使訓練更加不穩定，但有助於避免陷入區域性最優解；較大的批次則可以加速訓練，但可能導致記憶體不足。
6. `learning_rate`：學習率的調節方式，影響模型在每次迭代時調整權重的步長，預設值為`'constant'`，表示學習率保持不變。其他可選的值有`'invscaling'`和`'adaptive'`，前者表示學習率隨著迭代次數的增加而逐步減小，後者表示學習率會根據當前梯度變化自動調整，當梯度更新較小時學習率會增加，當梯度較大時學習率會減小。
7. `learning_rate_init`：初始學習率，預設值為`0.001`。
8. `max_iter`：最大迭代次數，控制訓練過程中最佳化演算法的最大迭代次數，預設值為`200`。若達到最大迭代次數後模型仍未收斂，訓練將停止。這對於防止過長時間訓練是有用的，但過小的值可能會導致訓練中止太早，導致欠擬合。
9. `tol`：訓練過程中的最佳化容忍度，決定了當目標函式變化小於該值時訓練停止，預設值為`0.0001`。較小的 `tol` 會導致更長的訓練時間，較大的 `tol` 可能會提前停止訓練。

除了 scikit-learn 庫，我們還可以使用 TensorFlow、PyTorch、MXNet 等三方庫來實現神經網路模型，這些庫很多都支援 GPU（圖形處理單元） 或 TPU（張量處理單元） 加速，而且內建很多深度網路模型（如卷積神經網路、迴圈神經網路、生成對抗網路等）以及相應的最佳化器，相較於 scikit-learn，這些庫可能是更好的選擇。對此感興趣的小夥伴，可以看看下面的程式碼，我們用 PyTorch 構建了神經網路模型來解決鳶尾花分類問題。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

# 載入鳶尾花資料集
iris = datasets.load_iris()
X, y = iris.data, iris.target

# 資料預處理（標準化）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 劃分訓練集和測試集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=0.8, random_state=3)
# 將陣列轉換為PyTorch張量
X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = (
    torch.tensor(X_train, dtype=torch.float32),
    torch.tensor(X_test, dtype=torch.float32),
    torch.tensor(y_train, dtype=torch.long),
    torch.tensor(y_test, dtype=torch.long)
)


class IrisNN(nn.Module):
    """鳶尾花神經網路模型"""

    def __init__(self):
        """初始化方法"""
        # 呼叫父類構造器
        super(IrisNN, self).__init__()
        # 輸入層到隱藏層（4個特徵到32個神經元全連線）
        self.fc1 = nn.Linear(4, 32)
        # 隱藏層到輸出層（32個神經元到3個輸出全連線）
        self.fc2 = nn.Linear(32, 3)

    def forward(self, x):
        """前向傳播"""
        # 隱藏層使用ReLU啟用函式
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x


# 建立模型例項
model = IrisNN()
# 定義損失函式（交叉熵損失函式）
loss_function = nn.CrossEntropyLoss()
# 使用Adam最佳化器（大多數任務表現較好）
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 訓練模型（迭代256個輪次）
for _ in range(256):
    model.train()
    # 清除上一次的梯度
    optimizer.zero_grad()
    # 計算輸出
    output = model(X_train_tensor)
    # 計算損失
    loss = loss_function(output, y_train_tensor)
    # 反向傳播
    loss.backward()
    # 更新權重
    optimizer.step()

# 評估模型
model.eval()
with torch.no_grad():
    output = model(X_test_tensor)
    # 獲取預測得分最大值的索引（預測標籤）
    _, y_pred_tensor = torch.max(output, 1)
    # 計算並輸出預測準確率
    print(f'Accuracy: {accuracy_score(y_test_tensor, y_pred_tensor):.2%}')
    # 輸出分類模型評估報告
    print(classification_report(y_test_tensor, y_pred_tensor))
```

> **說明**：如果還沒有安裝 PyTorch 庫，可以使用命令`pip install torch`進行安裝。

我們也可以透過神經網路模型來處理迴歸任務，這裡仍然以之前講迴歸模型時使用過的“汽車 MPG 資料集”為例，演示如何透過構造神經網路模型解決迴歸任務，完整的程式碼如下所示。

```python
import ssl

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

ssl._create_default_https_context = ssl._create_unverified_context


def load_prep_data():
    """載入準備資料"""
    df = pd.read_csv('https://archive.ics.uci.edu/static/public/9/data.csv')
    # 對特徵進行清洗
    df.drop(columns=['car_name'], inplace=True)
    df.dropna(inplace=True)
    df['origin'] = df['origin'].astype('category')
    df = pd.get_dummies(df, columns=['origin'], drop_first=True).astype('f8')
    # 對特徵進行縮放
    scaler = StandardScaler()
    return scaler.fit_transform(df.drop(columns='mpg').values), df['mpg'].values


class MLPRegressor(nn.Module):
    """神經網路模型"""

    def __init__(self, n):
        super(MLPRegressor, self).__init__()
        self.fc1 = nn.Linear(n, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x


def main():
    # 載入和準備資料集
    X, y = load_prep_data()
    # 劃分訓練集和測試集
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=3)
    # 將資料轉為PyTorch的Tensor
    X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = (
        torch.tensor(X_train, dtype=torch.float32),
        torch.tensor(X_test, dtype=torch.float32),
        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),
        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)
    )

    # 例項化神經網路模型
    model = MLPRegressor(X_train.shape[1])
    # 指定損失函式（均方誤差）
    criterion = nn.MSELoss()
    # 指定最佳化器（Adam最佳化器）
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # 模型訓練
    epochs = 256
    for epoch in range(epochs):
        # 前向傳播
        y_pred_tensor = model(X_train_tensor)
        loss = criterion(y_pred_tensor, y_train_tensor)
        # 反向傳播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 16 == 0:
            print(f'Epoch [{epoch + 1} / {epochs}], Loss: {loss.item():.4f}')

    # 模型評估
    model.eval()
    with torch.no_grad():
        y_pred = model(X_test_tensor)
        test_loss = mean_squared_error(y_test, y_pred.numpy())
        r2 = r2_score(y_test, y_pred.numpy())
    print(f'Test MSE: {test_loss:.4f}')
    print(f'Test R2: {r2:.4f}')


if __name__ == '__main__':
    main()
```

輸出：

```
Epoch [16 / 256], Loss: 588.2971
Epoch [32 / 256], Loss: 530.0340
Epoch [48 / 256], Loss: 429.9081
Epoch [64 / 256], Loss: 286.5121
Epoch [80 / 256], Loss: 133.6717
Epoch [96 / 256], Loss: 44.3843
Epoch [112 / 256], Loss: 30.3168
Epoch [128 / 256], Loss: 24.0182
Epoch [144 / 256], Loss: 19.5326
Epoch [160 / 256], Loss: 16.4941
Epoch [176 / 256], Loss: 14.2642
Epoch [192 / 256], Loss: 12.5515
Epoch [208 / 256], Loss: 11.2248
Epoch [224 / 256], Loss: 10.1825
Epoch [240 / 256], Loss: 9.3600
Epoch [256 / 256], Loss: 8.7116
Test MSE: 8.7226
Test R2: 0.8569
```

透過上面的輸出可以看到，隨著神經網路模型不斷的前向傳播和反向傳播，損失變得越來越小，模型的擬合變得越來越好。在預測的時候，我們利用訓練得到的模型引數進行一次正向傳播，就完成了從特徵到目標值的對映，評估迴歸模型的兩個指標 MSE 和 $\small{R^{2}}$ 看起來還不錯喲。目前，神經網路被廣泛應用於模式識別、影象處理、語音識別等領域，是深度學習中最核心的技術。對深度學習有興趣的讀者，可以關注我的另一個專案[“深度學習就是大力出奇跡”](https://github.com/jackfrued/Deep-Learning-Is-Nothing)，目前該專案仍然在創作更新中。

### 模型優缺點

神經網路模型最可愛的一點就是可以像搭積木一樣不斷的擴充套件模型邊界，對於模型內部具體的執行則不需要加以太多的干涉。神經網路能夠學習和擬合非常複雜的對映關係，能夠根據輸入資料自動調整其結構和引數，因此具有較強的適應性。當面臨不同型別的資料或任務時，神經網路可以透過調整網路的結構（層數、節點數等）來適應不同的需求，只要資料量足夠大、算力足夠強，最終都會得到一個不錯的結果，表現常常優於傳統的機器學習演算法，這就是我們常說的“大力出奇跡”。此外，大家耳熟能詳的深度學習一詞中的“深度”二字，指的就是神經網路模型的層次具備了足夠的深度，我們也通常把這樣的神經網路稱為深度神經網路（DNN）。

神經網路模型就像一個黑盒子，你給它資料它告訴你結果，至於為什麼會出現這樣的結果卻缺乏可解釋性。所以，在很多對模型的解釋性要求非常高的場景（如信用評級、金融風控等），我們可能沒有辦法使用神經網路模型。再者，神經網路特別是深度神經網路需要大量的計算資源來訓練，尤其是在處理高維資料時，訓練時間和所需的計算資源可能非常龐大，需要強大的硬體支援，如高效能的圖形處理單元（GPU）或專用硬體（TPU）。最後，也是我個人覺得神經網路模型最坑的一點是超引數調優過於困難，網路的層數、每層神經元的數量、學習率、啟用函式的選擇等都可能影響模型的效能，如果沒有強大的計算資源作為支撐，很難完成超引數的調優，個人和小公司基本就被勸退了。

### 總結

神經網路模型使用了與傳統機器學習演算法不同的方式從海量資料中學習知識，儘管模型缺乏可解釋性，但不能否認它在影象、語音和自然語言處理等多個領域已經取得了顯著的成績。當然，訓練神經網路模型需要消耗大量的資源，在應用神經網路模型時建議權衡其優缺點，並根據具體的任務需求選擇合適的模型和訓練方法。對於那些資料量較小、需要可解釋性或計算資源有限的場景，我們建議考慮使用傳統的機器學習演算法或者其他更輕量級的模型。
