## K-Means聚類演算法

聚類（Clustering）是資料探勘和機器學習中的一種重要技術，用於將資料集中的樣本劃分為多個相似的組或類別。這種方法在許多領域得到了廣泛應用，例如：

- **電商行業**：根據使用者行為資料，將消費者分為不同的群體（高消費使用者、高活躍使用者、流失風險使用者、價格敏感使用者等）以制定有針對性的運營策略，實現更精準的廣告投放，代表性的企業包括阿里巴巴、亞馬遜（Amazon）等。
- **金融行業**：各大銀行透過聚類演算法分析使用者的信用記錄、收入水平、消費行為等資料，將使用者分為不同的風險群體，對於高風險群體需要更嚴格的信用稽核，而低風險群體可以享受更優惠的貸款利率或信用額度。
- **醫療行業**：透過分析患者的健康資料（如病史、基因資料、生活習慣等），將患者分為不同健康風險群體，提高疾病預測的準確性，推動了精準醫療的發展，代表性的公司如強生（Johnson & Johnson）、輝瑞（Pfizer）等。
- **社交媒體**：透過分析使用者的好友關係、興趣愛好、社互動動等資料，將使用者劃分為不同的社交圈，用於推薦好友、定製個性化內容以及最佳化了平臺上的資訊流推薦系統。

聚類是一種**無監督學習**，因為它不需要預先定義的標籤，只是根據資料特徵去學習，透過度量特徵相似度或者距離，然後把已知的資料集劃分成若干個不同的類別。與分類不同，聚類任務的目標是發現資料內在的結構。聚類演算法大體上可以分為：**基於距離的聚類**、**基於密度的聚類**、**層次聚類**、**譜聚類**等。如果你還分不清楚聚類和分類到底有什麼區別，相信下面的圖可以幫到你。

<img src="res/06_classification_vs_clustering.png" style="zoom:38%;">

### 演算法原理

下面我們重點為大家介紹名為 K-Means 的聚類演算法。K-Means 是一種基於原型的分割槽聚類方法，其目標是將資料集劃分為K個簇，並使每個簇內的資料點儘可能相似。K-Means 演算法的實施步驟如下所示：

1. **初始化簇中心**：隨機選擇K個樣本作為初始簇中心，簇中心通常也稱為質心。
2. **分配樣本到最近的質心**：計算每個樣本與所有質心的距離，將樣本分配到最近的簇。
3. **更新質心**：計算每個簇的所有樣本的均值，並將其作為新的質心。
4. **重複步驟2和步驟3**，直到質心收斂或達到預設的迭代次數。

### 數學描述

我們將上面的演算法原理用數學語言進行描述。對於給定的資料集 ，K-Means 演算法的目標是最小化目標函式（總誤差平方和）。目標函式如下所示：

$$
J = \sum_{i=1}^{K} \sum_{x \in C_{i}} {\lVert x - \mu_{i} \rVert}^{2}
$$

其中， $\small{K}$ 是簇的數量， $\small{C_{i}}$ 表示第 $\small{i}$ 個簇中的樣本集合， $\small{\mu_{i}}$ 是第 $\small{i}$ 個簇的中心， $\small{x}$ 是資料點。因為這個問題屬於 NP 困難組合最佳化問題，所以在實際求解時我們會採用迭代的方式來尋求滿意解。

首先隨機選擇 $\small{K}$ 個點作為初始質心 $\small{\mu_{1}, \mu_{2}, \cdots, \mu_{K}}$ ，對於每個資料點 $\small{x_{j}}$ ，計算到每個質心的距離，選擇距離最近的質心，即：

$$
C_{i} = \lbrace {x_{j} \ \vert \ {\lVert x_{j} - \mu_{i} \rVert}^{2} \le {\lVert x_{j} - \mu_{k} \rVert}^{2} \ \text{for all} \ k \ne i} \rbrace
$$

更新質心為簇中所有點的均值，即：

$$
\mu_{i} = \frac{1}{\lvert C_{i} \rvert}\sum_{x \in C_{i}} x
$$

重複上面兩個動作，直到質心不再變化或變化小於某個閾值，這就確保了演算法的收斂性。

### 程式碼實現

下面我們用 Python 程式碼實現 K-Means 聚類，我們先暫時不使用 scikit-learn 庫，主要幫助大家理解演算法的工作原理。

```python
import numpy as np


def distance(u, v, p=2):
    """計算兩個向量的距離"""
    return np.sum(np.abs(u - v) ** p) ** (1 / p)


def init_centroids(X, k):
    """隨機選擇k個質心"""
    index = np.random.choice(np.arange(len(X)), k, replace=False)
    return X[index]


def closest_centroid(sample, centroids):
    """找到跟樣本最近的質心"""
    distances = [distance(sample, centroid) for i, centroid in enumerate(centroids)]
    return np.argmin(distances)


def build_clusters(X, centroids):
    """根據質心將資料分成簇"""
    clusters = [[] for _ in range(len(centroids))]
    for i, sample in enumerate(X):
        centroid_index = closest_centroid(sample, centroids)
        clusters[centroid_index].append(i)
    return clusters


def update_centroids(X, clusters):
    """更新質心的位置"""
    return np.array([np.mean(X[cluster], axis=0) for cluster in clusters])


def make_label(X, clusters):
    """生成標籤"""
    labels = np.zeros(len(X))
    for i, cluster in enumerate(clusters):
        for j in cluster:
            labels[j] = i
    return labels


def kmeans(X, *, k, max_iter=1000, tol=1e-4):
    """KMeans聚類"""
    # 隨機選擇k個質心
    centroids = init_centroids(X, k)
    # 透過不斷的迭代對資料進行劃分
    for _ in range(max_iter):
        # 透過質心將資料劃分到不同的簇
        clusters = build_clusters(X, centroids)
        # 重新計算新的質心的位置
        new_centroids = update_centroids(X, clusters)
        # 如果質心幾乎沒有變化就提前終止迭代
        if np.allclose(new_centroids, centroids, rtol=tol):
            break
        # 記錄新的質心的位置
        centroids = new_centroids
    # 給資料生成標籤
    return make_label(X, clusters), centroids
```

我們仍然以鳶尾花資料集為例，看看我們自己實現的`kmeans`函式能否為將三種鳶尾花劃分為三個不同的類別。由於是無監督學習，這裡我們直接把整個資料集帶入`kmeans`函式，程式碼如下所示。

```python
from sklearn.datasets import load_iris

iris = load_iris()
X, y = iris.data, iris.target
labels, centers = kmeans(X, k=3)
```

這裡，千萬不要直接拿`y_pred`和`y`進行比較，我們之前說過，聚類演算法並不知道資料對應的標籤，它只是根據特徵將資料劃分為不同的類別，這裡輸出的`0`、`1`、`2` 並不直接對應到山鳶尾、多彩鳶尾和為吉尼亞鳶尾。我們可以用視覺化的方式來看看預測的結果，程式碼如下所示：

```python
import matplotlib.pyplot as plt

colors = ['#FF6969', '#050C9C', '#365E32']
markers = ['o', 'x', '^']

plt.figure(dpi=200)
for i in range(len(centers)):
    samples = X[labels == i]
    print(markers[i])
    plt.scatter(samples[:, 2], samples[:, 3], marker=markers[i], color=colors[i])
    plt.scatter(centers[i, 2], centers[i, 3], marker='*', color='r', s=120)

plt.xlabel('Petal length')
plt.ylabel('Petal width')
plt.show()
```

輸出：

<img src="res/06_kmeans_plot_1.png" style="zoom:62%;">

我們用原始資料重新輸出，跟上面的圖做一個對比，程式碼如下所示。

```python
import matplotlib.pyplot as plt

colors = ['#FF6969', '#050C9C', '#365E32']
markers = ['o', 'x', '^']

plt.figure(dpi=200)
for i in range(len(centers)):
    samples = X[y == i]
    plt.scatter(samples[:, 2], samples[:, 3], marker=markers[i], color=colors[i])

plt.xlabel('Petal length')
plt.ylabel('Petal width')
plt.show()
```

輸出：

<img src="res/06_kmeans_plot_2.png" style="zoom:62%;">

直接使用 scikit-learn 庫`cluster`模組的`KMeans`類實現 K-Means 聚類是更好的選擇，程式碼如下所示。

```python
from sklearn.cluster import KMeans

# 建立KMeans物件
km_cluster = KMeans(
    n_clusters=3,       # k值（簇的數量）
    max_iter=30,        # 最大迭代次數
    n_init=10,          # 初始質心選擇嘗試次數
    init='k-means++',   # 初始質心選擇演算法
    algorithm='elkan',  # 是否使用三角不等式最佳化
    tol=1e-4,           # 質心變化容忍度
    random_state=3      # 隨機數種子
)
# 訓練模型
km_cluster.fit(X)
print(km_cluster.labels_)           # 分簇的標籤
print(km_cluster.cluster_centers_)  # 各個質心的位置
print(km_cluster.inertia_)          # 樣本到質心的距離平方和
```

下面我們對`KMeans`類的幾個超引數加以說明：

1. `n_clusters`：指定聚類的簇數，即 $\small{K}$ 值，預設值為`8`。
2. `max_iter`：最大迭代次數，預設值為`300`，控制每次初始化中 K-Means 迭代的最大步數。
3. `init`：初始化質心的方法，預設值為`'k-means++'`，表示從資料中多次隨機選取 K 個質心，每次都計算這一次選中的中心點之間的距離，然後取距離最大的一組作為初始化中心點，推薦大家使用這個值；如果設定為`'random'`則隨機選擇初始質心。
4. `n_init`：和上面的引數配合，指定演算法執行的初始化次數，預設值為`10`。
5. `algorithm`：K-Means 的計算演算法，預設值為`'lloyd'`。還有一個可選的值為`'elkan'`，表示基於三角不等式的最佳化演算法，適用於 K 值較大的情況，計算效率較高。
6. `tol`：容忍度，控制演算法的收斂精度，預設值為`1e-4`。如果資料集較大時，可適當增大此值以加快收斂速度。

### 總結

K-Means 是一種經典的聚類演算法，它的優點包括實現簡單，演算法收斂速度快；缺點是結果不穩定（跟初始值設定有關係），無法解決樣本不均衡的問題，容易收斂到區域性最優解，受噪聲資料影響較大。如果你想透過視覺化的方式理解聚類的過程，我們給大家推薦一個名為 Naftali 的人的部落格，該網站上提供了視覺化的方式展示 K-Means 和 DBSCAN 聚類（一種基於密度的聚類演算法），如下圖所示。

<img src="res/06_kmeans_visualization.png" style="zoom:45%;">

<img src="res/06_dbscan_visualization.png" style="zoom:45%;">
