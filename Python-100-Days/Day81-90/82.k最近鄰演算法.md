## k最近鄰演算法

k 最近鄰演算法（kNN）是一種用於分類和迴歸的非引數統計方法，由美國統計學家伊芙琳·費克斯和小約瑟夫·霍奇斯於 1951 年提出。kNN 演算法的原理是從歷史資料中找到 $\small{k}$ 個跟新輸入的例項最鄰近的例項，根據它們中的多數所屬的類別來對新例項進行分類或者輸出新例項的目標值，這種演算法我們在前面已經為大家做了簡單的展示。與主流的機器學習演算法不同，k 最近鄰演算法沒有顯式的學習訓練過程，它用的是“近朱者赤，近墨者黑”這樣一種簡單樸素的思想來實現分類或迴歸。k 最近鄰演算法有兩個關鍵問題，第一個是 $\small{k}$ 值如何選擇，即用多少個最近鄰來判定新例項所屬的類別或確定其目標值；第二個是如何判定兩個例項是近還是遠，這裡就涉及到度量距離的問題。

### 距離的度量

我們可以用距離（distance）來衡量特徵空間中兩個例項之間的相似度，常用的距離度量包括閔氏距離、馬氏距離、餘弦距離、編輯距離等。閔氏距離全稱閔可夫斯基距離（Minkowski Distance），對於兩個 $\small{n}$ 維向量 $\small{\mathbf{x}=(x_{1}, x_{2}, \cdots, x_{n})}$ 和 $\small{\mathbf{y}=(y_{1}, y_{2}, \cdots, y_{n})}$ ，它們之間的距離可以定義為：

$$
d(\mathbf{x}, \mathbf{y}) = (\sum_{i=1}^{n}{\vert x_{i} - y_{i} \rvert}^{p})^{\frac{1}{p}}
$$

其中， $\small{p \ge 1}$ ，雖然 $\small{p \lt 1}$ 可以計算，但不再嚴格滿足距離的定義，通常不被視為真正的距離。

當 $\small{p = 1}$ 時，閔氏距離即**曼哈頓距離**：

$$
d(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n} \lvert x_{i} - y_{i} \rvert
$$

當 $\small{p = 2}$ 時，閔氏距離即**歐幾里得距離**：

$$
d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n}(x_{i} - y_{i})^{2}}
$$

當 $\small{p \to \infty}$ 時，閔氏距離即**切比雪夫距離**：

$$
d(\mathbf{x}, \mathbf{y}) = \underset{i}{max}(\lvert x_{i} - y_{i} \rvert)
$$

其他的距離度量方式我們等用到的時候再為大家介紹。在使用 k 最近鄰演算法做分類時，我們的資料集通常都是數值型資料，此時直接使用歐幾里得距離是一個不錯的選擇。

<img src="res/02_distance_measurement.jpeg" style="zoom:38%;">

### 資料集介紹

接下來為大家隆重介紹一下我們後續會使用到的一個重要的資料集——鳶尾花資料集（iris dataset）。鳶尾花資料集是機器學習領域中最著名、最經典的資料集之一，由植物學家 *Edgar S. Anderson* 在加拿大魁北克加斯帕半島採集，由英國統計學家 *Ronald A. Fisher* 於 1936 年在他的論文*《The Use of Multiple Measurements in Taxonomic Problems》*中首次引入，被廣泛用於機器學習演算法的入門和實驗。

<img src="res/02_iris_dataset.png" style="zoom:38%;">

鳶尾花資料集共有 150 條樣本，其中包含 3 種型別的鳶尾花，分別是山鳶尾（Iris setosa）、多彩鳶尾（Iris versicolor）和為吉尼亞鳶尾（Iris virginica），如上圖所示，每種各有50條樣本。樣本資料包含了 4 個特徵（features）和 1 個類別標籤（class label），4 個特徵分別是花萼長度（Sepal length）、花萼寬度（Sepal width）、花瓣長度（Petal length）、花瓣寬度（Petal width），都是以釐米為單位的正數，資料集中的類別標籤有 0、1、2 三個值，對應上面提到的三種鳶尾花型別。

#### 資料集的載入

我們可以透過著名的 Python 機器學習庫 scikit-learn 來載入這個資料集，scikit-learn 包含了各種分類、迴歸、聚類演算法，同時還提供了多層感知機、支援向量機、隨機森林等模型，覆蓋了從資料預處理、特徵工程到模型訓練、模型評估和引數調優等各項功能，如下圖所示。我們推薦大家使用這個庫來完成機器學習中的各種操作，scikit-learn 的[官方網站](https://scikit-learn.org/stable/)上面還提供了使用者指南、API 文件和案例等內容，有興趣的讀者可以自行訪問。

<img src="res/02_scikit-learn_introduction.png">

由於我們後續的課程基本都會用到這個庫，大家可以先透過下面的命令安裝這個庫。

```
pip install scikit-learn
```

如果你已經開啟了 IPython 或 Jupyter 但尚未安裝 scikit-learn，可以透過下面的魔法指令來安裝這個庫。

```
%pip install scikit-learn
```

在安裝完成之後，我們可以透過下面的程式碼載入鳶尾花資料集並檢視該資料集的介紹。

```Python
from sklearn.datasets import load_iris

# 載入鳶尾花資料集
iris = load_iris()
# 檢視資料集的介紹
print(iris.DESCR)
```

接下來，我們可以獲得資料集中的特徵和標籤，程式碼如下所示。

```python
# 特徵（150行4列的二維陣列，分別是花萼長、花萼寬、花瓣長、花瓣寬）
X = iris.data
# 標籤（150個元素的一維陣列，包含0、1、2三個值分別代表三種鳶尾花）
y = iris.target
```

如果希望更直觀的檢視鳶尾花資料集，我們可以用上面獲得的特徵和標籤來建立一個 DataFrame 物件，有興趣的讀者可以自己試一試。

#### 資料集的劃分

通常，我們需要將原始資料劃分成訓練集和測試集，其中訓練集是為了訓練模型選擇的資料，而測試集則是為了測試模型訓練效果保留的資料。對於上面的鳶尾花資料集，我們可以選擇 80% 的資料（120 條）作為訓練集，保留 20% 的資料（30 條）作為測試集，下面的程式碼用 NumPy 實現了對資料集的劃分。

```python
# 將特徵和標籤堆疊到同一個陣列中
data = np.hstack((X, y.reshape(-1, 1)))
# 透過隨機亂序函式將原始資料打亂
np.random.shuffle(data)
# 選擇80%的資料作為訓練集
train_size = int(y.size * 0.8)
train, test = data[:train_size], data[train_size:]
X_train, y_train = train[:, :-1], train[:, -1]
X_test, y_test = test[:, :-1], test[:, -1]
```

當然，更簡便的劃分資料集的方式是使用 scikit-learn 封裝好的函式`train_test_split`，我們只需要傳入特徵和標籤，指定好`train_size`或`test_size`就可以實現同樣的操作。`train_test_split`函式返回一個四元組，四個元素分別代表用於訓練的特徵、用於訓練的標籤、用於測試的特徵和用於測試的標籤。

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=3)
```

> **說明**：上面程式碼中`train_test_split`函式的`random_state`引數可以理解成隨機數的種子，如果你使用跟我相同的隨機數種子，那麼我們劃分的訓練集和測試集也是完全相同的。

### kNN分類的實現

下面我們先不用 scikit-learn 而是用基礎的資料科學庫 NumPy 和 SciPy 來實現 kNN 演算法，這樣做的目的是幫助大家更好的理解演算法原理，在此基礎上我們再感受 scikit-learn 的強大並明白其中類、函式、引數等為什麼要如此設定。

#### 基於NumPy的實現

k 最近鄰演算法需要計算距離，下面我們先設計計算兩個資料點歐式距離的函式，程式碼如下所示。

```python
import numpy as np


def euclidean_distance(u, v):
    """計算兩個n維向量的歐式距離"""
    return np.sqrt(np.sum(np.abs(u - v) ** 2))
```

接下來，我們設計根據鄰居的標籤為新資料生成標籤的函式。

```python
from scipy import stats


def make_label(X_train, y_train, X_one, k):
    """
    根據歷史資料中k個最近鄰為新資料生成標籤
    :param X_train: 訓練集中的特徵
    :param y_train: 訓練集中的標籤
    :param X_one: 待預測的樣本（新資料）特徵
    :param k: 鄰居的數量
    :return: 為待預測樣本生成的標籤（鄰居標籤的眾數）
    """
    # 計算x跟每個訓練樣本的距離
    distes = [euclidean_distance(X_one, X_i) for X_i in X_train]
    # 透過一次劃分找到k個最小距離對應的索引並獲取到相應的標籤
    labels = y_train[np.argpartition(distes, k - 1)[:k]]
    # 獲取標籤的眾數
    return stats.mode(labels).mode
```

> **說明**：`np.partition`函式可以對陣列進行一次劃分，將 k 個比較小的元素放在陣列的左邊，n - k 個比較大的元素放在陣列的右邊，跟快速排序演算法中做一次劃分操作的效果是一樣的。上面程式碼中的`np.argpartition`函式是把 k 個較小元素的索引放在陣列的左邊，這樣經過`[:k]`切片操作和對`y_train`的花式索引運算，就可以獲得 k 個跟新資料距離較小的樣本對應的標籤。最後，我們透過 scipy 庫的 stats 模組的`mode`函式可以獲得標籤的眾數，並用它作為我們給新資料預測的類別標籤。

在完成上述準備工作後，用 k 最近鄰做預測的函式就呼之欲出了，程式碼如下所示。

```python
def predict_by_knn(X_train, y_train, X_new, k=5):
    """
    KNN演算法
    :param X_train: 訓練集中的特徵
    :param y_train: 訓練集中的標籤
    :param X_new: 待預測的樣本構成的陣列
    :param k: 鄰居的數量（預設值為5）
    :return: 儲存預測結果（標籤）的陣列
    """
    return np.array([make_label(X_train, y_train, X, k) for X in X_new])
```

我們用上面準備好的鳶尾花資料的訓練集和測試集來做實驗，看看我們的`predict_by_knn`函式能否很好的運轉起來。下面程式碼中的`y_pred`是我們透過函式預測的30條鳶尾花的型別，我們跟`y_test`做一個比較，就可以看到我們的預測效果。

```python
y_pred = predict_by_knn(X_train, y_train, X_test)
y_pred == y_test
```

我這裡的輸出是：

```
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True, False,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True])
```

輸出的結果中有一個`False`，這表示預測的標籤跟真實的標籤並不相同，是一個錯誤的預測結果，也就是說我們預測的準確率為 $\small{\frac{29}{30}}$ ，即`96.67%`。當然，如果你劃分訓練集和測試集時跟我指定的`random_state`引數不相同，這裡得到的結果可能會跟我不同。

#### 基於scikit-learn的實現

使用 scikit-learn 來實現 kNN 演算法就要簡單許多了，我們基本上只需要做三個動作就可以得到想要的結果，程式碼如下所示。

```python
from sklearn.neighbors import KNeighborsClassifier

# 建立模型
model = KNeighborsClassifier()
# 訓練模型
model.fit(X_train, y_train)
# 預測結果
y_pred = model.predict(X_test)
```

我們來看看輸出的結果，是不是跟我們自己手撕的程式碼完全一致。

```python
y_pred == y_test
```

輸出：

```
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True, False,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True])
```

當然，scikit-learn 庫考慮到的東西肯定比我們自己手撕的程式碼豐富得多，例如我們想知道模型預測的準確率（accuracy），可以使用下面的函式。

```python
model.score(X_test, y_test)
```

輸出：

```
0.9666666666666667
```

### 模型評估

當然，評價一個分類器的預測效果是否良好，不能只簡單的看一下準確率，因為在類別不平衡的情況下，準確率很可能會誤導你的判斷。例如，我們的測試資料中維吉尼亞鳶尾花的樣本數量非常小，那麼即便我們的模型根本無法判斷維吉尼亞鳶尾花，模型也會表現出很高的準確率。所以，我們還要考察查準率（精確率）、查全率（召回率）、F1 分數等指標，而**混淆矩陣**則是用於詳細展示分類模型效能的工具，適用於二分類和多分類任務，二分類問題的混淆矩陣如下所示。

|      | **預測為正類（Positive）** | **預測為負類（Negative）** |
| ---- | ------------------------- | ------------------------- |
| **實際為正類（Positive）** | True Positive（TP） | False Negative（FN） |
| **實際為負類（Negative）** | False Positive（FP） | True Negative（TN） |

我們舉一個例子來說明如何使用混淆矩陣並計算出分類模型的評估指標。假設某種醫學檢測系統的目標預測是否患有某種疾病，類別“正類”表示“患病”，類別“負類”表示“未患病”，1000個樣本的測試結果如下所示。

|      | **預測為患病** | **預測為未患病** |
| ---- | -------------- | ---------------- |
| **實際為患病** | 80（TP） | 20（FN） |
| **實際為未患病** | 30（FP） | 870（TN） |

1. **準確率**（Accuracy）。

$$
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}}
$$

上面的例子，模型預測的準確率為： $\frac{80 + 870}{80 + 30 + 20 + 870} = \frac{950}{1000} = 0.95$ 。

2. **精確率**（Precesion）。精確率用於衡量在所有被預測為正類的樣本中，實際上屬於正類的比例，通常也被稱為查準率。

$$
\text{Precesion} = \frac{\text{TP}}{\text{TP} + \text{FP}}
$$

上面的例子，模型預測的精確率為： $\frac{80}{80 + 30} = \frac{80}{110} = 0.73$ 。

3. **召回率**（Recall）。召回率用於衡量在所有實際為正類的樣本中，被模型正確預測為正類的比例，通常也被稱為查全率或真正例率（True Positive Rate）。
$$
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
$$

上面的例子，模型預測的召回率為： $\frac{80}{80 + 20} = \frac{80}{100} = 0.8$ 。

4. **F1 分數**（F1 Score）。F1 分數是精確率和召回率的調和平均數，它在精確率和召回率之間尋求一個平衡，尤其適用於在兩者之間有權衡的情況。
  
$$
\text{F1 Score} = \frac{2}{\frac{1}{\text{Precision}} + \frac{1}{\text{Recall}}} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precesion} + \text{Recall}}
$$

上面的例子，模型預測的F1 分數為： $2 \times \frac{0.7273 * 0.8}{0.7273 + 0.8} = 0.76$ 。

5. **特異度**（Specificity）和**假正例率**（False Positive Rate，簡稱 FPR）。特異度用於衡量的是在所有實際為負類的樣本中，被模型正確預測為負類的比例，類似於召回率，只不過針對的是負類樣本。

$$
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}} \\\\
\text{FPR} = 1 - \text{Specificity}
$$

上面的例子，模型預測的特異度為： $\frac{870}{870 + 30} = \frac{870}{900} = 0.97$ 。

6. **ROC** 和 **AUC**。

    - **ROC**（Receiver Operating Characteristic Curve）繪製了召回率與假正例率的關係，如下圖所示。

        <img src="res/02_ROC_curve.png" style="zoom:38%;">

    - **AUC**（Area Under the Curve）是 ROC 曲線下的面積，衡量模型區分正類和負類的能力。AUC 值的範圍是 $\small[0, 1]$ ，值越接近 1，表示模型區分正負類的能力越強。0.5 < AUC < 1，說明我們的模型優於隨機猜測，只要這個分類器（模型）妥善設定閾值的話，就有預測價值。AUC = 0.5，說明我們的模型跟隨機猜測一樣，模型沒有預測價值。AUC < 0.5，模型比隨機猜測還差，但只要總能反向預測，它的實際效果就優於隨機猜測。

對於多分類問題，混淆矩陣的行數和列數都等於類別數，混淆矩陣是一個方陣，也就是說如果有 $\small{n}$ 個類別，那麼混淆矩陣就是一個 $\small{n \times n}$ 的方陣。根據上面我們得到的鳶尾花資料集的預測結果，我們先輸出真實值和預測值，然後製作對應的混淆矩陣，如下所示。

```python
print(y_test)
print(y_pred)
```

輸出：

```
[0 0 0 0 0 2 1 0 2 1 1 0 1 1 2 0 1 2 2 0 2 2 2 1 0 2 2 1 1 1]
[0 0 0 0 0 2 1 0 2 1 1 0 1 1 2 0 2 2 2 0 2 2 2 1 0 2 2 1 1 1]
```

|      | **預測為類別0（山鳶尾）** | **預測為類別1（多彩鳶尾）** | **預測為類別2（為吉尼亞鳶尾）** |
| :---- | :---------------: | :---------------: | :---------------: |
| **實際為類別0（山鳶尾）** | 10 | 0 | 0 |
| **實際為類別1（多彩鳶尾）** | 0 | 9 | 1 |
| **實際為類別2（吉尼亞鳶尾）** | 0 | 0 | 10 |


我們可以用 scikit-learn 中的`confusion_matrix`和`classification_report`函式來輸出混淆矩陣和評估報告，程式碼如下所示。

```python
from sklearn.metrics import classification_report, confusion_matrix

# 輸出分類模型混淆矩陣
print('混淆矩陣: ')
print(confusion_matrix(y_test, y_pred))
# 輸出分類模型評估報告
print('評估報告: ')
print(classification_report(y_test, y_pred))
```

輸出：

```
混淆矩陣: 
[[10  0  0]
 [ 0  9  1]
 [ 0  0 10]]
評估報告: 
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        10
           1       1.00      0.90      0.95        10
           2       0.91      1.00      0.95        10

    accuracy                           0.97        30
   macro avg       0.97      0.97      0.97        30
weighted avg       0.97      0.97      0.97        30
```

如果希望用視覺化的方式輸出混淆矩陣，可以使用如下所示的程式碼。

```python
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

# 建立混淆矩陣顯示物件
cm_display_obj = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=iris.target_names)
# 繪製並顯示混淆矩陣
cm_display_obj.plot(cmap=plt.cm.Reds)
plt.show()
```

> **說明**：其中的`iris.target_names`就是類別標籤`0`、`1`、`2`對應的三種鳶尾花的英文名。

<img src="res/02_confusion_matrix.png" style="zoom:85%;">

對於二分類問題，如果想要繪製出 ROC 曲線並顯示 AUC 值，可以使用如下所示的程式碼。

```python
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import RocCurveDisplay

# 手動構造一組真實值和對應的預測值
y_test_ex = np.array([0, 0, 0, 1, 1, 0, 1, 1, 1, 0])
y_pred_ex = np.array([1, 0, 0, 1, 1, 0, 1, 1, 0, 1])
# 透過roc_curve函式計算出FPR（假正例率）和TPR（真正例率）
fpr, tpr, _ = roc_curve(y_test_ex, y_pred_ex)
# 透過auc函式計算出AUC值並透過RocCurveDisplay類繪製圖形
RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc(fpr, tpr)).plot()
plt.show()
```

<img src="res/02_ROC_plot.png" style="zoom:92%;">

### 引數調優

之前我們說過，kNN 演算法有兩個關鍵問題，一是距離的度量，二是 k 值的選擇。我們使用 scikit-learn 的`KNeighborsClassifier`建立分類器模型時，可以對模型的超引數進行設定，這裡有幾個比較重要的引數：

1. `n_neighbors`：近鄰的數量，就是 kNN 演算法中 k 的值。
2. `weights`：可以選擇`uniform`或`distance`，前者表示所有樣本的權重相同，後者表示距離越近權重越高，預設值是`uniform`。當然，我們也可以透過傳入自定義的函式來確定每個樣本的權重。
3. `algorithm`：有`auto`、`ball_tree`、`kd_tree`、`brute`四個選項，預設值為`auto`。其中`ball_tree`是一種樹形結構，基於球體劃分的方法將資料點分配到層次化的樹結構中，在高維資料和稀疏資料場景下有較好的效能；`kd_tree`也是一種樹形結構，透過選擇一個維度將空間劃分為若干個子區域再進行搜尋，從而避免跟所有的鄰居進行比較，對於低維度和空間分佈均勻的資料，後者有較好的效果，在高維空間中會遇到的維度災難問題；`auto`選項是根據輸入資料的維度自動選擇`ball_tree`或`kd_tree`；`brute`選項則是使用暴力搜尋演算法（窮舉法），再處理小資料集時，它是一個簡單且有效的選擇。
4. `leaf_size`：使用`ball_tree`或`kd_tree`演算法時，該引數用於限制樹結構葉子節點最大樣本數量，預設值為`30`，該引數會影響樹的構建和節點查詢的效能。
5. `p`：閔可夫斯基距離公式中的`p`，預設值為2，計算歐氏距離。

我們可以使用**網格搜尋**（Grid Search）和**交叉驗證**（Cross Validation）的方式對模型的超引數進行調整，評估模型的泛化能力，提升模型的預測效果。網格搜尋就是透過窮舉法遍歷給定的超引數空間，找到最優的超引數組合；交叉驗證則是將訓練集分成多個子集，透過在不同的訓練集和驗證集上進行多次訓練和評估，對模型的預測效果進行綜合評判。K-Fold交叉驗證是最常用的交叉驗證方法，透過將資料集劃分為 K 個子集，每次選取其中一個子集作為驗證集，剩下的 K-1 個子集作為訓練集，對每個子集重複這個過程，完成 K 次訓練和評估並將平均值作為模型的最終效能評估，如下圖所示。

![](res/02_k-fold_cross_validation.png)

下面的程式碼了 scikit-learn 庫中的`GridSearchCV`來做網格搜尋和交叉驗證，透過這種方式找到針對鳶尾花資料集實施 kNN 演算法的最優引數。

```python
from sklearn.model_selection import GridSearchCV

# 網格搜尋交叉驗證
gs = GridSearchCV(
    estimator=KNeighborsClassifier(),
    param_grid={
        'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15],
        'weights': ['uniform', 'distance'],
        'p': [1, 2]
    },
    cv=5
)
gs.fit(X_train, y_train)
```

可以透過下面的程式碼獲得最優引數及其評分。

```python
print('最優引數:', gs.best_params_)
print('評分:', gs.best_score_)
```

輸出：

```
最優引數: {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
評分: 0.9666666666666666
```

如果需要使用訓練好的最優模型進行預測，可以按照如下所示的程式碼進行操作。

```python
gs.predict(X_test)
```

### kNN迴歸

kNN 演算法通常用於解決分類問題，當然它也可以用於解決迴歸問題，其基本思想也是透過找出跟新例項最近的 k 個鄰居，然後根據這 k 個鄰居的標籤（在迴歸問題中我們通常稱之為目標值）來預測新例項的目標值。由於我們需要預測的不是類別標籤而是一個數值，所以通常會使用平均或加權平均的方式來處理 k 個鄰居的目標值，從而獲得對新例項目標值的預測。讓我們回到上一課用月收入預測網購支出的那個例子，然後使用 scikit-learn 中的`KNeighborsRegressor`來構建迴歸模型，我們先做一些準備工作，程式碼如下所示。

```python
# 每月收入
incomes = np.array([
    9558, 8835, 9313, 14990, 5564, 11227, 11806, 10242, 11999, 11630,
    6906, 13850, 7483, 8090, 9465, 9938, 11414, 3200, 10731, 19880,
    15500, 10343, 11100, 10020, 7587, 6120, 5386, 12038, 13360, 10885,
    17010, 9247, 13050, 6691, 7890, 9070, 16899, 8975, 8650, 9100,
    10990, 9184, 4811, 14890, 11313, 12547, 8300, 12400, 9853, 12890
])
# 每月網購支出
outcomes = np.array([
    3171, 2183, 3091, 5928, 182, 4373, 5297, 3788, 5282, 4166,
    1674, 5045, 1617, 1707, 3096, 3407, 4674, 361, 3599, 6584,
    6356, 3859, 4519, 3352, 1634, 1032, 1106, 4951, 5309, 3800,
    5672, 2901, 5439, 1478, 1424, 2777, 5682, 2554, 2117, 2845,
    3867, 2962,  882, 5435, 4174, 4948, 2376, 4987, 3329, 5002
])
X = np.sort(incomes).reshape(-1, 1)  # 將收入排序後處理成二維陣列
y = outcomes[np.argsort(incomes)]    # 將網購支出按照收入進行排序
```

> **說明**：即便只有一個特徵，我們也需要將自變數處理二維陣列的形式，因為我們訓練模型時使用的`fit`方法並不能接受一維陣列作為它的第一個引數。這裡提前對 X 和 y 排序是為了待會繪製散點圖和折線圖的時候根據 X 的值從小到大依次繪製。

接下來，我們建立 kNN 迴歸模型。

```python
from sklearn.neighbors import KNeighborsRegressor

# 建立模型
model = KNeighborsRegressor()
# 訓練模型
model.fit(X, y)
# 預測結果
y_pred = model.predict(X)
```

上面我們直接用所有的歷史資料進行模型訓練，然後透過繪製下面的圖形來看看預測效果到底如何。

```python
# 原始資料散點圖
plt.scatter(X, y, color='navy')
# 預測結果折線圖
plt.plot(X, y_pred, color='coral')
plt.show()
```

<img src="res/02_knn_regression_plot.png" style="zoom:85%;">

kNN 迴歸模型計算複雜度較高，而且對噪聲資料非常敏感，在很多時候它可能並不是一個很好的選擇。當然，如何評估一個迴歸模型的效果，這個問題我們留到後續的章節再為大家進行講解。

### 總結

kNN 演算法是一種簡單但不失強大的機器學習演算法，適用於小資料集上的分類和迴歸任務，它的優點是簡單易懂，沒有顯示的訓練過程，不依賴於對資料分佈的假設，可以適應複雜的資料模式。當然，kNN 演算法的缺點也非常明顯，最大的問題在於計算效率，所以在資料集較大時可能並不是最好的選擇。除此以外，kNN 演算法對噪聲非常敏感，同時預測結果依賴於 k 的取值和樣本是否均衡（各個類別的樣本數量是否大致相等），k 值過小可能導致過擬合，k值過大又可能導致欠擬合，而不均衡的樣本可能會導致類別偏倚（Class Imbalance Bias），也就是說，如果大部分訓練樣本都屬於類別 A，而類別 B 樣本較少，那麼在分類時，KNN 更容易將測試樣本預測為類別 A。
