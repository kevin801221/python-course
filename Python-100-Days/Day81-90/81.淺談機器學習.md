## 淺談機器學習

人工智慧無疑是最近幾年熱度極高的一個詞，從2016年穀歌 DeepMind 團隊開發的 AlphaGo 圍棋程式戰勝人類頂尖棋手，到2017年基於 Transformer 架構的 NLP 模型釋出，再到2023年 OpenAI 推出基於 GPT-4 的 ChatGPT 以及人工智慧在醫療、自動駕駛等領域的深度應用，人工智慧的熱潮到達了自1956年達特茅斯會議以來前所未有的高度，可以說幾乎每個人的生活都或多或少的受到了人工智慧的影響。人工智慧是電腦科學的一個重要分支，涉及計算機模擬智慧行為的能力以及機器模仿人類智慧行為的能力。研究人工智慧的主要目標是開發出能夠獨立做出決策的系統，從而在醫療、工程、金融、教育、科研、公共服務等諸多領域幫助人類更高效的工作。人工智慧的英文是“*artificial intelligence*”，因此通常被簡稱為 *AI*，人工智慧包含了諸多的內容，我們經常說到的機器學習、深度學習、自然語言處理、計算機視覺、強化學習、資料探勘、專家系統、工業機器人、自動駕駛等都屬於人工智慧的範疇。狹義的人工智慧通常只能執行特定的任務，會聊天的人工智慧通常不會開車，會開車的人工智慧通常不會下棋；廣義的人工智慧需要具備通用智慧，能夠執行任何人類智慧可以執行的任務；而更進一步的能夠超越人類智慧的人工智慧，我們稱之為超人工智慧。

本課程我們主要探討人工智慧中的機器學習（Machine Learning）。機器學習是人工智慧的一個子領域，關注如何透過資料和演算法來使計算機系統從經驗中學習並進行預測或決策。簡單的說，機器學習是實現人工智慧的一種方法，有很多 AI 系統都是透過機器學習技術開發的。在一些特定場景，人們也用資料探勘（Data Mining）這個詞來指代機器學習，所謂的資料探勘就是從資料中提取有用的資訊和知識，分析和解釋資料中的模式和趨勢，最終達成預測未來趨勢和行為的目標。當然，我們在提到這兩個詞的時候，表達的側重點還是有所區別，資料探勘主要關注知識發現，而機器學習側重於構建和最佳化預測模型。當下，還有一個非常熱門的概念和研究領域叫深度學習（Deep Learning），它是機器學習的一個子領域，特別側重於使用多層神經網路（深度神經網路）來進行資料處理和學習。深度學習在處理影象、語音和自然語言等複雜資料時表現出色，能夠自動學習資料的層次化特徵，從而降低人工干預的需求。當然，深度學習模型通常比傳統的機器學習模型更復雜，且需要更多的資料和計算資源。

### 人工智慧發展史

治學先治史，我們簡單回顧一下人工智慧發展史上的里程碑事件，對於比較久遠的歷史，我們只簡單提一些重要的時間節點，我們把重點放在最近幾年的重大事件上。

![](res/01_AI_history.jpg)

> **說明**：上圖來自於[AMiner網站](https://www.aminer.cn/ai-history)，按照該網站的要求，在引用上圖需要引用下面的論文。需要高畫質原圖的，也可以在該網站上獲取。
>
> Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. [ArnetMiner: Extraction and Mining of Academic Social Networks.](https://www.aminer.cn/pub/53e9a5afb7602d9702edacce/arnetminer-extraction-and-mining-of-academic-social-networks) In Proceedings of the Fourteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD'2008). pp.990-998.

1. 1950年，艾倫·圖靈發表了一篇劃時代的論文，文中預言了創造出具有智慧的機器的可能性並提出了著名的圖靈測試。圖靈測試旨在透過計算機與人類進行對話，如果計算機能夠使人類無法區分回答問題的是機器還是人類，就認為該計算機具有智慧。
2. 1956年，達特茅斯會議召開，該會議被認為是人工智慧的誕生標誌。會議上約翰·麥卡錫、馬文·明斯基、克勞德·夏農等人提出了人工智慧的研究議程。達特茅斯會議的幾位主要參與者也被後人譽為“人工智慧七俠”，包括約翰·麥卡錫、馬文·明斯基、阿倫·紐厄爾、赫伯特·西蒙、克勞德·夏農、奧利弗·塞爾弗裡奇和內森·羅切斯特，他們為人工智慧的發展奠定了基礎，其中有五個人獲得了圖靈獎。
3. 1957年，弗蘭克·羅森布拉特提出了感知機模型，這是最早的神經網路模型之一，用於解決二分類問題。
4. 1958年，約翰·麥卡錫開發了 LISP 程式語言，這種語言特別適合於符號處理，成為早期 AI 研究的主要程式語言。
5. 1980年，卡內基梅隆大學為 DEC 公司設計了一個名為 XCON 的專家系統，每年為公司省下約四千萬美元的開銷。由於專家系統在 DEC 公司取得了巨大的成功，全世界有很多公司都開始研發和應用專家系統。
6. 1982年，物理學家約翰·霍普菲爾德證明一種新型的神經網路（後被稱為“Hopfield網路”）能夠用一種全新的方式學習和處理資訊。
7. 1986年，傑弗裡·辛頓及其團隊提出了反向傳播演算法，解決了訓練深層神經網路的難題，神經網路相關的方法論再次受到關注。這一突破使得神經網路在各類任務中表現優異，成為機器學習的重要工具。
8. 1997年，IBM 開發的國際象棋程式 Deep Blue 戰勝了國際象棋世界冠軍傳奇棋手卡斯帕羅夫。
9. 2011年，IBM 開發的能夠使用自然語言來回答問題的人工智慧系統在綜藝節目《危險邊緣》中打敗了最高獎金得主布拉德·魯特爾和連勝紀錄保持者肯·詹寧斯，贏得了100萬美元的獎金。
10. 2012年，傑弗裡·辛頓和他的學生在 ImageNet 競賽中使用深度卷積神經網路（CNN）獲得了巨大成功，標誌著深度學習的興起。隨後，深度學習在影象識別、語音識別、自然語言處理等領域取得了顯著進展。
11. 2016年，谷歌 DeepMind開發的 AlphaGo 戰勝了圍棋世界冠軍李世乭，展示了深度學習和蒙特卡洛樹搜尋技術的強大能力。
12. 2017年，谷歌的研究人員提出了 Transformer 模型，徹底改變了自然語言處理領域。Transformer 模型基於自注意力機制，大大提高了序列建模的效率和效果。後面的 BERT、GPT 等模型都是基於 Transformer 架構。
13. 2018年，OpenAI 開發的 AI 系統 OpenAI Five 在多人線上戰術競技遊戲《Dota 2》中擊敗了半專業和專業玩家團隊。
14. 2019年，OpenAI 釋出了 GPT-2，一個具有15億引數的語言模型，展示了生成高質量自然語言文字的能力。同年，DeepMind 開發的 AlphaStar 在實時戰略遊戲《星際爭霸II》中擊敗了人類頂級選手。
15. 2020年，OpenAI 釋出了 GPT-3，一個具有1750億引數的語言模型，是當時最大的語言模型，GPT-3 在生成自然語言文字、翻譯、編寫程式碼等任務中表現出色，推動了自然語言處理和生成模型的進一步發展。同年，DeepMind 的 AlphaFold 2 在蛋白質結構預測的 CASP 競賽中取得了突破性成果，預測精度達到了實驗級別，這一成就被認為是生物學領域的重大突破，有望推動藥物研發和生物學研究的進步。
16. 2021年，OpenAI 釋出了 DALL-E，這個模型能夠根據文字描述生成影象，展示了生成模型在多模態學習和影象生成中的潛力。同年，OpenAI 釋出了 Codex，一個可以理解和生成程式碼的語言模型，是 Copilot 的核心技術。
17. 2023年，OpenAI 推出了基於 GPT-4 的 ChatGPT，可以進行更自然、更流暢的對話。隨後，國內外行業巨頭紛紛入局大模型，各種各樣的大模型應用遍地開花。
18. 2024年，OpenAI 在 DALL-E 的基礎上推出了 Sora，一個可以透過文字描述生成影片的人工智慧模型。幾天後，一家名為 Cognition AI 的初創公司推出了第一位 AI 軟體工程師 Devin。 

人工智慧自圖靈測試和達特茅斯會議以來，經歷了多次的高潮和低谷。目前，受益於由於計算機運算和儲存能力的飛速提升，從前很多難以實現的技術都成為了現實，而人工智慧也又一次被推到了風口浪尖。從早期的專家系統到今天的大模型，AI 技術還在不斷的演進和突破。無論是 AlphaGo 攻佔了人類智慧的最後高地，還是自動駕駛技術的逐漸普及，又或是人工智慧內容生成（AIGC）的廣泛應用，你可以清楚的感受到，人工智慧正在改變我們的生活。

### 什麼是機器學習

人類透過記憶和歸納這兩種方式進行學習，透過記憶可以積累單個事實，使用歸納可以從舊的事實推匯出新的事實。機器學習是賦予機器從資料中學習知識的能力，這個過程並不需要人類的幫助（給出明確的規則），也就是說機器學習關注的是從資料中學習一種模式（pattern），即便資料本身存在問題（噪聲），這也是機器學習演算法和傳統演算法最根本的區別。傳統的演算法需要計算機被告知如何從複雜系統中找到答案，演算法利用計算機的運算能力去尋找最佳結果。傳統演算法最大的缺點就是人類必須首先知道最佳的解決方案是什麼，而機器學習演算法並不需要人類告訴模型最佳解決方案，取而代之的是，我們提供和問題相關的示例資料。

我們可以把需要計算機解決的問題分為四類，分類的依據一方面是輸入是精確的還是模糊的，另一方面是輸出是最優的還是滿意的，我們可以製作出如下所示的表格。可以看出，傳統演算法擅長解決的只有第一類問題，而機器學習比較擅長解決第三類和第四類問題；第二類問題基本屬於 NP 問題（非確定性多項式時間問題），包括旅行經銷商問題、圖著色問題、集合覆蓋問題等，我們通常會採用啟發式演算法（如模擬退火演算法、遺傳演算法等）或近似演算法來解決這類問題，當然機器學習演算法也可以為這類問題提供滿意解。

<img src="res/01_four_kinds_problems.png" style="zoom:45%;">

當然，機器學習演算法並非完美到無懈可擊。在透過資料訓練模型時，我們需要使用預處理和清洗後的資料，如果資料本身質量非常糟糕，我們也很難訓練出一個好的模型，這也是我們經常說到的 GIGO（Garbage In Garbage Out）。如果要使用機器學習，我們還得確定變數之間是否存在某種關係，機器學習無法處理不存在任何關係的資料。大多數時候，機器學習模型輸出的是一系列的數字和指標，需要人類解讀這些數字和指標並做出決策，判定模型的好壞並決定模型如何在實際的業務場景中落地。通常，我們用來訓練模型的資料會存在噪聲資料（noisy data），很多機器學習模型對噪聲都非常敏感，如果不能處理好這些噪聲資料，我們就不太容易得到好的模型。

### 機器學習的應用領域

即使對於機器學習這個概念不那麼熟悉，但是機器學習的成果已經廣泛滲透到了生產生活的各個領域，下面的這些場景對於你來說一定不陌生。

**場景1**：搜尋引擎會根據搜尋和使用習慣，最佳化下一次搜尋的結果。

**場景2**：電商網站會根據你的訪問歷史自動推薦你可能感興趣的商品。

**場景3**：金融類產品會透過你最近的金融活動資訊綜合評定你的貸款申請。

**場景4**：影片和直播平臺會自動識別圖片和影片中有沒有不和諧的內容。

**場景5**：智慧家電和智慧汽車會根據你的語音指令做出相應的動作。

簡單的總結一下，機器學習可以應用到但不限於以下領域：

#### 1. **影象識別與計算機視覺**

計算機視覺（Computer Vision）是機器學習的一個重要應用領域，涉及到使機器能夠理解和處理影象和影片資料。

- **人臉識別**：透過深度學習模型識別圖片中的人臉，如安全監控、手機解鎖等。
- **自動駕駛**：自動駕駛汽車使用計算機視覺來識別道路標誌、行人、其他車輛、交通訊號燈等，從而實現自主導航。
- **醫療影像分析**：機器學習應用於X光片、MRI掃描和CT影象的分析，幫助醫生髮現疾病（如癌症、腦卒中等）。
- **影象分類與標註**：自動為影象打標籤，比如在社交媒體平臺上自動識別圖片中的物品和場景。

#### 2. **自然語言處理（NLP）**

自然語言處理是機器學習在文字和語音資料上的應用，目的是讓計算機能夠理解和生成自然語言。

- **語音識別**：如智慧助手（Siri、Google Assistant）透過語音識別技術將語音轉換為文字。
- **機器翻譯**：Google翻譯、百度翻譯等應用，使用機器學習技術進行語言之間的自動翻譯。
- **情感分析**：分析社交媒體帖子、評論等文字資料的情感傾向（積極、消極、中立）。
- **文字生成**：自動生成文章或新聞（如GPT系列），為使用者提供文章自動寫作、智慧客服等功能。
- **聊天機器人**：例如客服機器人，透過自然語言處理技術與使用者進行對話。

#### 3. **推薦系統**

推薦系統利用使用者行為資料來預測使用者可能感興趣的物品或服務。

- **電子商務**：如亞馬遜、淘寶等平臺根據使用者的瀏覽和購買記錄推薦商品。
- **影視推薦**：如Netflix、YouTube等根據使用者觀看歷史推薦電影、影片和節目。
- **社交網路推薦**：例如，Facebook和Twitter根據使用者的興趣推薦朋友、帖子或廣告。

#### 4. **金融領域**

機器學習在金融領域的應用主要體現在風險管理、預測和自動化交易等方面。

- **信用評分**：銀行和金融機構利用機器學習模型評估借款人的信用風險。
- **欺詐檢測**：透過分析交易模式，機器學習模型能夠檢測到潛在的欺詐行為（如信用卡欺詐、洗錢行為）。
- **演算法交易**：利用機器學習演算法進行股票和其他金融資產的自動化交易，實時根據市場資料進行決策。
- **投資組合管理**：使用機器學習模型進行資產配置和投資組合最佳化。

#### 5. **醫療健康**

機器學習在醫療行業中被廣泛應用，尤其是在疾病預測、個性化治療和醫療影象分析等方面。

- **疾病預測**：使用機器學習演算法預測疾病的發生機率，例如糖尿病、心臟病等。
- **個性化醫療**：基於患者的歷史健康資料、基因資料等，機器學習可以幫助提供個性化的治療方案。
- **藥物發現**：透過大資料分析，機器學習能夠加速新藥的發現過程，例如透過預測化合物的藥效來篩選潛在藥物。
- **健康監測**：利用可穿戴裝置（如智慧手錶、健康追蹤器）收集的資料，機器學習可以監測健康狀況並預測疾病風險。

#### 6. **智慧交通與自動駕駛**

自動駕駛技術和智慧交通系統都依賴於機器學習技術。

- **自動駕駛汽車**：自動駕駛依賴於機器學習來識別周圍環境（如行人、交通訊號、其他車輛等），並作出決策。
- **交通預測**：根據交通流量、天氣、節假日等因素，機器學習可以預測路況、交通擁堵情況，最佳化路線規劃。
- **車聯網（V2X）**：車與車、車與基礎設施之間的通訊系統，利用機器學習進行資料分析和決策。

#### 7. **智慧家居與物聯網**

物聯網（IoT）裝置可以透過機器學習實現自動化和智慧化操作。

- **智慧家居**：如智慧音響（Amazon Echo、Google Home）和智慧家電（智慧空調、智慧冰箱等），透過語音和感測器資料自動調整裝置設定。
- **環境監控**：基於機器學習的感測器資料分析，可以監測室內空氣質量、溫度等引數，並自動調節環境條件。
- **智慧安防**：監控攝像頭透過人臉識別和行為識別技術，識別潛在的安全威脅。

#### 8. **體育分析**

機器學習也在體育領域得到應用，特別是在資料分析和比賽策略最佳化方面。

- **運動員表現分析**：透過分析運動員的訓練資料、比賽資料等，機器學習可以幫助教練制定個性化的訓練計劃。
- **比賽結果預測**：透過歷史資料和實時資料，機器學習可以預測比賽的結果、球隊表現等。
- **虛擬體育教練**：利用機器學習技術為運動員提供資料驅動的訓練建議和反饋。

#### 9. **製造與工業自動化**

機器學習在工業和製造領域主要用於最佳化生產流程、提高效率和減少故障。

- **預測性維護**：機器學習模型透過分析裝置的歷史資料，預測裝置可能發生故障的時間，從而提前進行維護。
- **質量控制**：透過對生產資料進行分析，機器學習可以自動識別生產過程中出現的質量問題。
- **自動化生產**：機器學習可以幫助機器人根據環境和任務自動調節生產流程，提高生產效率和質量。

### 機器學習的分類

機器學習模型可以按照不同的標準進行分類，主要包括以下幾種：

#### 按照學習方式分類

<img src="res/01_machine_learning_category.png" style="zoom:32%;">

1. 監督學習（Supervised Learning）
   - 迴歸（Regression）：用於預測連續值的模型，例如線性迴歸、Ridge 迴歸、Lasso 迴歸等。
   - 分類（Classification）：用於預測離散類別的模型，例如邏輯迴歸、支援向量機、決策樹、隨機森林、k近鄰、樸素貝葉斯等。

2. 無監督學習（Unsupervised Learning）
   - 聚類（Clustering）：用於將資料分組的模型，例如K均值聚類、層次聚類、DBSCAN 等。
   - 降維（Dimensionality Reduction）：用於減少特徵數量的模型，例如主成分分析（PCA）、線性判別分析（LDA）、t分佈隨機近鄰嵌入（t-SNE）等。
   - 關聯規則學習（Association Rule Learning）：用於發現資料集中項之間關係的模型，例如 Apriori 演算法、Eclat 演算法等。

3. 半監督學習（Semi-Supervised Learning）
   - 結合了監督學習和無監督學習的方法，使用大量未標記的資料和少量標記的資料來構建模型。

4. 強化學習（Reinforcement Learning）
   - 基於獎勵機制的學習方法，例如Q-Learning、深度Q網路（DQN）、策略梯度方法等。

#### 按照模型的複雜度分類

1. 線性模型（Linear Models）：如線性迴歸、邏輯迴歸。
2. 非線性模型（Non-linear Models）：如帶核函式的支援向量機（SVM with Kernel）、神經網路。

#### 按照模型的結構分類

1. 生成模型（Generative Models）：可以生成新的資料點，如樸素貝葉斯、隱馬爾可夫模型（HMM）。
2. 判別模型（Discriminative Models）：僅用於分類或迴歸，如邏輯迴歸、支援向量機。

如果不理解上面的分類也沒有關係，後續的課程中我們會覆蓋到上面說到的很多演算法，我們會透過講解演算法的原理、適用場景、優缺點和程式碼實現來幫助大家掌握這些演算法並將其應用於解決實際問題。

### 機器學習的步驟

機器學習的實施步驟通常分為多個階段，從問題定義、資料準備到模型部署和維護，每個步驟都非常重要，具體如下所示。

1. **定義問題**。首先我們需要做業務理解，明確問題的性質和型別，這個會直接影響到後續的資料收集、特徵工程、選擇演算法以及評估指標的確定。

2. **資料收集**。機器學習模型的訓練需要大量的資料，這些資料可能包含結構化資料（資料庫、Excel 電子表格等）、非結構化資料（文字、影象、音訊、影片等）、其他型別的資料集。

3. **資料清洗**。資料清洗要確保資料質量高且適合模型訓練，具體包括：缺失值和異常值處理、資料標準化和歸一化、特殊編碼、特徵工程等。

4. **資料劃分**。為了評估機器學習模型的泛化能力，需要將資料劃分為訓練集和測試集。除此以外，還可能使用交叉驗證的方式將資料分成多個子集，每個子集輪流作為驗證集，從而對模型的超引數進行調整。

5. **模型選擇**。針對分類問題、迴歸問題、聚類問題、深度學習，我們選擇的機器學習演算法或模型是不一樣的。

6. **模型訓練**。使用訓練集對模型進行訓練，使模型能夠學習到輸入特徵與目標之間的關係。此外，每個機器學習演算法都有超引數，這些引數需要根據資料和任務來調優。

7. **模型評估**。模型評估主要的目標是確定模型在新資料（測試集）上的表現，確保模型沒有出現過擬合（overfitting）或欠擬合（underfitting），如下圖所示。欠擬合會導致模型的預測效果糟糕，而過擬合會導致模型缺乏泛化能力，即在測試集和新資料上表現欠佳。當然，為了提高模型的效能或適應性，可能還要透過正則化、整合學習、演算法調整等方式進行調優。

    <img src="res/01_overfitting_vs_underfitting.png" style="zoom:85%;">

8. **模型部署**。當你對模型的效能感到滿意時，可以將模型部署到生產環境中，進行實時預測或批次預測。我們透過監控模型在實際應用中的表現，確保其持續保持較好的預測效果。如果模型效能下降，可能需要重新訓練或調整。

9. **模型維護**。機器學習模型不是一成不變的，隨著時間的推移，模型可能需要透過重新訓練、增量學習等方式來維持其效能。

> **說明**：如果對上面出現的概念不是很理解，可以先跳過去，我們會在遇到這些問題的時候展開講解。

### 第一次機器學習

下面，我們用一個極為簡單的例子帶大家開啟第一次機器學習之旅。當然，作為本課的第一個案例，我們並沒有嚴格執行上面提到的機器學習的實施步驟，我們只想透過這個例子對機器學習有一個感性的認知，消除機器學習在很多人心中的“神秘感”。為了研究某城市某類消費者每月收入和每月網購支出的關係，我們收集到了50條樣本資料（後面我們統稱為歷史資料），分別儲存在兩個列表中，如下所示。

```python
# 每月收入
x = [9558, 8835, 9313, 14990, 5564, 11227, 11806, 10242, 11999, 11630,
     6906, 13850, 7483, 8090, 9465, 9938, 11414, 3200, 10731, 19880,
     15500, 10343, 11100, 10020, 7587, 6120, 5386, 12038, 13360, 10885,
     17010, 9247, 13050, 6691, 7890, 9070, 16899, 8975, 8650, 9100,
     10990, 9184, 4811, 14890, 11313, 12547, 8300, 12400, 9853, 12890]
# 每月網購支出
y = [3171, 2183, 3091, 5928, 182, 4373, 5297, 3788, 5282, 4166,
     1674, 5045, 1617, 1707, 3096, 3407, 4674, 361, 3599, 6584,
     6356, 3859, 4519, 3352, 1634, 1032, 1106, 4951, 5309, 3800,
     5672, 2901, 5439, 1478, 1424, 2777, 5682, 2554, 2117, 2845,
     3867, 2962,  882, 5435, 4174, 4948, 2376, 4987, 3329, 5002]
```

我們假設月收入和月網購支出都來自於正態總體，接下來我們可以透過計算皮爾遜相關係數來判定兩組資料是否存在相關性，程式碼如下所示。如果對相關性和相關係數不理解，可以移步到我的[《資料思維和統計思維》](https://www.zhihu.com/column/c_1620074540456964096)專欄看看。

```python
import numpy as np

np.corrcoef(x, y)
```

輸出：

```
array([[1.        , 0.94862936],
       [0.94862936, 1.        ]])
```

可以看出該城市該類人群的月收入和月網購支出之間存在強正相關性（相關係數為`0.94862936`）。當然，計算皮爾遜相關係數也可以使用 scipy 中的`stats.pearsonr`函式，該函式在計算相關係數的同時，還會給出統計檢驗的 P 值，我們可以根據 P 值來判定相關性是否顯著，程式碼如下所示。

```python 
from scipy import stats

stats.pearsonr(x, y)
```

輸出：

```
PearsonRResult(statistic=0.9486293572644154, pvalue=1.2349851929268588e-25)
```

上面，我們已經確認了月收入和月網購支出之間存在強相關性，那麼一個很自然的想法就是透過某人的月收入來預測他的月網購支出，反過來當然也是可以的。為了做到這一點，可以充分利用我們收集到的歷史資料，讓計算機透過對資料的“學習”獲得相應的知識，從而實現對未知狀況的預測。我們可以將上述資料中的 $\small{X}$ 稱為自變數或特徵（*feature*），將 $\small{y}$ 稱為因變數或目標值（*target*），**機器學習的關鍵就是要透過歷史資料掌握如何實現從特徵到目標值的對映**。

#### kNN演算法

要透過月收入預測月網購支出，一個最樸素的想法就是將歷史資料做成一個字典，月收入作為字典中的鍵，月網購支出作為對應的值，這樣就可以透過查字典的方式透過收入查到支出，如下所示。

```python
sample_data = {key: value for key, value in zip(x, y)}
```

但是，輸入的月收入未必在字典中對應的鍵，沒有鍵就無法獲取對應的值，這個時候我們可以找到跟輸入的月收入最為接近的 k 個鍵，對這 k 個鍵對應的值求平均，用這個平均值作為對月網購支出的預測。這裡的方法就是機器學習中最為簡單的 k 最近鄰演算法（kNN），它是一種用於分類和迴歸的非引數統計方法，下面我們用原生 Python 程式碼來實現 kNN 演算法，暫時不使用 NumPy、SciPy、Scikit-Learn 這樣的三方庫，主要幫助大家理解演算法的原理。

```python
import heapq
import statistics


def predict_by_knn(history_data, param_in, k=5):
    """用kNN演算法做預測
    :param history_data: 歷史資料
    :param param_in: 模型的輸入
    :param k: 鄰居數量（預設值為5）
    :return: 模型的輸出（預測值）
    """
    neighbors = heapq.nsmallest(k, history_data, key=lambda x: (x - param_in) ** 2)
    return statistics.mean([history_data[neighbor] for neighbor in neighbors])
```

上面的程式碼中，我們用 Python 中`heapq`模組的`nsmallest`來找到歷史資料中最小的 k 個元素，透過該函式的`key`引數，我們界定了最小指的是跟輸入的引數`param_in`誤差最小，由於誤差有正數和負數，所以通常都需要求平方或者取絕對值。對於算術平均值的計算，我們使用了 Python 中`statistics`模組的`mean`函式，對`heapq`和`statistics`模組不熟悉的可以看看 Python 的官方文件，此處不再贅述。接下來，我們用上面的函式來預測月網購支出，程式碼如下所示。

```python
incomes = [1800, 3500, 5200, 6600, 13400, 17800, 20000, 30000]
for income in incomes:
    print(f'月收入: {income:>5d}元, 月網購支出: {predict_by_knn(sample_data, income):>6.1f}元')
```

輸出：

```
月收入:  1800元, 月網購支出:  712.6元
月收入:  3500元, 月網購支出:  712.6元
月收入:  5200元, 月網購支出:  936.0元
月收入:  6600元, 月網購支出: 1487.0元
月收入: 13400元, 月網購支出: 5148.6元
月收入: 17800元, 月網購支出: 6044.4元
月收入: 20000元, 月網購支出: 6044.4元
月收入: 30000元, 月網購支出: 6044.4元
```

透過上面的輸出，我們可以看出 kNN 演算法的一個弊端，樣本資料的缺失或者不均衡情況會導致預測結果非常糟糕。上面月收入`17800`元、`20000`元、`30000`元的網購支出預測值都是`6044.4`元，那是因為歷史資料中月收入的最大值是`19880`，所以跟它們最近的 k 個鄰居是完全相同的，所以從歷史資料預測出的網購支出也是相同的；同理，月收入`1800`元跟月收入`3500`元的網購支出預測值也是相同的，因為跟`1800`元最近的 k 個鄰居和跟`3500`元最近的 k 個鄰居也是完全相同的。當然，上面我們給出的 kNN 演算法實現還有其他的問題，大家應該不難發現`predict_by_knn`函式中，我們每次找尋最近的 k 個鄰居時，都要將整個歷史資料遍歷一次，如果資料集體量非常大，那麼這個地方就會產生很大的開銷。

#### 迴歸模型

我們換一種思路來預測網購支出，我們將月收入和月網購支出分別作為橫縱座標來繪製散點圖。既然網購支出跟月收入之間存在強正相關，這就意味著可以找一條直線來擬合這些歷史資料點，我們把這條直線的方程 $\small{y = aX + b}$ 稱為迴歸方程或迴歸模型，如下圖所示。

<img src="res/01_regression_model.png" style="zoom:50%;">

現在，我們的問題就變成了如何利用收集到的歷史資料計算出迴歸模型的斜率 $\small{a}$ 和截距 $\small{b}$ 。為了評價迴歸模型的好壞，也就是我們計算出的斜率和截距是否理想，我們可以先定義評判標準，一個簡單且直觀的評判標準就是我們將月收入 $\small{X}$ 帶入迴歸模型，計算出月網購支出的預測值 $\small{\hat{y}}$ ，預測值 $\small{\hat{y}}$ 和真實值 $\small{y}$ 之間的誤差越小，說明我們的迴歸模型越理想。之前我們提到過，計算誤差的地方通常都需要取絕對值或者求平方，我們可以用誤差平方的均值來作為評判標準，通常稱之為均方誤差（MSE），如下所示。

$$
MSE = \frac{1}{n}{\sum_{i=1}^{n}(y_{i} - \hat{y_{i}})^{2}}
$$

根據上面的公式，我們可以寫出計算均方誤差的函式，通常稱之為損失函式。

```python
import statistics


def get_loss(X_, y_, a_, b_):
    """損失函式
    :param X_: 迴歸模型的自變數
    :param y_: 迴歸模型的因變數
    :param a_: 迴歸模型的斜率
    :param b_: 迴歸模型的截距
    :return: MSE（均方誤差）
    """
    y_hat = [a_ * x + b_ for x in X_]
    return statistics.mean([(v1 - v2) ** 2 for v1, v2 in zip(y_, y_hat)])
```

能讓 MSE 達到最小的 $\small{a}$ 和 $\small{b}$ ，我們稱之為迴歸方程的最小二乘解，接下來的工作就是要找到這個最小二乘解。簡單的說，我們要將可能的 $\small{a}$ 和 $\small{b}$ 帶入損失函式，看看什麼樣的 $\small{a}$ 和 $\small{b}$ 能讓損失函式取到最小值。如果對 $\small{a}$ 和 $\small{b}$ 的取值一無所知，我們可以透過不斷產生隨機數的方式來尋找 $\small{a}$ 和 $\small{b}$ ，這種方法稱為蒙特卡洛模擬，通俗點說就是隨機瞎蒙，程式碼如下所示。

```python
import random

# 先將最小損失定義為一個很大的值
min_loss, a, b = 1e12, 0, 0

for _ in range(100000):
    # 透過產生隨機數的方式獲得斜率和截距
    _a, _b = random.random(), random.random() * 4000 - 2000
    # 帶入損失函式計算迴歸模型的MSE
    curr_loss = get_loss(x, y, _a, _b)
    if curr_loss < min_loss:
        # 找到更小的MSE就記為最小損失
        min_loss = curr_loss
        # 記錄下當前最小損失對應的a和b
        a, b = _a, _b

print(f'MSE = {min_loss}')
print(f'{a = }, {b = }')
```

上面的程式碼進行了`100000`次的模擬， $\small{a}$ 和 $\small{b}$ 的值在 $\small{[-2000, 2000)}$ 範圍隨機選擇的，下面是在我的電腦上跑出來的結果。大家可以把自己蒙特卡羅模擬的結果分享到評論區，看看誰的運氣更好，找到了讓誤差更小的 $\small{a}$ 和 $\small{b}$ 。

```
MSE = 270690.1419424315
a = 0.4824040159203802, b = -1515.0162977046068
```

對於數學知識比較豐富的小夥伴，我們可以將回歸模型帶入損失函式，由於 $\small{X}$ 和 $\small{y}$ 是已知的歷史資料，那麼損失函式其實是一個關於 $\small{a}$ 和 $\small{b}$ 的二元函式，如下所示。

$$
MSE = f(a, b) = \frac{1}{n}{\sum_{i=1}^{n}(y_{i} - (ax_{i} + b))^{2}}
$$

根據微積分的極值定理，我們可以對 $\small{f(a, b)}$ 求偏導數，並令其等於0，這樣我們就可以計算讓 $\small{f(a, b)}$ 取到最小值的 $\small{a}$ 和 $\small{b}$ 分別是多少，即：

$$
\frac{\partial{f(a, b)}}{\partial{a}} = -\frac{2}{n}\sum_{i=1}^{n}x_{i}(y_{i} - x_{i}a - b) = 0
$$

$$
\frac{\partial{f(a, b)}}{\partial{b}} = -\frac{2}{n}\sum_{i=1}^{n}(y_i - x_{i}a - b) = 0
$$

求解上面的方程組可以得到：

$$
a = \frac{\sum(x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum(x_{i} - \bar{x})^{2}}
$$

$$
b = \bar{y} - a\bar{x}
$$

需要說明的是，如果迴歸模型本身比較複雜，不像線性模型 $\small{y = aX + b}$ 這麼簡單，可能沒有辦法像上面那樣直接求解方程，而是要透過其他的方式來找尋極值點，這個我們會在後續的內容中會為大家講解。回到剛才的問題，我們可以透過上面的公式計算出 $\small{a}$ 和 $\small{b}$ 的值，為了運算方便，下面直接使用了 NumPy 中的函式，因為 NumPy 中的運算都是向量化的，通常不需要我們自己寫迴圈結構，對 NumPy 不熟悉的小夥伴，可以移步到我的[《基於Python的資料分析》](https://www.zhihu.com/column/c_1217746527315496960)專欄。

```python
import numpy as np

x_bar, y_bar = np.mean(x), np.mean(y)
a = np.dot((x - x_bar), (y - y_bar)) / np.sum((x - x_bar) ** 2)
b = y_bar - a * x_bar
print(f'{a = }, {b = }')
```

輸出：

```
a = 0.482084452824066, b = -1515.2028590756745
```

事實上，NumPy 庫中還有封裝好的函式可以幫我們完成引數擬合，你可以使用`linalg`模組的`lstsq`函式來計算最小二乘解，也可以使用`polyfit`函式或`Polynomial`型別的`fit`方法來獲得最小二乘解，程式碼如下所示。大家可以看看，跟上面求偏導數找極值點獲得的結果是否相同。

```python
a, b = np.polyfit(x, y, deg=1)
print(f'{a = }, {b = }')
```

或

```python
from numpy.polynomial import Polynomial

b, a = Polynomial.fit(x, y, deg=1).convert().coef
print(f'{a = }, {b = }')
```

> **說明**：上面程式碼中的`deg`引數代表多項式的最高次項是幾次項，`deg=1`說明我們使用的是線性迴歸模型。

### 小結

首先，希望本篇內容能對大家瞭解機器學習有那麼一點點幫助；其次，希望透過本章向大家傳達一個理念，自己手撕機器學習的程式碼比直接呼叫三方庫弄出一個結果來其實更有意思，它會讓你對演算法原理、應用場景、利弊情況等都有一個更好的認知。

