## 自然語言處理入門

自然語言處理（Natural Language Processing，NLP）是電腦科學和人工智慧領域中的一個重要分支，旨在使計算機能夠理解、生成和與人類語言進行互動。NLP 的應用場景廣泛，包括文字分類、情感分析、機器翻譯、語音識別、聊天機器人等。隨著深度學習的迅猛發展，NLP 的研究也逐漸由傳統的機器學習方法轉向深度學習方法。經典的 NLP 方法依賴於特徵工程和模型設計，而現代的深度學習方法則更多地依賴於資料驅動和自動特徵學習。

### 詞袋模型

詞袋模型（Bag of Words）是 NLP 中最簡單的文字表示方法之一，它將文字中的每個單詞看作一個“詞袋”，忽略了單詞的順序和語法，只考慮每個單詞出現的頻率。簡單來說，詞袋模型透過將文字轉換為詞頻向量（TF）來表示文字。構造詞袋模型有兩個非常簡單的步驟：

1. **構建詞彙表**：首先掃描所有文件，列出文字中所有的單詞（重複的單詞只保留一項），形成一個詞彙表。
2. **文字向量化**：將每個文件中的單詞對映到詞彙表中，並統計每個單詞的出現次數，形成文件的向量表示。

例如，有三個文件分別是：

1. `I love programming.`
2. `I love machine learning.`
3. `I love apple.`

那麼，構造詞彙表將得到：`['I', 'love', 'programming', 'machine', 'learning', 'apple']`。然後，將每個文件轉換為詞頻向量，分別是：

1. `[1, 1, 1, 0, 0, 0]`
2. `[1, 1, 0, 1, 1, 0]`
3. `[1, 1, 0, 0, 0, 1]`

Scikit-learn 庫提供了構建詞詞袋模型的類`CountVectorizer`，使用的方法如下所示。

```python
from sklearn.feature_extraction.text import CountVectorizer

# 文件列表
documents = [
    'I love programming.',
    'I love machine learning.',
    'I love apple.'
]

# 建立詞袋模型
cv = CountVectorizer()
X = cv.fit_transform(documents)

# 輸出詞彙表和詞頻向量
print('詞彙表:\n', cv.get_feature_names_out())
print('詞頻向量:\n', X.toarray())
```

輸出：

```
詞彙表:  ['apple' 'learning' 'love' 'machine' 'programming']
詞頻向量:
[[0 0 1 0 1]
 [0 1 1 1 0]
 [1 0 1 0 0]]
```

> **注意**：上面的詞彙表中沒有單詞`'I'`，因為這個單詞被認為是停用詞而忽略不計了。所謂停用詞通常是指特定語言中高頻出現但對文字分析任務貢獻較小的詞語。

對於中文文件，需要利用三方庫`jieba`進行中文分詞處理，先將句子拆分為詞，然後再構造詞頻向量，程式碼如下所示。

```python
import jieba
from sklearn.feature_extraction.text import CountVectorizer

# 文件列表
documents = [
    '我在四川大學讀書',
    '四川大學是四川最好的大學',
    '大學校園裡面有很多學生',
]

# 建立詞袋模型並指定分詞函式
cv = CountVectorizer(
    tokenizer=lambda x: jieba.cut(x),
    token_pattern=None
)
X = cv.fit_transform(documents)

# 輸出詞彙表和詞頻向量
print('詞彙表:\n', cv.get_feature_names_out())
print('詞頻向量:\n', X.toarray())
```

> **說明**：如果尚未安裝中文分詞庫，可以透過命令`pip install jieba`進行安裝。

輸出：

```
詞彙表:
 ['四川' '四川大學' '在' '大學' '大學校園' '學生' '很多' '我' '是' '最好' '有' '的' '讀書' '裡面']
詞頻向量:
 [[0 1 1 0 0 0 0 1 0 0 0 0 1 0]
  [1 1 0 1 0 0 0 0 1 1 0 1 0 0]
  [0 0 0 0 1 1 1 0 0 0 1 0 0 1]]
```

如果想去掉中文中常見的停用詞，可以在建立`CountVectorizer`物件時指定`stop_words`引數，我們可以從中文停用詞檔案中載入停用詞清單，程式碼如下所示。

```python
import jieba
from sklearn.feature_extraction.text import CountVectorizer

with open('哈工大停用詞表.txt') as file_obj:
    stop_words_list = file_obj.read().split('\n')

# 文件列表
documents = [
    '我在四川大學讀書',
    '四川大學是四川最好的大學',
    '大學校園裡面有很多學生',
]

# 建立詞袋模型並指定分詞函式
cv = CountVectorizer(
    tokenizer=lambda x: jieba.lcut(x),
    token_pattern=None,
    stop_words=stop_words_list
)
X = cv.fit_transform(documents)

# 輸出詞彙表和詞頻向量
print('詞彙表:\n', cv.get_feature_names_out())
print('詞頻向量:\n', X.toarray())
```

> **說明**：上面用到的中文停用詞檔案可以點選[傳送門](https://github.com/goto456/stopwords)獲得。

輸出：

```
詞彙表:
 ['四川' '四川大學' '大學' '大學校園' '學生' '很多' '最好' '讀書' '裡面']
詞頻向量:
 [[0 1 0 0 0 0 0 1 0]
  [1 1 1 0 0 0 1 0 0]
  [0 0 0 1 1 1 0 0 1]]
```

### 詞向量

與詞袋模型不同，詞向量（Word Embedding）將每個單詞對映到一個稠密的向量空間中，從而使計算機能夠理解和處理文字資料。這些向量透過捕捉單詞之間的語義和上下文關係，可以讓模型更好地進行詞義匹配和語義分析。例如，單詞`'king'`和`'queen'`之間的關係、單詞`'cat'`和`'dog'`之間的關係等在向量空間中應當有所體現。

常見的詞向量模型有：

1. **Word2Vec**：透過淺層神經網路訓練來學習詞向量，分為兩種架構：CBOW （Continuous Bag Of Words）和 Skip-Gram。
    - CBOW 模型的核心思想是：給定一個上下文詞彙，預測中心詞，即透過上下文中的詞彙來預測目標詞彙，上下文的順序不重要，因為詞袋是一個集合，而集合沒有順序的概念。假設你有一個句子"The cat sits on the mat."，如果目標是預測 "sits" 這個詞，那麼 CBOW 會使用上下文詞彙"The"、"cat"、"on"、"the"、"mat"來預測"sits"。在訓練過程中，CBOW 會根據上下文中的詞來調整 "sits" 的詞向量，使得在上下文環境下該詞能夠更準確地表示語義。透過這種方式，CBOW 學習到的是一個詞彙的上下文相關的表示。
    - 與 CBOW 相反，Skip-Gram 模型的核心思想是：給定一個目標詞，預測該詞的上下文詞彙。即透過一箇中心詞來預測其周圍的詞彙。假設我們還是以句子"The cat sits on the mat."為例，目標詞是"sits"。在 Skip-Gram 模型中，系統會用"sits"來預測它的上下文詞彙，即"The"、"cat"、"on"、"the"、"mat"。Skip-Gram 模型會透過訓練，使得"sits"的詞向量能最大化其在訓練資料中與上下文詞的關聯度。
2. **GloVe**：基於詞頻矩陣的分解，能夠捕捉到詞彙的全域性統計資訊，由斯坦福大學的研究團隊在2014年提出。GloVe 的詞向量可以用於多種自然語言處理任務，如文字分類、情感分析、機器翻譯等。它為後續的深度學習模型提供了有效的輸入特徵。

下面，我們簡單講一下詞向量學習的基本思路，主要有三個核心要素：

1. **輸入和目標**：假設我們使用 Word2Vec 模型（CBOW 或 Skip-Gram），目標是透過一個單詞（中心詞）或上下文詞來學習詞向量。在 Skip-Gram 中，給定一個單詞，目標是預測其上下文中的單詞；在 CBOW 中，給定上下文單詞，目標是預測中心詞。

2. **神經網路模型**：Word2Vec 透過一個簡單的神經網路來學習詞向量，輸入層是每個單詞對應的獨熱編碼；隱藏層是一個低維的稠密向量（即詞向量），它在訓練過程中透過反向傳播逐漸最佳化；輸出層是目標單詞的機率分佈，如下圖所示。

    <img src="res/09_word2vec.png" style="zoom:45%;">

3. **訓練過程**：透過上下文和目標單詞的共現資訊，模型根據每個單詞的上下文調整詞向量，使得語義上相似的詞向量距離更近。訓練過程中，神經網路透過最小化誤差（即預測的詞與實際詞之間的差距）來調整詞向量，使得能夠準確預測詞彙之間的聯絡。

詞向量的維度通常是一個超引數，需要根據實際情況來選擇。通常，常見的維度範圍為 50 到 300 之間，但有時可以更高或更低。較低的維度可能無法捕捉到詞彙的豐富語義資訊，但會減少計算複雜度，適用於訓練資料比較少或對精度要求不高的場景；較高的維度可以捕捉更多的語義特徵，適用於大規模資料集，但計算成本也會增加。

一旦詞向量被訓練出來，它們就能捕捉到詞彙之間的多種關係。最常見的方式是透過計算詞向量之間的距離或相似度來理解它們的關係。計算兩個詞向量之間的餘弦相似度是最常用的方式，餘弦相似度的公式我們之前提到過，如下所示。

$$
\text{Cosine Similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\lVert \mathbf{A} \rVert \lVert \mathbf{B} \rVert}
$$

其中， $\small{\mathbf{A}}$ 和 $\small{\mathbf{B}}$ 是兩個詞的詞向量， $\small{\cdot}$ 是向量的點積運算， $\small{\lVert \mathbf{A} \rVert}$ 和 $\small{\lVert \mathbf{B} \rVert}$ 是它們的模長。餘弦相似度的值介於 -1 到 1 之間，值越大表示兩個詞越相似，越小則表示越不相似。

另一方面，我們可以研究詞向量的空間關係並完成一些有趣的運算。例如，如果我們想知道`'king'`（國王）和`'queen'`（王后）之間的關係，可以透過這樣的方式來探索：

$$
\text{king} − \text{man} + \text{woman} \approx \text{queen}
$$

這個式子表示，`'king'`去掉`'man'`（男人）的部分，再加上`'woman'`（女人）的部分，得到的結果約等於`'queen'`。這樣的運算可以捕捉到性別上的相似性。此外，利用詞向量我們還可以進行聚類分析、詞義消歧（透過上下文和相似度判斷詞在特定場景中的意思）、文字分類、情感分析等操作。

我們用下面的程式碼為大家展示詞向量的構建以及詞與詞相似性的度量，這裡需要安裝一個名為`gensim`的三方庫。

```bash
pip install gensim
```

此外，我們還需要提前載入訓練模型的語料庫，這裡我用的是 Hugging Face 提供了的 `datasets` 庫，透過它能夠載入多種公開資料集，包括一些常用的語料庫。當然，你也可以選擇像`nltk`這樣專門用於自然語言處理的工具包來下載語料庫。如果還沒有安裝`datasets`庫，需要使用下面的命令進行安裝。

```bash
pip install datasets
```

我們先載入 IMDB 資料集，它是一個廣泛使用的文字分類資料集，尤其在情感分析和自然語言處理領域中。IMDB 資料集包含大量電影評論，標註了每條評論的情感傾向（這裡我們暫時用不到）。IMDB 資料集通常包含 50000 條評論，其中 25000 條用於訓練，25000 條用於測試。每條評論都是觀眾對電影的評價，內容有長有短，我們就用它來作為訓練模型的語料庫。

```python
import re

from datasets import load_dataset
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity

# 載入 IMDB 資料集
imdb = load_dataset('imdb')
# 直接將 50000 條評論用作語料
temp = [imdb['unsupervised'][i]['text'] for i in range(50000)]
# 用正規表示式對評論文字進行簡單處理
corpus = [re.sub(r'[^\w\s]', '', x) for x in temp]

# 預處理語料庫（英文分詞）
sentences = [sentence.lower().split() for sentence in corpus]
# 訓練 Word2Vec 模型
# sentences - 輸入語料庫（句子構成的列表，每個句子是一個或多個單詞的列表）
# vector_size - 詞向量的維度（維度越高能夠表示的資訊越多）
# windows - 上下文視窗大小（模型在訓練時使用的上下文單詞的範圍）
# min_count - 忽略頻率低於此值的單詞（過濾掉在語料庫中出現次數較少的單詞）
# workers - 訓練時使用的 CPU 核心數量
# seed - 隨機數種子（用於初始化模型的權重）
model = Word2Vec(sentences, vector_size=100, window=10, min_count=2, workers=4, seed=3)

# 透過模型獲取 king 和 queen 的詞向量
king_vec, queen_vec = model.wv['king'], model.wv['queen']

# 計算兩個詞向量的餘弦相似度
cos_similarity = cosine_similarity([king_vec], [queen_vec])
print(f'king 和 queen 的餘弦相似度: {cos_similarity[0, 0]:.2f}')

# 透過詞向量進行推理（king - man + woman ≈ queen）
man_vec, woman_vec = model.wv['man'], model.wv['woman']
result_vec = king_vec - man_vec + woman_vec
# 查詢與計算結果最相似的三個詞
similar_words = model.wv.similar_by_vector(result_vec, topn=3)
print(f'跟 king - man + woman 最相似的詞:\n {similar_words}')

# 查詢與 dog 最相似的五個詞
dog_similar_words = model.wv.most_similar('dog', topn=5)
print(f'跟 dog 最相似的詞:\n {dog_similar_words}')
```

輸出：

```
king 和 queen 的餘弦相似度: 0.43
跟 king - man + woman 最相似的詞:
 [('queen', 0.7126370668411255), ('princess', 0.6760246753692627), ('mary', 0.5891180038452148)]
跟 dog 最相似的詞:
 [('cat', 0.7801533937454224), ('sheep', 0.6783771514892578), ('pet', 0.6658452749252319), 
  ('doll', 0.655034065246582), ('dude', 0.6548768877983093)]
```

從程式碼的輸出可以看出，跟`'king' - 'man' + 'woman'`最相似的詞就是`'queen'`和`'princess'`（公主、王妃），另外一個單詞是女孩子的名字。跟`'dog'`（狗）最相似的單詞有：`'cat'`（貓）、`'sheep'`（綿羊）、`'pet'`（寵物）、`'doll'`（玩偶）和`'dude'`（傢伙）。

### NPLM和RNN

Word2Vec 雖然已經幫助我們掌握了詞與詞之間的聯絡，但是由於無法捕捉長距離依賴（超出上下文視窗的詞彙）和無法處理未知詞（訓練中沒有出現的詞彙）等問題，無法讓 NLP 在具體的下游任務中實現突破和應用落地。隨著對 Word2Vec 侷限性的認識，研究者提出了 **NPLM**（神經網路語言模型），採用神經網路結構來預測單詞序列中的下一個詞。與傳統的語言模型不同，NPLM 透過神經網路（通常是前饋神經網路）來處理整個單詞序列，而不僅僅是區域性的上下文資訊，從而能夠建模更為複雜的語言結構，其結構如下圖所示。

<img src="res/09_nplm_model.png" style="zoom:85%;">

當然，NPLM 模型也存在無法有效捕捉長期依賴關係的問題，為此，研究者又提出了 **RNN**（迴圈神經網路）模型。RNN 是一種能夠處理序列資料的神經網路，它的關鍵特性是能夠將前一時刻的隱藏狀態傳遞到下一時刻，從而在時間維度上建立聯絡。RNN 在訓練時能夠捕捉文字中的時序依賴關係，並且不侷限於固定長度的上下文視窗。RNN 在每個時間步依賴於當前輸入和上一時刻的隱藏狀態，能夠有效捕捉序列資料中的時間依賴性，透過反向傳播演算法更新權重，如下圖所示。

<img src="res/09_rnn_model.png" style="zoom:50%;">

為了解決傳統 RNN 中的梯度消失和梯度爆炸問題，研究人員又提出了**長短期記憶**（LSTM） 和**門控迴圈單元**（GRU）等改進演算法。LSTM 和 GRU 都引入了門控機制，允許模型控制資訊的流動和遺忘，從而有效防止了梯度消失問題，可以捕捉長時間序列的依賴關係。但是，LSTM 和 GRU 仍然存在計算效率低、訓練速度慢的問題，且無法進行並行化訓練的問題。

### Seq2Seq

起初，研究人員嘗試用一個獨立的 RNN 來解決機器翻譯、文字摘要、語音識別等 NLP 任務，但是發現效果並不理想。主要原因是 RNN 在同時處理輸入和輸出序列時，既要負責編碼又要負責解碼，所以很容易出現資訊損失。後來，谷歌提出了 Seq2Seq 模型，核心思想是透過學習輸入與輸出序列之間的對映關係，從而實現序列到序列的轉換。Seq2Seq 模型通常包含一個**編碼器**和一個**解碼器**，編碼器通常由 RNN 或其變體構成（LSTM、GRU等）構成，其主要任務是讀取輸入序列並將其轉換成上下文向量，上下文向量包含了整個輸入序列的語義資訊，並作為解碼器生成輸出的基礎；解碼器通常也是由 RNN 或其變體構成，其主要任務是根據上下文向量生成目標序列。Seq2Seq 模型的結構如下圖所示。

<img src="res/09_seq2seq_model.png" style="zoom:62%;">

簡單的 Seq2Seq 模型將整個輸入序列的資訊壓縮到一個固定大小的上下文向量中，這可能導致資訊丟失，尤其是在處理長序列時。**注意力機制**是 Seq2Seq 模型的一個重要擴充套件，透過在每個解碼步驟中動態的對編碼器輸出的不同部分進行加權求和，允許解碼器在生成每個詞時能夠關注輸入序列中的相關部分。這樣，模型能夠根據當前的解碼需求自動“聚焦”在輸入的某個子部分，而不是依賴於一個固定大小的上下文向量。

### Transformer

為了進一步克服 LSTM、GRU 等模型在長序列建模中的侷限，2017 年穀歌大腦（Google Brain）團隊的 Vaswani 等人在其論文 [*Attention Is All You Need*](https://arxiv.org/pdf/1706.03762) 中提出了 **Transformer** 架構，這個架構迅速成為 NLP 領域的主流方法，徹底改變了這個領域的生態，大名鼎鼎的 GPT 和 BERT 都是基於 Transformer 架構的。Transformer 的核心是**自注意力機制**（Self-Attention），它能夠為輸入序列中的每個元素分配不同的權重，從而更好的捕捉序列內部的依賴關係。此外，Transformer 摒棄了 RNN 和 LSTM 中的迴圈結構，採用了全新的編碼器-解碼器架構，這種設計使得模型可以並行處理輸入資料，進一步加速訓練的過程。除了 NLP 領域，Transformer 在計算機視覺、語音識別等領域也取得了顯著的成果。

<img src="res/09_transformer_arch.png" style="zoom:45%;">

Transformer 架構如上圖所示，它也是一個編碼器-解碼器結構，圖左邊部分紅框中是堆疊的 N 組編碼器，右邊部分藍框中是堆疊的 N 組解碼器。編碼器負責處理輸入序列並將其轉化為一個上下文相關的表示，解碼器則根據編碼器的輸出生成目標序列（例如翻譯任務中的目標語言）。下面，我們先簡單說一下編碼器和解碼器的構成。

1. 編碼器：每個編碼器都包括多頭注意力機制、前饋神經網路、殘差連線和層歸一化。
2. 解碼器：解碼器結構類似編碼器，但在自注意力層和編碼器-解碼器注意力層之間引入了額外的層，以便處理編碼器輸出的上下文資訊。

我們再來展開說說 Transformer 的核心元件，如下所示。

1. 嵌入層（輸入嵌入和輸出嵌入）：與傳統的詞向量方法（如 Word2Vec 或 GloVe）類似，Transformer 的輸入首先需要透過詞嵌入層將離散的單詞對映為稠密的向量。

2. 位置編碼：由於 Transformer 不像 RNN 或 LSTM 那樣處理序列中的時序資訊，因此需要透過位置編碼（Positional Encoding）來注入詞序資訊。位置編碼的形式通常是一個與詞嵌入大小相同的向量，每個維度編碼了該詞在序列中的位置資訊。常見的做法是使用正弦和餘弦函式來構造這些位置編碼。

3. 自注意力機制：允許模型在每個時間步根據輸入序列中的所有其他詞來計算每個詞的表示。這樣每個詞都能關注到序列中其他位置的資訊，而不依賴於輸入的順序。自注意力機制的原理是：對每個輸入的詞，透過嵌入層計算三個向量，分別是：**查詢向量**（Q）、**鍵向量**（K）、**值向量**（V）。

- 查詢向量：用來查詢其他詞資訊的向量，它代表我們關注的內容，每個輸入詞都會有一個對應的查詢向量。

- 鍵向量：根據輸入序列中每個元素生成的一個向量，它與查詢向量進行對比，計算每個詞與其他詞的注意力分數，這個分數決定了當前詞和其他詞的相關性，計算公式如下，其中， $\small{d_{k}}$ 是鍵向量的維度，這裡除以 $\small{\sqrt{d_k}}$ 的操作是為了防止點積值過大。

$$
\text{Attention Score}(Q, K) = \frac{Q \cdot K^{T}}{\sqrt{d_{k}}}
$$

- 值向量：實際的資訊載體。在計算出注意力分數後，透過加權和運算得到的就是每個詞的最終表示，這個最終表示是該詞的上下文相關的向量，反映了該詞在上下文中的重要性。

$$
\text{Output} = \sum_{i} \text{softmax} \left( \frac{Q \cdot K^{T}}{\sqrt{d_{k}}} \right) \cdot V_{i}
$$

這裡，我們得到的某個詞的最終表示，不僅僅是它自身的語義，還包含了它與其他詞的關係和上下文資訊。

4. 多頭注意力：在標準的自注意力機制中，Q、K、V 向量都是透過同一個線性變換得到的，但這可能會限制資訊的捕獲。多頭注意力透過使用多個並行的注意力頭來解決這個問題，每個頭使用不同的線性變換來捕捉不同的特徵或關係。

5. 前饋神經網路：為每個詞提供非線性的變換，提取更高階別的特徵，增強模型的能力。

6. 殘差連線和層歸一化：Transformer 中廣泛使用了殘差連線（將輸入直接加到輸出上）和層歸一化（對每個樣本的各個特徵進行標準化），這些技術有助於避免訓練過程中梯度消失並加速訓練。

### 總結

從傳統的基於規則、基於統計的方法到深度學習和 Transformer 架構，研究人員一直沒有停下對 NLP 探索的指令碼，即便經歷了人工智慧的幾次寒冬。隨著 GPT 和 BERT 的出現，再到今天各種大模型在不同領域得到廣泛應用，計算機已經能夠理解和生成幾乎所有語言的精髓並順利的透過“圖靈測試”。這一切不僅改變了我們與機器的互動方式，更為資訊的獲取、理解和交流開闢了嶄新的天地，語言的力量永不止步！



