"""
æ–°èèšåˆçˆ¬èŸ² - News Aggregator Scraper
=====================================
é€™æ˜¯ä¸€å€‹ä½¿ç”¨ Requests åº«çš„å®Œæ•´æ–°èçˆ¬èŸ²æ‡‰ç”¨ç¨‹å¼ï¼Œå±•ç¤º HTTP è«‹æ±‚çš„å„ç¨®æŠ€å·§ã€‚

åŠŸèƒ½ï¼š
1. å¾å¤šå€‹æ–°èä¾†æºæ“·å–æœ€æ–°æ–°è
2. è™•ç† Session å’Œ Cookies
3. è‡ªå‹•é‡è©¦æ©Ÿåˆ¶
4. è«‹æ±‚é »ç‡æ§åˆ¶
5. åŒ¯å‡ºç‚ºå¤šç¨®æ ¼å¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python news_scraper.py
"""

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from dataclasses import dataclass, asdict
from typing import Optional, Generator
import json
import csv
import time
import random
from datetime import datetime
from abc import ABC, abstractmethod


@dataclass
class NewsArticle:
    """æ–°èæ–‡ç« è³‡æ–™çµæ§‹"""
    title: str
    url: str
    source: str
    summary: str = ""
    author: str = ""
    published_date: str = ""
    category: str = ""
    image_url: str = ""
    scraped_at: str = ""


class BaseNewsScraper(ABC):
    """æ–°èçˆ¬èŸ²åŸºç¤é¡åˆ¥"""

    def __init__(self, name: str, base_url: str):
        self.name = name
        self.base_url = base_url
        self.session = self._create_session()

    def _create_session(self) -> requests.Session:
        """å‰µå»ºå¸¶æœ‰é‡è©¦æ©Ÿåˆ¶çš„ Session"""
        session = requests.Session()

        # è¨­å®šé‡è©¦ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # è¨­å®šé è¨­æ¨™é ­
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

        return session

    def _polite_request(self, url: str, min_delay: float = 0.5, max_delay: float = 2.0) -> Optional[requests.Response]:
        """ç¦®è²Œçš„è«‹æ±‚ï¼ˆå¸¶æœ‰éš¨æ©Ÿå»¶é²ï¼‰"""
        time.sleep(random.uniform(min_delay, max_delay))

        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            print(f"  âš ï¸ è«‹æ±‚å¤±æ•—: {url} - {e}")
            return None

    @abstractmethod
    def scrape(self) -> Generator[NewsArticle, None, None]:
        """æ“·å–æ–°èï¼ˆå­é¡åˆ¥å¿…é ˆå¯¦ä½œï¼‰"""
        pass


class HackerNewsScraper(BaseNewsScraper):
    """Hacker News çˆ¬èŸ²"""

    def __init__(self):
        super().__init__("Hacker News", "https://news.ycombinator.com")

    def scrape(self, pages: int = 2) -> Generator[NewsArticle, None, None]:
        """æ“·å– Hacker News æ–‡ç« """
        print(f"\nğŸ“° æ­£åœ¨æ“·å– {self.name}...")

        for page in range(1, pages + 1):
            url = f"{self.base_url}/news?p={page}" if page > 1 else self.base_url

            response = self._polite_request(url)
            if not response:
                continue

            soup = BeautifulSoup(response.text, 'html.parser')

            # æ‰¾åˆ°æ‰€æœ‰æ–‡ç« 
            for item in soup.select('tr.athing'):
                try:
                    title_cell = item.select_one('td.title > span.titleline > a')
                    if not title_cell:
                        continue

                    title = title_cell.text.strip()
                    link = title_cell.get('href', '')

                    # è™•ç†ç›¸å°é€£çµ
                    if link.startswith('item?'):
                        link = f"{self.base_url}/{link}"

                    # å–å¾—å‰¯æ¨™é¡Œè³‡è¨Šï¼ˆåˆ†æ•¸ã€ä½œè€…ç­‰ï¼‰
                    subtext = item.find_next_sibling('tr')
                    score = ""
                    author = ""
                    age = ""

                    if subtext:
                        score_elem = subtext.select_one('span.score')
                        author_elem = subtext.select_one('a.hnuser')
                        age_elem = subtext.select_one('span.age')

                        if score_elem:
                            score = score_elem.text
                        if author_elem:
                            author = author_elem.text
                        if age_elem:
                            age = age_elem.text

                    yield NewsArticle(
                        title=title,
                        url=link,
                        source=self.name,
                        summary=f"Score: {score}" if score else "",
                        author=author,
                        published_date=age,
                        category="Tech",
                        scraped_at=datetime.now().isoformat()
                    )

                except Exception as e:
                    print(f"  âš ï¸ è§£æéŒ¯èª¤: {e}")
                    continue

            print(f"  âœ“ å®Œæˆç¬¬ {page} é ")


class PythonOrgNewsScraper(BaseNewsScraper):
    """Python.org æ–°èçˆ¬èŸ²"""

    def __init__(self):
        super().__init__("Python.org", "https://www.python.org")

    def scrape(self) -> Generator[NewsArticle, None, None]:
        """æ“·å– Python.org æœ€æ–°æ¶ˆæ¯"""
        print(f"\nğŸ“° æ­£åœ¨æ“·å– {self.name}...")

        url = f"{self.base_url}/blogs/"
        response = self._polite_request(url)

        if not response:
            return

        soup = BeautifulSoup(response.text, 'html.parser')

        # æ‰¾åˆ°æ–°èåˆ—è¡¨
        for article in soup.select('ul.list-recent-posts li'):
            try:
                link_elem = article.select_one('a')
                if not link_elem:
                    continue

                title = link_elem.text.strip()
                href = link_elem.get('href', '')
                full_url = f"{self.base_url}{href}" if href.startswith('/') else href

                # å–å¾—æ—¥æœŸ
                date_elem = article.select_one('time')
                date = date_elem.get('datetime', '') if date_elem else ""

                yield NewsArticle(
                    title=title,
                    url=full_url,
                    source=self.name,
                    published_date=date,
                    category="Python",
                    scraped_at=datetime.now().isoformat()
                )

            except Exception as e:
                print(f"  âš ï¸ è§£æéŒ¯èª¤: {e}")
                continue

        print(f"  âœ“ å®Œæˆæ“·å–")


class GitHubTrendingScraper(BaseNewsScraper):
    """GitHub Trending çˆ¬èŸ²"""

    def __init__(self):
        super().__init__("GitHub Trending", "https://github.com")

    def scrape(self, language: str = "python") -> Generator[NewsArticle, None, None]:
        """æ“·å– GitHub è¶¨å‹¢å°ˆæ¡ˆ"""
        print(f"\nğŸ“° æ­£åœ¨æ“·å– {self.name} ({language})...")

        url = f"{self.base_url}/trending/{language}?since=daily"
        response = self._polite_request(url)

        if not response:
            return

        soup = BeautifulSoup(response.text, 'html.parser')

        for repo in soup.select('article.Box-row'):
            try:
                # å°ˆæ¡ˆåç¨±å’Œé€£çµ
                name_elem = repo.select_one('h2 a')
                if not name_elem:
                    continue

                repo_path = name_elem.get('href', '').strip()
                full_url = f"{self.base_url}{repo_path}"
                name = repo_path.strip('/')

                # æè¿°
                desc_elem = repo.select_one('p')
                description = desc_elem.text.strip() if desc_elem else ""

                # èªè¨€
                lang_elem = repo.select_one('[itemprop="programmingLanguage"]')
                lang = lang_elem.text.strip() if lang_elem else language

                # æ˜Ÿæ˜Ÿæ•¸
                stars_elem = repo.select('a.Link--muted')
                stars = ""
                for elem in stars_elem:
                    if 'stargazers' in elem.get('href', ''):
                        stars = elem.text.strip()
                        break

                yield NewsArticle(
                    title=name,
                    url=full_url,
                    source=self.name,
                    summary=description,
                    category=lang,
                    published_date=f"â­ {stars}" if stars else "",
                    scraped_at=datetime.now().isoformat()
                )

            except Exception as e:
                print(f"  âš ï¸ è§£æéŒ¯èª¤: {e}")
                continue

        print(f"  âœ“ å®Œæˆæ“·å–")


class NewsAggregator:
    """æ–°èèšåˆå™¨"""

    def __init__(self):
        self.scrapers = [
            HackerNewsScraper(),
            PythonOrgNewsScraper(),
            GitHubTrendingScraper(),
        ]
        self.articles: list[NewsArticle] = []

    def scrape_all(self):
        """å¾æ‰€æœ‰ä¾†æºæ“·å–æ–°è"""
        print("\n" + "=" * 60)
        print("ğŸš€ é–‹å§‹æ“·å–æ–°è")
        print("=" * 60)

        for scraper in self.scrapers:
            try:
                for article in scraper.scrape():
                    self.articles.append(article)
            except Exception as e:
                print(f"âŒ çˆ¬èŸ² {scraper.name} ç™¼ç”ŸéŒ¯èª¤: {e}")

        print(f"\nâœ… ç¸½å…±æ“·å– {len(self.articles)} ç¯‡æ–‡ç« ")

    def get_by_source(self, source: str) -> list[NewsArticle]:
        """ä¾ä¾†æºç¯©é¸"""
        return [a for a in self.articles if a.source.lower() == source.lower()]

    def get_by_category(self, category: str) -> list[NewsArticle]:
        """ä¾åˆ†é¡ç¯©é¸"""
        return [a for a in self.articles if a.category.lower() == category.lower()]

    def display_summary(self):
        """é¡¯ç¤ºæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ“·å–æ‘˜è¦")
        print("=" * 60)

        # ä¾ä¾†æºçµ±è¨ˆ
        sources = {}
        for article in self.articles:
            sources[article.source] = sources.get(article.source, 0) + 1

        for source, count in sources.items():
            print(f"  {source}: {count} ç¯‡")

        # é¡¯ç¤ºå‰ 10 ç¯‡
        print("\nğŸ“° æœ€æ–°æ–‡ç«  (å‰ 10 ç¯‡)")
        print("-" * 60)

        for i, article in enumerate(self.articles[:10], 1):
            print(f"\n{i}. [{article.source}] {article.title[:50]}...")
            print(f"   ğŸ”— {article.url[:60]}...")
            if article.summary:
                print(f"   ğŸ“ {article.summary[:50]}...")

    def export_json(self, filename: str = "news_data.json"):
        """åŒ¯å‡ºç‚º JSON"""
        data = [asdict(a) for a in self.articles]
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²åŒ¯å‡ºåˆ°: {filename}")

    def export_csv(self, filename: str = "news_data.csv"):
        """åŒ¯å‡ºç‚º CSV"""
        if not self.articles:
            print("âš ï¸ æ²’æœ‰è³‡æ–™å¯åŒ¯å‡º")
            return

        with open(filename, 'w', newline='', encoding='utf-8') as f:
            fieldnames = list(asdict(self.articles[0]).keys())
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for article in self.articles:
                writer.writerow(asdict(article))

        print(f"âœ… å·²åŒ¯å‡ºåˆ°: {filename}")

    def export_markdown(self, filename: str = "news_report.md"):
        """åŒ¯å‡ºç‚º Markdown å ±å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# æ–°èèšåˆå ±å‘Š\n\n")
            f.write(f"> ç”¢ç”Ÿæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"ç¸½å…± {len(self.articles)} ç¯‡æ–‡ç« \n\n")

            # ä¾ä¾†æºåˆ†çµ„
            current_source = ""
            for article in self.articles:
                if article.source != current_source:
                    current_source = article.source
                    f.write(f"\n## {current_source}\n\n")

                f.write(f"### {article.title}\n\n")
                f.write(f"- ğŸ”— é€£çµ: [{article.url}]({article.url})\n")
                if article.summary:
                    f.write(f"- ğŸ“ æ‘˜è¦: {article.summary}\n")
                if article.author:
                    f.write(f"- ğŸ‘¤ ä½œè€…: {article.author}\n")
                if article.published_date:
                    f.write(f"- ğŸ“… æ—¥æœŸ: {article.published_date}\n")
                f.write("\n")

        print(f"âœ… å·²åŒ¯å‡ºåˆ°: {filename}")


def main():
    """ä¸»ç¨‹å¼"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    æ–°èèšåˆçˆ¬èŸ²                                â•‘
â•‘              News Aggregator Scraper                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    aggregator = NewsAggregator()

    # åŸ·è¡Œæ“·å–
    aggregator.scrape_all()

    # é¡¯ç¤ºæ‘˜è¦
    aggregator.display_summary()

    # åŒ¯å‡ºé¸é …
    print("\n" + "=" * 60)
    print("ğŸ“¤ åŒ¯å‡ºé¸é …")
    print("=" * 60)
    print("1. åŒ¯å‡º JSON")
    print("2. åŒ¯å‡º CSV")
    print("3. åŒ¯å‡º Markdown å ±å‘Š")
    print("4. å…¨éƒ¨åŒ¯å‡º")
    print("5. è·³éåŒ¯å‡º")

    choice = input("\nè«‹é¸æ“‡ (1-5): ").strip()

    if choice == '1':
        aggregator.export_json()
    elif choice == '2':
        aggregator.export_csv()
    elif choice == '3':
        aggregator.export_markdown()
    elif choice == '4':
        aggregator.export_json()
        aggregator.export_csv()
        aggregator.export_markdown()
    else:
        print("è·³éåŒ¯å‡º")

    print("\nğŸ‰ å®Œæˆ!")


if __name__ == "__main__":
    main()
