# 資料儲存與處理教學

> 將爬取的資料儲存到各種格式

## 概述

網頁爬蟲的最終目的是取得並儲存有用的資料。本課程將介紹如何將爬取的資料儲存到各種格式，包括 CSV、JSON、Excel 和資料庫。

## 儲存到 CSV

### 使用 Python 內建 csv 模組

```python
import csv

# 爬取的資料
data = [
    {'title': '產品 A', 'price': 100, 'url': 'https://example.com/a'},
    {'title': '產品 B', 'price': 200, 'url': 'https://example.com/b'},
]

# 寫入 CSV
with open('products.csv', 'w', newline='', encoding='utf-8') as f:
    fieldnames = ['title', 'price', 'url']
    writer = csv.DictWriter(f, fieldnames=fieldnames)

    writer.writeheader()
    for item in data:
        writer.writerow(item)
```

### 使用 Pandas

```python
import pandas as pd

# 爬取的資料
data = [
    {'title': '產品 A', 'price': 100, 'url': 'https://example.com/a'},
    {'title': '產品 B', 'price': 200, 'url': 'https://example.com/b'},
]

# 轉換為 DataFrame
df = pd.DataFrame(data)

# 儲存為 CSV
df.to_csv('products.csv', index=False, encoding='utf-8')
```

## 儲存到 JSON

### 基本 JSON 儲存

```python
import json

data = [
    {'title': '產品 A', 'price': 100},
    {'title': '產品 B', 'price': 200},
]

# 儲存為 JSON
with open('products.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
```

### JSON Lines 格式 (適合大型資料集)

```python
import json

data = [
    {'title': '產品 A', 'price': 100},
    {'title': '產品 B', 'price': 200},
]

# 每行一個 JSON 物件
with open('products.jsonl', 'w', encoding='utf-8') as f:
    for item in data:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')
```

## 儲存到 Excel

```python
import pandas as pd

data = [
    {'title': '產品 A', 'price': 100, 'category': '電子'},
    {'title': '產品 B', 'price': 200, 'category': '服飾'},
]

df = pd.DataFrame(data)

# 儲存為 Excel
df.to_excel('products.xlsx', index=False, sheet_name='Products')

# 儲存多個工作表
with pd.ExcelWriter('report.xlsx') as writer:
    df.to_excel(writer, sheet_name='Products', index=False)
    df.describe().to_excel(writer, sheet_name='Statistics')
```

## 儲存到 SQLite 資料庫

```python
import sqlite3
import pandas as pd

# 建立連線
conn = sqlite3.connect('products.db')
cursor = conn.cursor()

# 建立資料表
cursor.execute('''
    CREATE TABLE IF NOT EXISTS products (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        title TEXT NOT NULL,
        price REAL,
        url TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
''')

# 插入資料
data = [
    ('產品 A', 100, 'https://example.com/a'),
    ('產品 B', 200, 'https://example.com/b'),
]

cursor.executemany('''
    INSERT INTO products (title, price, url)
    VALUES (?, ?, ?)
''', data)

conn.commit()

# 使用 Pandas 讀取
df = pd.read_sql_query('SELECT * FROM products', conn)
print(df)

conn.close()
```

## 儲存到 PostgreSQL

```python
import psycopg2
from psycopg2.extras import execute_values

# 建立連線
conn = psycopg2.connect(
    host="localhost",
    database="scraping_db",
    user="postgres",
    password="password"
)
cursor = conn.cursor()

# 建立資料表
cursor.execute('''
    CREATE TABLE IF NOT EXISTS products (
        id SERIAL PRIMARY KEY,
        title VARCHAR(255) NOT NULL,
        price DECIMAL(10, 2),
        url TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
''')

# 批量插入
data = [
    ('產品 A', 100, 'https://example.com/a'),
    ('產品 B', 200, 'https://example.com/b'),
]

execute_values(cursor, '''
    INSERT INTO products (title, price, url)
    VALUES %s
''', data)

conn.commit()
cursor.close()
conn.close()
```

## 儲存到 MongoDB

```python
from pymongo import MongoClient

# 建立連線
client = MongoClient('mongodb://localhost:27017/')
db = client['scraping_db']
collection = db['products']

# 插入單筆資料
product = {'title': '產品 A', 'price': 100, 'url': 'https://example.com/a'}
result = collection.insert_one(product)
print(f'插入 ID: {result.inserted_id}')

# 批量插入
products = [
    {'title': '產品 B', 'price': 200, 'url': 'https://example.com/b'},
    {'title': '產品 C', 'price': 300, 'url': 'https://example.com/c'},
]
result = collection.insert_many(products)
print(f'插入 {len(result.inserted_ids)} 筆')

# 查詢資料
for product in collection.find({'price': {'$gte': 200}}):
    print(product)
```

## 資料清理與處理

### 使用 Pandas 清理資料

```python
import pandas as pd

# 假設這是爬取的原始資料
raw_data = [
    {'title': '  產品 A  ', 'price': '$100.00', 'stock': 'In Stock'},
    {'title': '產品 B', 'price': '$200.50', 'stock': 'Out of Stock'},
    {'title': None, 'price': '$150.00', 'stock': 'In Stock'},
]

df = pd.DataFrame(raw_data)

# 清理標題
df['title'] = df['title'].str.strip()

# 清理價格 (轉換為數字)
df['price'] = df['price'].str.replace('$', '').astype(float)

# 處理庫存 (轉換為布林值)
df['in_stock'] = df['stock'] == 'In Stock'

# 移除空值
df = df.dropna(subset=['title'])

# 移除重複
df = df.drop_duplicates(subset=['title'])

print(df)
```

### 資料驗證

```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class Product:
    title: str
    price: float
    url: str
    description: Optional[str] = None

    def __post_init__(self):
        if not self.title:
            raise ValueError("標題不能為空")
        if self.price < 0:
            raise ValueError("價格不能為負數")
        if not self.url.startswith('http'):
            raise ValueError("URL 格式不正確")

# 使用
try:
    product = Product(
        title="產品 A",
        price=100.0,
        url="https://example.com/a"
    )
    print(product)
except ValueError as e:
    print(f"驗證錯誤: {e}")
```

## 增量儲存

避免重複爬取相同資料：

```python
import sqlite3
import hashlib

def get_url_hash(url):
    return hashlib.md5(url.encode()).hexdigest()

def is_scraped(conn, url):
    cursor = conn.cursor()
    url_hash = get_url_hash(url)
    cursor.execute('SELECT 1 FROM scraped_urls WHERE url_hash = ?', (url_hash,))
    return cursor.fetchone() is not None

def mark_scraped(conn, url):
    cursor = conn.cursor()
    url_hash = get_url_hash(url)
    cursor.execute('INSERT OR IGNORE INTO scraped_urls (url_hash, url) VALUES (?, ?)',
                   (url_hash, url))
    conn.commit()

# 使用
conn = sqlite3.connect('scraping.db')

# 建立追蹤表
conn.execute('''
    CREATE TABLE IF NOT EXISTS scraped_urls (
        url_hash TEXT PRIMARY KEY,
        url TEXT,
        scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
''')

urls_to_scrape = [
    'https://example.com/product/1',
    'https://example.com/product/2',
]

for url in urls_to_scrape:
    if is_scraped(conn, url):
        print(f'跳過 (已爬取): {url}')
        continue

    # 執行爬取...
    print(f'爬取: {url}')

    # 標記為已爬取
    mark_scraped(conn, url)

conn.close()
```

## 定時儲存與備份

```python
import schedule
import time
from datetime import datetime

def backup_data():
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    # 備份邏輯
    print(f'備份完成: backup_{timestamp}.json')

# 每天凌晨 2 點備份
schedule.every().day.at("02:00").do(backup_data)

# 或每 6 小時備份一次
schedule.every(6).hours.do(backup_data)

while True:
    schedule.run_pending()
    time.sleep(60)
```

## 下一步

學會資料儲存後，接下來學習反爬蟲對策，讓你的爬蟲更穩定。

---

*參考資源：ScrapingBee, Oxylabs (2024-2025)*
