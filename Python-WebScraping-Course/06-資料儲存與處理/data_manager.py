"""
è³‡æ–™ç®¡ç†å™¨ - Data Manager
========================
é€™æ˜¯ä¸€å€‹å®Œæ•´çš„çˆ¬èŸ²è³‡æ–™ç®¡ç†æ‡‰ç”¨ç¨‹å¼ï¼Œå±•ç¤ºå¤šç¨®è³‡æ–™å„²å­˜å’Œè™•ç†æŠ€è¡“ã€‚

åŠŸèƒ½ï¼š
1. å¤šæ ¼å¼åŒ¯å…¥åŒ¯å‡ºï¼ˆCSV, JSON, Excel, SQLiteï¼‰
2. è³‡æ–™æ¸…ç†èˆ‡é©—è­‰
3. å¢é‡æ›´æ–°èˆ‡å»é‡
4. è³‡æ–™åˆ†æèˆ‡çµ±è¨ˆ
5. è³‡æ–™å‚™ä»½èˆ‡é‚„åŸ

ä½¿ç”¨æ–¹æ³•ï¼š
    python data_manager.py
"""

import json
import csv
import sqlite3
import hashlib
import shutil
from pathlib import Path
from dataclasses import dataclass, asdict, field
from typing import Optional, Any
from datetime import datetime
from abc import ABC, abstractmethod
import re


# =====================================
# è³‡æ–™æ¨¡å‹
# =====================================

@dataclass
class ScrapedItem:
    """çˆ¬èŸ²è³‡æ–™åŸºç¤çµæ§‹"""
    id: str = ""
    title: str = ""
    url: str = ""
    content: str = ""
    category: str = ""
    tags: list = field(default_factory=list)
    metadata: dict = field(default_factory=dict)
    scraped_at: str = ""
    updated_at: str = ""

    def __post_init__(self):
        if not self.id:
            self.id = self._generate_id()
        if not self.scraped_at:
            self.scraped_at = datetime.now().isoformat()

    def _generate_id(self) -> str:
        """æ ¹æ“š URL ç”Ÿæˆå”¯ä¸€ ID"""
        content = f"{self.url}{self.title}"
        return hashlib.md5(content.encode()).hexdigest()[:12]

    def to_dict(self) -> dict:
        """è½‰æ›ç‚ºå­—å…¸"""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> 'ScrapedItem':
        """å¾å­—å…¸å»ºç«‹"""
        return cls(**{k: v for k, v in data.items() if k in cls.__dataclass_fields__})


# =====================================
# è³‡æ–™æ¸…ç†å™¨
# =====================================

class DataCleaner:
    """è³‡æ–™æ¸…ç†å·¥å…·"""

    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡å­—"""
        if not text:
            return ""
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        text = ' '.join(text.split())
        # ç§»é™¤ç‰¹æ®Šå­—å…ƒ
        text = text.strip()
        return text

    @staticmethod
    def clean_url(url: str) -> str:
        """æ¸…ç† URL"""
        if not url:
            return ""
        url = url.strip()
        # ç¢ºä¿æœ‰å”è­°å‰ç¶´
        if url and not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        return url

    @staticmethod
    def clean_tags(tags: Any) -> list:
        """æ¸…ç†æ¨™ç±¤"""
        if isinstance(tags, str):
            tags = [t.strip() for t in tags.split(',') if t.strip()]
        elif isinstance(tags, list):
            tags = [str(t).strip() for t in tags if t]
        else:
            tags = []
        return list(set(tags))  # å»é‡

    @staticmethod
    def validate_item(item: ScrapedItem) -> tuple[bool, list]:
        """é©—è­‰è³‡æ–™é …ç›®"""
        errors = []

        if not item.title:
            errors.append("æ¨™é¡Œä¸èƒ½ç‚ºç©º")
        if not item.url:
            errors.append("URL ä¸èƒ½ç‚ºç©º")
        elif not item.url.startswith(('http://', 'https://')):
            errors.append("URL æ ¼å¼ä¸æ­£ç¢º")

        return len(errors) == 0, errors

    def clean_item(self, item: ScrapedItem) -> ScrapedItem:
        """æ¸…ç†å–®å€‹è³‡æ–™é …ç›®"""
        item.title = self.clean_text(item.title)
        item.url = self.clean_url(item.url)
        item.content = self.clean_text(item.content)
        item.category = self.clean_text(item.category)
        item.tags = self.clean_tags(item.tags)
        item.updated_at = datetime.now().isoformat()
        return item


# =====================================
# å„²å­˜å¾Œç«¯ï¼ˆæŠ½è±¡é¡åˆ¥ï¼‰
# =====================================

class StorageBackend(ABC):
    """å„²å­˜å¾Œç«¯æŠ½è±¡é¡åˆ¥"""

    @abstractmethod
    def save(self, items: list[ScrapedItem]) -> int:
        pass

    @abstractmethod
    def load(self) -> list[ScrapedItem]:
        pass

    @abstractmethod
    def count(self) -> int:
        pass


# =====================================
# JSON å„²å­˜
# =====================================

class JsonStorage(StorageBackend):
    """JSON æª”æ¡ˆå„²å­˜"""

    def __init__(self, filepath: str = "data.json"):
        self.filepath = Path(filepath)

    def save(self, items: list[ScrapedItem]) -> int:
        """å„²å­˜åˆ° JSON"""
        data = [item.to_dict() for item in items]
        with open(self.filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return len(items)

    def load(self) -> list[ScrapedItem]:
        """å¾ JSON è¼‰å…¥"""
        if not self.filepath.exists():
            return []
        with open(self.filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return [ScrapedItem.from_dict(d) for d in data]

    def count(self) -> int:
        return len(self.load())

    def append(self, items: list[ScrapedItem]) -> int:
        """è¿½åŠ è³‡æ–™"""
        existing = self.load()
        existing.extend(items)
        return self.save(existing)


class JsonLinesStorage(StorageBackend):
    """JSON Lines å„²å­˜ï¼ˆé©åˆå¤§å‹è³‡æ–™é›†ï¼‰"""

    def __init__(self, filepath: str = "data.jsonl"):
        self.filepath = Path(filepath)

    def save(self, items: list[ScrapedItem]) -> int:
        """å„²å­˜åˆ° JSONL"""
        with open(self.filepath, 'w', encoding='utf-8') as f:
            for item in items:
                f.write(json.dumps(item.to_dict(), ensure_ascii=False) + '\n')
        return len(items)

    def load(self) -> list[ScrapedItem]:
        """å¾ JSONL è¼‰å…¥"""
        if not self.filepath.exists():
            return []
        items = []
        with open(self.filepath, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    items.append(ScrapedItem.from_dict(json.loads(line)))
        return items

    def count(self) -> int:
        if not self.filepath.exists():
            return 0
        with open(self.filepath, 'r', encoding='utf-8') as f:
            return sum(1 for _ in f)

    def append(self, items: list[ScrapedItem]) -> int:
        """è¿½åŠ è³‡æ–™"""
        with open(self.filepath, 'a', encoding='utf-8') as f:
            for item in items:
                f.write(json.dumps(item.to_dict(), ensure_ascii=False) + '\n')
        return len(items)


# =====================================
# CSV å„²å­˜
# =====================================

class CsvStorage(StorageBackend):
    """CSV æª”æ¡ˆå„²å­˜"""

    def __init__(self, filepath: str = "data.csv"):
        self.filepath = Path(filepath)
        self.fieldnames = ['id', 'title', 'url', 'content', 'category', 'tags', 'scraped_at', 'updated_at']

    def save(self, items: list[ScrapedItem]) -> int:
        """å„²å­˜åˆ° CSV"""
        with open(self.filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=self.fieldnames)
            writer.writeheader()
            for item in items:
                row = item.to_dict()
                row['tags'] = ', '.join(row.get('tags', []))
                row['metadata'] = json.dumps(row.get('metadata', {}))
                writer.writerow({k: row.get(k, '') for k in self.fieldnames})
        return len(items)

    def load(self) -> list[ScrapedItem]:
        """å¾ CSV è¼‰å…¥"""
        if not self.filepath.exists():
            return []
        items = []
        with open(self.filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                row['tags'] = [t.strip() for t in row.get('tags', '').split(',') if t.strip()]
                items.append(ScrapedItem.from_dict(row))
        return items

    def count(self) -> int:
        if not self.filepath.exists():
            return 0
        with open(self.filepath, 'r', encoding='utf-8') as f:
            return sum(1 for _ in f) - 1  # æ¸›å»æ¨™é ­


# =====================================
# SQLite å„²å­˜
# =====================================

class SQLiteStorage(StorageBackend):
    """SQLite è³‡æ–™åº«å„²å­˜"""

    def __init__(self, db_path: str = "data.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """åˆå§‹åŒ–è³‡æ–™åº«"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS items (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                url TEXT,
                content TEXT,
                category TEXT,
                tags TEXT,
                metadata TEXT,
                scraped_at TEXT,
                updated_at TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # å»ºç«‹ç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON items(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_scraped_at ON items(scraped_at)')

        conn.commit()
        conn.close()

    def _get_connection(self):
        return sqlite3.connect(self.db_path)

    def save(self, items: list[ScrapedItem]) -> int:
        """å„²å­˜åˆ°è³‡æ–™åº«ï¼ˆæ›´æ–°æˆ–æ’å…¥ï¼‰"""
        conn = self._get_connection()
        cursor = conn.cursor()
        count = 0

        for item in items:
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO items
                    (id, title, url, content, category, tags, metadata, scraped_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    item.id,
                    item.title,
                    item.url,
                    item.content,
                    item.category,
                    ', '.join(item.tags),
                    json.dumps(item.metadata),
                    item.scraped_at,
                    item.updated_at or datetime.now().isoformat()
                ))
                count += 1
            except sqlite3.Error as e:
                print(f"å„²å­˜éŒ¯èª¤: {e}")

        conn.commit()
        conn.close()
        return count

    def load(self, limit: int = None, offset: int = 0) -> list[ScrapedItem]:
        """å¾è³‡æ–™åº«è¼‰å…¥"""
        conn = self._get_connection()
        cursor = conn.cursor()

        query = 'SELECT id, title, url, content, category, tags, metadata, scraped_at, updated_at FROM items'
        if limit:
            query += f' LIMIT {limit} OFFSET {offset}'

        cursor.execute(query)
        items = []

        for row in cursor.fetchall():
            items.append(ScrapedItem(
                id=row[0],
                title=row[1],
                url=row[2],
                content=row[3],
                category=row[4],
                tags=[t.strip() for t in (row[5] or '').split(',') if t.strip()],
                metadata=json.loads(row[6]) if row[6] else {},
                scraped_at=row[7],
                updated_at=row[8]
            ))

        conn.close()
        return items

    def count(self) -> int:
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM items')
        count = cursor.fetchone()[0]
        conn.close()
        return count

    def search(self, keyword: str) -> list[ScrapedItem]:
        """æœå°‹è³‡æ–™"""
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT id, title, url, content, category, tags, metadata, scraped_at, updated_at
            FROM items
            WHERE title LIKE ? OR content LIKE ? OR tags LIKE ?
        ''', (f'%{keyword}%', f'%{keyword}%', f'%{keyword}%'))

        items = []
        for row in cursor.fetchall():
            items.append(ScrapedItem(
                id=row[0], title=row[1], url=row[2], content=row[3],
                category=row[4], tags=[t.strip() for t in (row[5] or '').split(',') if t.strip()],
                metadata=json.loads(row[6]) if row[6] else {},
                scraped_at=row[7], updated_at=row[8]
            ))

        conn.close()
        return items

    def get_stats(self) -> dict:
        """å–å¾—çµ±è¨ˆè³‡è¨Š"""
        conn = self._get_connection()
        cursor = conn.cursor()

        stats = {}

        # ç¸½æ•¸
        cursor.execute('SELECT COUNT(*) FROM items')
        stats['total'] = cursor.fetchone()[0]

        # åˆ†é¡çµ±è¨ˆ
        cursor.execute('SELECT category, COUNT(*) FROM items GROUP BY category ORDER BY COUNT(*) DESC')
        stats['by_category'] = dict(cursor.fetchall())

        # æœ€è¿‘æ›´æ–°
        cursor.execute('SELECT MAX(scraped_at) FROM items')
        stats['last_scraped'] = cursor.fetchone()[0]

        conn.close()
        return stats

    def delete(self, item_id: str) -> bool:
        """åˆªé™¤è³‡æ–™"""
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute('DELETE FROM items WHERE id = ?', (item_id,))
        deleted = cursor.rowcount > 0
        conn.commit()
        conn.close()
        return deleted


# =====================================
# è³‡æ–™ç®¡ç†å™¨
# =====================================

class DataManager:
    """è³‡æ–™ç®¡ç†å™¨ - æ•´åˆæ‰€æœ‰åŠŸèƒ½"""

    def __init__(self, base_dir: str = "scraped_data"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–å„ç¨®å„²å­˜å¾Œç«¯
        self.json_storage = JsonStorage(self.base_dir / "data.json")
        self.jsonl_storage = JsonLinesStorage(self.base_dir / "data.jsonl")
        self.csv_storage = CsvStorage(self.base_dir / "data.csv")
        self.sqlite_storage = SQLiteStorage(str(self.base_dir / "data.db"))

        self.cleaner = DataCleaner()
        self.items: list[ScrapedItem] = []

    def add_item(self, item: ScrapedItem, clean: bool = True) -> bool:
        """æ–°å¢è³‡æ–™é …ç›®"""
        if clean:
            item = self.cleaner.clean_item(item)

        is_valid, errors = self.cleaner.validate_item(item)
        if not is_valid:
            print(f"é©—è­‰å¤±æ•—: {errors}")
            return False

        self.items.append(item)
        return True

    def add_items(self, items: list[ScrapedItem], clean: bool = True) -> int:
        """æ‰¹é‡æ–°å¢"""
        count = 0
        for item in items:
            if self.add_item(item, clean):
                count += 1
        return count

    def remove_duplicates(self) -> int:
        """ç§»é™¤é‡è¤‡é …ç›®"""
        seen = set()
        unique_items = []

        for item in self.items:
            if item.id not in seen:
                seen.add(item.id)
                unique_items.append(item)

        removed = len(self.items) - len(unique_items)
        self.items = unique_items
        return removed

    def save_all(self) -> dict:
        """å„²å­˜åˆ°æ‰€æœ‰æ ¼å¼"""
        results = {}

        print("æ­£åœ¨å„²å­˜è³‡æ–™...")
        results['json'] = self.json_storage.save(self.items)
        print(f"  âœ“ JSON: {results['json']} ç­†")

        results['jsonl'] = self.jsonl_storage.save(self.items)
        print(f"  âœ“ JSONL: {results['jsonl']} ç­†")

        results['csv'] = self.csv_storage.save(self.items)
        print(f"  âœ“ CSV: {results['csv']} ç­†")

        results['sqlite'] = self.sqlite_storage.save(self.items)
        print(f"  âœ“ SQLite: {results['sqlite']} ç­†")

        return results

    def load_from(self, format: str = 'sqlite') -> int:
        """å¾æŒ‡å®šæ ¼å¼è¼‰å…¥"""
        if format == 'json':
            self.items = self.json_storage.load()
        elif format == 'jsonl':
            self.items = self.jsonl_storage.load()
        elif format == 'csv':
            self.items = self.csv_storage.load()
        elif format == 'sqlite':
            self.items = self.sqlite_storage.load()
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ ¼å¼: {format}")

        return len(self.items)

    def export_to(self, format: str, filepath: str = None) -> str:
        """åŒ¯å‡ºåˆ°æŒ‡å®šæ ¼å¼"""
        if not filepath:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = self.base_dir / f"export_{timestamp}.{format}"

        if format == 'json':
            JsonStorage(filepath).save(self.items)
        elif format == 'csv':
            CsvStorage(filepath).save(self.items)
        elif format == 'jsonl':
            JsonLinesStorage(filepath).save(self.items)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ ¼å¼: {format}")

        return str(filepath)

    def backup(self, backup_dir: str = None) -> str:
        """å‚™ä»½è³‡æ–™"""
        if not backup_dir:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_dir = self.base_dir / f"backup_{timestamp}"

        backup_path = Path(backup_dir)
        backup_path.mkdir(parents=True, exist_ok=True)

        # è¤‡è£½æ‰€æœ‰è³‡æ–™æª”æ¡ˆ
        for file in self.base_dir.glob("data.*"):
            shutil.copy2(file, backup_path / file.name)

        print(f"âœ“ å·²å‚™ä»½åˆ°: {backup_path}")
        return str(backup_path)

    def get_statistics(self) -> dict:
        """å–å¾—çµ±è¨ˆè³‡è¨Š"""
        stats = {
            'total_items': len(self.items),
            'categories': {},
            'tags': {},
        }

        for item in self.items:
            # åˆ†é¡çµ±è¨ˆ
            cat = item.category or 'Unknown'
            stats['categories'][cat] = stats['categories'].get(cat, 0) + 1

            # æ¨™ç±¤çµ±è¨ˆ
            for tag in item.tags:
                stats['tags'][tag] = stats['tags'].get(tag, 0) + 1

        # æ’åº
        stats['categories'] = dict(sorted(stats['categories'].items(), key=lambda x: x[1], reverse=True))
        stats['tags'] = dict(sorted(stats['tags'].items(), key=lambda x: x[1], reverse=True)[:20])

        return stats

    def display_summary(self):
        """é¡¯ç¤ºæ‘˜è¦"""
        stats = self.get_statistics()

        print("\n" + "=" * 60)
        print("ğŸ“Š è³‡æ–™æ‘˜è¦")
        print("=" * 60)

        print(f"\nç¸½è³‡æ–™æ•¸: {stats['total_items']}")

        print("\nğŸ“ åˆ†é¡çµ±è¨ˆ:")
        for cat, count in list(stats['categories'].items())[:10]:
            print(f"  {cat}: {count}")

        print("\nğŸ·ï¸ ç†±é–€æ¨™ç±¤:")
        for tag, count in list(stats['tags'].items())[:10]:
            print(f"  #{tag}: {count}")


def demo():
    """ç¤ºç¯„ç¨‹å¼"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                 è³‡æ–™ç®¡ç†å™¨ - Data Manager                      â•‘
â•‘              å¤šæ ¼å¼è³‡æ–™å„²å­˜èˆ‡è™•ç†ç¤ºç¯„                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    manager = DataManager("demo_data")

    # å»ºç«‹ç¯„ä¾‹è³‡æ–™
    sample_data = [
        ScrapedItem(
            title="Python ç¶²é çˆ¬èŸ²å…¥é–€",
            url="https://example.com/python-scraping",
            content="å­¸ç¿’ä½¿ç”¨ Python é€²è¡Œç¶²é çˆ¬èŸ²çš„åŸºç¤æ•™å­¸...",
            category="æ•™å­¸",
            tags=["python", "çˆ¬èŸ²", "å…¥é–€"]
        ),
        ScrapedItem(
            title="BeautifulSoup å®Œæ•´æŒ‡å—",
            url="https://example.com/beautifulsoup-guide",
            content="BeautifulSoup æ˜¯ä¸€å€‹å¼·å¤§çš„ HTML è§£æåº«...",
            category="æ•™å­¸",
            tags=["python", "beautifulsoup", "è§£æ"]
        ),
        ScrapedItem(
            title="Scrapy æ¡†æ¶æ•™å­¸",
            url="https://example.com/scrapy-tutorial",
            content="Scrapy æ˜¯ Python æœ€å¼·å¤§çš„çˆ¬èŸ²æ¡†æ¶...",
            category="æ¡†æ¶",
            tags=["python", "scrapy", "æ¡†æ¶"]
        ),
        ScrapedItem(
            title="åçˆ¬èŸ²å°ç­–",
            url="https://example.com/anti-scraping",
            content="å¦‚ä½•è™•ç†ç¶²ç«™çš„åçˆ¬èŸ²æ©Ÿåˆ¶...",
            category="é€²éš",
            tags=["çˆ¬èŸ²", "åçˆ¬èŸ²", "é€²éš"]
        ),
        ScrapedItem(
            title="è³‡æ–™æ¸…ç†æŠ€å·§",
            url="https://example.com/data-cleaning",
            content="çˆ¬å–è³‡æ–™å¾Œçš„æ¸…ç†å’Œè™•ç†æŠ€å·§...",
            category="è³‡æ–™è™•ç†",
            tags=["è³‡æ–™", "æ¸…ç†", "pandas"]
        ),
    ]

    print("ğŸ“ æ–°å¢ç¯„ä¾‹è³‡æ–™...")
    count = manager.add_items(sample_data)
    print(f"  âœ“ æ–°å¢ {count} ç­†è³‡æ–™")

    # ç§»é™¤é‡è¤‡
    removed = manager.remove_duplicates()
    print(f"  âœ“ ç§»é™¤ {removed} ç­†é‡è¤‡è³‡æ–™")

    # é¡¯ç¤ºæ‘˜è¦
    manager.display_summary()

    # å„²å­˜åˆ°æ‰€æœ‰æ ¼å¼
    print("\n" + "=" * 60)
    manager.save_all()

    # æœå°‹ç¤ºç¯„
    print("\n" + "=" * 60)
    print("ğŸ” æœå°‹ç¤ºç¯„")
    print("=" * 60)

    results = manager.sqlite_storage.search("python")
    print(f"\næœå°‹ 'python' æ‰¾åˆ° {len(results)} ç­†çµæœ:")
    for item in results[:5]:
        print(f"  â€¢ {item.title}")

    # å‚™ä»½
    print("\n" + "=" * 60)
    backup_path = manager.backup()

    print("\n" + "=" * 60)
    print("âœ… ç¤ºç¯„å®Œæˆ!")
    print(f"è³‡æ–™ç›®éŒ„: {manager.base_dir}")


if __name__ == "__main__":
    demo()
