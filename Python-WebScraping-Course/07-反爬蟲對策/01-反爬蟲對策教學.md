# 反爬蟲對策教學

> 讓你的爬蟲更穩定、更不容易被封鎖

## 概述

現代網站大多都有反爬蟲機制。了解這些機制並採取適當的對策，是成功爬取資料的關鍵。

## 常見的反爬蟲機制

1. **IP 封鎖**：同一 IP 請求過多會被封鎖
2. **User-Agent 檢測**：識別非瀏覽器請求
3. **請求頻率限制**：短時間內請求過多會被阻擋
4. **CAPTCHA 驗證碼**：要求人工驗證
5. **JavaScript 檢測**：檢測是否有執行 JavaScript
6. **蜜罐陷阱**：隱藏的連結，只有爬蟲會點擊

## 對策一：設定正確的 Headers

```python
import requests

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-User': '?1',
    'Cache-Control': 'max-age=0',
}

response = requests.get('https://example.com', headers=headers)
```

## 對策二：輪換 User-Agent

```python
import random
import requests

user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
]

def get_random_headers():
    return {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
    }

# 使用
response = requests.get('https://example.com', headers=get_random_headers())
```

### 使用 fake-useragent 套件

```bash
uv pip install fake-useragent
```

```python
from fake_useragent import UserAgent

ua = UserAgent()

# 隨機 User-Agent
print(ua.random)

# 特定瀏覽器
print(ua.chrome)
print(ua.firefox)
print(ua.safari)
```

## 對策三：控制請求頻率

```python
import time
import random
import requests

def polite_request(url, min_delay=1, max_delay=3):
    """禮貌的請求函式，自動加入隨機延遲"""
    time.sleep(random.uniform(min_delay, max_delay))
    return requests.get(url)

# 使用
urls = ['https://example.com/page1', 'https://example.com/page2']
for url in urls:
    response = polite_request(url)
    print(f"爬取: {url}, 狀態碼: {response.status_code}")
```

## 對策四：使用代理 (Proxy)

### 單一代理

```python
import requests

proxies = {
    'http': 'http://user:pass@proxy.example.com:8080',
    'https': 'http://user:pass@proxy.example.com:8080',
}

response = requests.get('https://example.com', proxies=proxies)
```

### 輪換代理

```python
import random
import requests

proxy_list = [
    'http://proxy1.example.com:8080',
    'http://proxy2.example.com:8080',
    'http://proxy3.example.com:8080',
]

def get_with_proxy(url, max_retries=3):
    for _ in range(max_retries):
        proxy = random.choice(proxy_list)
        proxies = {'http': proxy, 'https': proxy}

        try:
            response = requests.get(url, proxies=proxies, timeout=10)
            if response.status_code == 200:
                return response
        except requests.RequestException as e:
            print(f"代理 {proxy} 失敗: {e}")
            continue

    return None
```

## 對策五：處理 Cookies 和 Session

```python
import requests

# 使用 Session 保持 cookies
session = requests.Session()

# 設定 cookies
session.cookies.set('session_id', 'abc123')

# 第一次請求會取得 cookies
response1 = session.get('https://example.com')

# 後續請求會自動帶上 cookies
response2 = session.get('https://example.com/dashboard')
```

## 對策六：處理重試和錯誤

```python
import time
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def create_session_with_retries():
    session = requests.Session()

    # 重試策略
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,  # 1, 2, 4 秒的遞增延遲
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS"]
    )

    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    return session

# 使用
session = create_session_with_retries()
response = session.get('https://example.com')
```

### 指數退避 (Exponential Backoff)

```python
import time
import random
import requests

def request_with_backoff(url, max_retries=5):
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=10)

            if response.status_code == 200:
                return response
            elif response.status_code == 429:  # Too Many Requests
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"請求過於頻繁，等待 {wait_time:.2f} 秒...")
                time.sleep(wait_time)
            else:
                return response

        except requests.RequestException as e:
            wait_time = (2 ** attempt) + random.uniform(0, 1)
            print(f"請求失敗，{wait_time:.2f} 秒後重試: {e}")
            time.sleep(wait_time)

    raise Exception(f"達到最大重試次數: {url}")
```

## 對策七：模擬瀏覽器行為

### 使用 Selenium

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time
import random

options = Options()
options.add_argument('--headless')
options.add_argument('--window-size=1920,1080')
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_experimental_option('excludeSwitches', ['enable-automation'])

driver = webdriver.Chrome(options=options)

# 設定 navigator.webdriver 為 undefined
driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
    'source': '''
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined
        })
    '''
})

driver.get('https://example.com')

# 模擬人類行為
time.sleep(random.uniform(2, 5))

# 模擬滾動
driver.execute_script("window.scrollTo(0, document.body.scrollHeight / 2);")
time.sleep(random.uniform(1, 2))

driver.quit()
```

### 使用 undetected-chromedriver

```bash
uv pip install undetected-chromedriver
```

```python
import undetected_chromedriver as uc

driver = uc.Chrome(headless=True)
driver.get('https://example.com')
print(driver.page_source)
driver.quit()
```

## 對策八：處理 JavaScript 挑戰

有些網站會使用 JavaScript 來驗證請求。使用無頭瀏覽器可以自動處理：

```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(headless=True)
    context = browser.new_context(
        user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        viewport={'width': 1920, 'height': 1080},
    )
    page = context.new_page()

    # 自動處理 JavaScript 挑戰
    page.goto('https://example.com')
    page.wait_for_load_state('networkidle')

    content = page.content()
    print(content)

    browser.close()
```

## 對策九：遵守 robots.txt

```python
import urllib.robotparser
from urllib.parse import urljoin

def can_fetch(base_url, path, user_agent='*'):
    rp = urllib.robotparser.RobotFileParser()
    robots_url = urljoin(base_url, '/robots.txt')
    rp.set_url(robots_url)

    try:
        rp.read()
        target_url = urljoin(base_url, path)
        return rp.can_fetch(user_agent, target_url)
    except:
        return True  # 如果無法讀取，假設允許

# 使用
base_url = 'https://example.com'
if can_fetch(base_url, '/products'):
    print("可以爬取")
else:
    print("不建議爬取")
```

## 最佳實踐清單

1. **始終設定正確的 User-Agent**
2. **控制請求頻率**：1-3 秒的隨機延遲
3. **使用 Session**：保持 cookies
4. **處理錯誤**：實作重試機制
5. **使用代理**：避免 IP 被封鎖
6. **遵守 robots.txt**：尊重網站規則
7. **設定超時**：避免請求卡住
8. **記錄日誌**：方便除錯

## 完整範例

```python
import time
import random
import logging
import requests
from fake_useragent import UserAgent

# 設定日誌
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PoliteScraper:
    def __init__(self, min_delay=1, max_delay=3):
        self.session = requests.Session()
        self.ua = UserAgent()
        self.min_delay = min_delay
        self.max_delay = max_delay

    def get_headers(self):
        return {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }

    def get(self, url, max_retries=3):
        for attempt in range(max_retries):
            try:
                # 隨機延遲
                delay = random.uniform(self.min_delay, self.max_delay)
                time.sleep(delay)

                response = self.session.get(
                    url,
                    headers=self.get_headers(),
                    timeout=10
                )

                if response.status_code == 200:
                    logger.info(f"成功: {url}")
                    return response
                elif response.status_code == 429:
                    wait_time = (2 ** attempt) + random.uniform(0, 1)
                    logger.warning(f"429 錯誤，等待 {wait_time:.2f} 秒")
                    time.sleep(wait_time)
                else:
                    logger.warning(f"狀態碼 {response.status_code}: {url}")
                    return response

            except requests.RequestException as e:
                logger.error(f"請求錯誤: {e}")
                time.sleep(2 ** attempt)

        logger.error(f"達到最大重試次數: {url}")
        return None

# 使用
scraper = PoliteScraper(min_delay=2, max_delay=5)
response = scraper.get('https://example.com')
if response:
    print(response.text[:500])
```

---

*參考資源：ScrapingBee, Oxylabs, Scrapfly (2024-2025)*
