"""
ç©©å¥çˆ¬èŸ² - Robust Scraper
=========================
é€™æ˜¯ä¸€å€‹å¯¦ç¾å„ç¨®ååçˆ¬èŸ²æŠ€è¡“çš„å®Œæ•´çˆ¬èŸ²æ‡‰ç”¨ç¨‹å¼ã€‚

åŠŸèƒ½ï¼š
1. éš¨æ©Ÿ User-Agent è¼ªæ›
2. è«‹æ±‚é »ç‡æ§åˆ¶èˆ‡æŒ‡æ•¸é€€é¿
3. ä»£ç†è¼ªæ›æ”¯æ´
4. Session å’Œ Cookie ç®¡ç†
5. è‡ªå‹•é‡è©¦æ©Ÿåˆ¶
6. robots.txt éµå®ˆæª¢æŸ¥
7. è«‹æ±‚æŒ‡ç´‹éš¨æ©ŸåŒ–
8. è©³ç´°æ—¥èªŒè¨˜éŒ„

ä½¿ç”¨æ–¹æ³•ï¼š
    python robust_scraper.py

æ³¨æ„ï¼šæœ¬ç¨‹å¼åƒ…ä¾›å­¸ç¿’ç”¨é€”ï¼Œè«‹éµå®ˆç¶²ç«™çš„æœå‹™æ¢æ¬¾å’Œæ³•å¾‹è¦å®šã€‚
"""

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from dataclasses import dataclass, field
from typing import Optional, Callable
import urllib.robotparser
from urllib.parse import urljoin, urlparse
import random
import time
import logging
import json
from datetime import datetime
from abc import ABC, abstractmethod


# =====================================
# æ—¥èªŒè¨­å®š
# =====================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('RobustScraper')


# =====================================
# User-Agent ç®¡ç†å™¨
# =====================================

class UserAgentManager:
    """User-Agent ç®¡ç†å™¨"""

    # æ¡Œé¢ç€è¦½å™¨
    DESKTOP_AGENTS = [
        # Chrome (Windows)
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        # Chrome (Mac)
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        # Firefox
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0',
        # Safari
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
        # Edge
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
    ]

    # è¡Œå‹•è£ç½®ç€è¦½å™¨
    MOBILE_AGENTS = [
        # iPhone
        'Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1',
        # Android
        'Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
    ]

    def __init__(self, include_mobile: bool = False):
        self.agents = self.DESKTOP_AGENTS.copy()
        if include_mobile:
            self.agents.extend(self.MOBILE_AGENTS)
        self.current_index = 0

    def random(self) -> str:
        """éš¨æ©Ÿé¸æ“‡ä¸€å€‹ User-Agent"""
        return random.choice(self.agents)

    def next(self) -> str:
        """æŒ‰é †åºè¼ªæ› User-Agent"""
        agent = self.agents[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.agents)
        return agent


# =====================================
# ä»£ç†ç®¡ç†å™¨
# =====================================

class ProxyManager:
    """ä»£ç†ç®¡ç†å™¨"""

    def __init__(self, proxies: list[str] = None):
        self.proxies = proxies or []
        self.failed_proxies = set()

    def add_proxy(self, proxy: str):
        """æ–°å¢ä»£ç†"""
        self.proxies.append(proxy)

    def get_proxy(self) -> Optional[dict]:
        """å–å¾—å¯ç”¨ä»£ç†"""
        available = [p for p in self.proxies if p not in self.failed_proxies]
        if not available:
            return None

        proxy = random.choice(available)
        return {
            'http': proxy,
            'https': proxy
        }

    def mark_failed(self, proxy: str):
        """æ¨™è¨˜ä»£ç†ç‚ºå¤±æ•—"""
        self.failed_proxies.add(proxy)
        logger.warning(f"ä»£ç†å¤±æ•—: {proxy}")

    def reset_failed(self):
        """é‡ç½®å¤±æ•—åˆ—è¡¨"""
        self.failed_proxies.clear()


# =====================================
# è«‹æ±‚æŒ‡ç´‹éš¨æ©ŸåŒ–
# =====================================

class RequestFingerprint:
    """è«‹æ±‚æŒ‡ç´‹ç”Ÿæˆå™¨ - æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨è«‹æ±‚"""

    ACCEPT_HEADERS = [
        'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    ]

    ACCEPT_LANGUAGES = [
        'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'zh-TW,zh;q=0.9,en;q=0.8',
        'en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7',
        'en-US,en;q=0.9',
    ]

    ACCEPT_ENCODINGS = [
        'gzip, deflate, br',
        'gzip, deflate',
    ]

    @classmethod
    def generate(cls, user_agent: str) -> dict:
        """ç”Ÿæˆéš¨æ©Ÿè«‹æ±‚æ¨™é ­"""
        headers = {
            'User-Agent': user_agent,
            'Accept': random.choice(cls.ACCEPT_HEADERS),
            'Accept-Language': random.choice(cls.ACCEPT_LANGUAGES),
            'Accept-Encoding': random.choice(cls.ACCEPT_ENCODINGS),
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

        # éš¨æ©ŸåŠ å…¥å…¶ä»–æ¨™é ­
        if random.random() > 0.5:
            headers['Cache-Control'] = 'max-age=0'

        if random.random() > 0.5:
            headers['Sec-Fetch-Dest'] = 'document'
            headers['Sec-Fetch-Mode'] = 'navigate'
            headers['Sec-Fetch-Site'] = random.choice(['none', 'same-origin', 'same-site'])
            headers['Sec-Fetch-User'] = '?1'

        return headers


# =====================================
# Robots.txt æª¢æŸ¥å™¨
# =====================================

class RobotsChecker:
    """Robots.txt éµå®ˆæª¢æŸ¥å™¨"""

    def __init__(self):
        self.parsers = {}

    def can_fetch(self, url: str, user_agent: str = '*') -> bool:
        """æª¢æŸ¥æ˜¯å¦å…è¨±çˆ¬å–"""
        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"

        if base_url not in self.parsers:
            rp = urllib.robotparser.RobotFileParser()
            robots_url = urljoin(base_url, '/robots.txt')
            rp.set_url(robots_url)

            try:
                rp.read()
                self.parsers[base_url] = rp
            except Exception as e:
                logger.warning(f"ç„¡æ³•è®€å– robots.txt: {e}")
                return True  # é è¨­å…è¨±

        return self.parsers[base_url].can_fetch(user_agent, url)

    def get_crawl_delay(self, url: str, user_agent: str = '*') -> Optional[float]:
        """å–å¾—å»ºè­°çš„çˆ¬å–å»¶é²"""
        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"

        if base_url in self.parsers:
            return self.parsers[base_url].crawl_delay(user_agent)
        return None


# =====================================
# é€Ÿç‡é™åˆ¶å™¨
# =====================================

class RateLimiter:
    """è«‹æ±‚é€Ÿç‡é™åˆ¶å™¨"""

    def __init__(self, min_delay: float = 1.0, max_delay: float = 3.0):
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.last_request_time = 0
        self.consecutive_errors = 0

    def wait(self):
        """ç­‰å¾…é©ç•¶çš„æ™‚é–“"""
        # è¨ˆç®—åŸºç¤å»¶é²
        delay = random.uniform(self.min_delay, self.max_delay)

        # æŒ‡æ•¸é€€é¿ï¼ˆé‡åˆ°éŒ¯èª¤æ™‚å¢åŠ å»¶é²ï¼‰
        if self.consecutive_errors > 0:
            delay *= (2 ** min(self.consecutive_errors, 5))
            logger.info(f"æŒ‡æ•¸é€€é¿: ç­‰å¾… {delay:.2f} ç§’")

        # ç¢ºä¿èˆ‡ä¸Šæ¬¡è«‹æ±‚æœ‰è¶³å¤ é–“éš”
        elapsed = time.time() - self.last_request_time
        if elapsed < delay:
            time.sleep(delay - elapsed)

        self.last_request_time = time.time()

    def record_success(self):
        """è¨˜éŒ„æˆåŠŸè«‹æ±‚"""
        self.consecutive_errors = 0

    def record_error(self):
        """è¨˜éŒ„éŒ¯èª¤è«‹æ±‚"""
        self.consecutive_errors += 1


# =====================================
# ç©©å¥çˆ¬èŸ²ä¸»é¡åˆ¥
# =====================================

@dataclass
class ScrapedPage:
    """çˆ¬å–çµæœ"""
    url: str
    status_code: int
    content: str = ""
    soup: Optional[BeautifulSoup] = None
    headers: dict = field(default_factory=dict)
    elapsed_time: float = 0.0
    scraped_at: str = ""
    error: str = ""


class RobustScraper:
    """ç©©å¥çˆ¬èŸ² - å¯¦ç¾å„ç¨®ååçˆ¬èŸ²æŠ€è¡“"""

    def __init__(
        self,
        min_delay: float = 1.0,
        max_delay: float = 3.0,
        max_retries: int = 3,
        timeout: int = 30,
        respect_robots: bool = True,
        proxies: list[str] = None
    ):
        # åŸºæœ¬è¨­å®š
        self.max_retries = max_retries
        self.timeout = timeout
        self.respect_robots = respect_robots

        # ç®¡ç†å™¨
        self.ua_manager = UserAgentManager()
        self.proxy_manager = ProxyManager(proxies)
        self.robots_checker = RobotsChecker()
        self.rate_limiter = RateLimiter(min_delay, max_delay)

        # Session
        self.session = self._create_session()

        # çµ±è¨ˆ
        self.stats = {
            'requests': 0,
            'success': 0,
            'failed': 0,
            'retried': 0,
            'blocked': 0
        }

    def _create_session(self) -> requests.Session:
        """å»ºç«‹å¸¶æœ‰é‡è©¦æ©Ÿåˆ¶çš„ Session"""
        session = requests.Session()

        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        return session

    def _check_robots(self, url: str) -> bool:
        """æª¢æŸ¥ robots.txt"""
        if not self.respect_robots:
            return True

        can_fetch = self.robots_checker.can_fetch(url)
        if not can_fetch:
            logger.warning(f"robots.txt ç¦æ­¢çˆ¬å–: {url}")
            self.stats['blocked'] += 1

        # æª¢æŸ¥å»ºè­°å»¶é²
        crawl_delay = self.robots_checker.get_crawl_delay(url)
        if crawl_delay:
            self.rate_limiter.min_delay = max(self.rate_limiter.min_delay, crawl_delay)
            logger.info(f"ä½¿ç”¨ robots.txt å»ºè­°å»¶é²: {crawl_delay}s")

        return can_fetch

    def get(self, url: str, **kwargs) -> ScrapedPage:
        """ç™¼é€ GET è«‹æ±‚"""
        self.stats['requests'] += 1

        # æª¢æŸ¥ robots.txt
        if not self._check_robots(url):
            return ScrapedPage(
                url=url,
                status_code=0,
                error="robots.txt ç¦æ­¢çˆ¬å–",
                scraped_at=datetime.now().isoformat()
            )

        # é€Ÿç‡é™åˆ¶
        self.rate_limiter.wait()

        # ç”Ÿæˆè«‹æ±‚æ¨™é ­
        user_agent = self.ua_manager.random()
        headers = RequestFingerprint.generate(user_agent)
        headers.update(kwargs.pop('headers', {}))

        # å–å¾—ä»£ç†
        proxies = self.proxy_manager.get_proxy()

        attempt = 0
        last_error = ""

        while attempt < self.max_retries:
            try:
                start_time = time.time()

                response = self.session.get(
                    url,
                    headers=headers,
                    proxies=proxies,
                    timeout=self.timeout,
                    **kwargs
                )

                elapsed = time.time() - start_time

                # æª¢æŸ¥æ˜¯å¦è¢«å°é–
                if response.status_code == 403:
                    logger.warning(f"403 Forbidden - å¯èƒ½è¢«å°é–: {url}")
                    self.stats['blocked'] += 1

                if response.status_code == 429:
                    logger.warning(f"429 Too Many Requests - è«‹æ±‚éæ–¼é »ç¹")
                    self.rate_limiter.record_error()
                    attempt += 1
                    self.stats['retried'] += 1
                    continue

                if response.status_code == 200:
                    self.rate_limiter.record_success()
                    self.stats['success'] += 1

                    soup = BeautifulSoup(response.text, 'html.parser')

                    return ScrapedPage(
                        url=url,
                        status_code=response.status_code,
                        content=response.text,
                        soup=soup,
                        headers=dict(response.headers),
                        elapsed_time=elapsed,
                        scraped_at=datetime.now().isoformat()
                    )
                else:
                    logger.warning(f"HTTP {response.status_code}: {url}")
                    return ScrapedPage(
                        url=url,
                        status_code=response.status_code,
                        error=f"HTTP {response.status_code}",
                        scraped_at=datetime.now().isoformat()
                    )

            except requests.exceptions.ProxyError as e:
                if proxies:
                    self.proxy_manager.mark_failed(list(proxies.values())[0])
                    proxies = self.proxy_manager.get_proxy()
                last_error = str(e)
                attempt += 1
                self.stats['retried'] += 1

            except requests.exceptions.Timeout as e:
                logger.warning(f"è«‹æ±‚è¶…æ™‚: {url}")
                last_error = str(e)
                attempt += 1
                self.rate_limiter.record_error()
                self.stats['retried'] += 1

            except requests.exceptions.RequestException as e:
                logger.error(f"è«‹æ±‚éŒ¯èª¤: {url} - {e}")
                last_error = str(e)
                attempt += 1
                self.rate_limiter.record_error()
                self.stats['retried'] += 1

        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
        self.stats['failed'] += 1
        return ScrapedPage(
            url=url,
            status_code=0,
            error=f"é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸: {last_error}",
            scraped_at=datetime.now().isoformat()
        )

    def scrape_multiple(
        self,
        urls: list[str],
        callback: Callable[[ScrapedPage], None] = None
    ) -> list[ScrapedPage]:
        """æ‰¹é‡çˆ¬å–å¤šå€‹ URL"""
        results = []

        for i, url in enumerate(urls, 1):
            logger.info(f"[{i}/{len(urls)}] æ­£åœ¨çˆ¬å–: {url}")

            page = self.get(url)
            results.append(page)

            if callback:
                callback(page)

            if page.status_code == 200:
                logger.info(f"  âœ“ æˆåŠŸ ({page.elapsed_time:.2f}s)")
            else:
                logger.warning(f"  âœ— å¤±æ•—: {page.error}")

        return results

    def get_stats(self) -> dict:
        """å–å¾—çµ±è¨ˆè³‡è¨Š"""
        return {
            **self.stats,
            'success_rate': f"{(self.stats['success'] / max(self.stats['requests'], 1) * 100):.1f}%"
        }

    def print_stats(self):
        """é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š"""
        stats = self.get_stats()
        print("\n" + "=" * 50)
        print("ğŸ“Š çˆ¬å–çµ±è¨ˆ")
        print("=" * 50)
        print(f"  ç¸½è«‹æ±‚æ•¸: {stats['requests']}")
        print(f"  æˆåŠŸ: {stats['success']}")
        print(f"  å¤±æ•—: {stats['failed']}")
        print(f"  é‡è©¦: {stats['retried']}")
        print(f"  è¢«å°é–: {stats['blocked']}")
        print(f"  æˆåŠŸç‡: {stats['success_rate']}")


def demo():
    """ç¤ºç¯„ç¨‹å¼"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  ç©©å¥çˆ¬èŸ² - Robust Scraper                     â•‘
â•‘                ååçˆ¬èŸ²æŠ€è¡“å®Œæ•´å¯¦ä½œ                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # å»ºç«‹çˆ¬èŸ²å¯¦ä¾‹
    scraper = RobustScraper(
        min_delay=1.0,
        max_delay=2.0,
        max_retries=3,
        respect_robots=True
    )

    # æ¸¬è©¦ URL åˆ—è¡¨
    test_urls = [
        "http://quotes.toscrape.com/",
        "http://quotes.toscrape.com/page/2/",
        "http://books.toscrape.com/",
        "https://httpbin.org/html",
        "https://httpbin.org/headers",
    ]

    print("\nğŸš€ é–‹å§‹çˆ¬å–æ¸¬è©¦...")
    print("=" * 50)

    results = []

    def on_page_scraped(page: ScrapedPage):
        """æ¯é çˆ¬å–å®Œæˆçš„å›èª¿"""
        if page.soup:
            title = page.soup.find('title')
            title_text = title.text.strip() if title else "ç„¡æ¨™é¡Œ"
            print(f"  ğŸ“„ {title_text[:40]}...")

    results = scraper.scrape_multiple(test_urls, callback=on_page_scraped)

    # é¡¯ç¤ºçµ±è¨ˆ
    scraper.print_stats()

    # é¡¯ç¤ºè©³ç´°çµæœ
    print("\n" + "=" * 50)
    print("ğŸ“‹ è©³ç´°çµæœ")
    print("=" * 50)

    for page in results:
        status = "âœ“" if page.status_code == 200 else "âœ—"
        print(f"\n{status} {page.url}")
        print(f"  ç‹€æ…‹ç¢¼: {page.status_code}")
        if page.elapsed_time:
            print(f"  è€—æ™‚: {page.elapsed_time:.2f}s")
        if page.error:
            print(f"  éŒ¯èª¤: {page.error}")

    # åŒ¯å‡ºçµæœ
    print("\n" + "=" * 50)
    export = input("æ˜¯å¦åŒ¯å‡ºçµæœ? (y/n): ").strip().lower()

    if export == 'y':
        output_data = []
        for page in results:
            output_data.append({
                'url': page.url,
                'status_code': page.status_code,
                'elapsed_time': page.elapsed_time,
                'scraped_at': page.scraped_at,
                'error': page.error
            })

        with open('scrape_results.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print("âœ… å·²åŒ¯å‡ºåˆ°: scrape_results.json")

    print("\nğŸ‰ å®Œæˆ!")


if __name__ == "__main__":
    demo()
