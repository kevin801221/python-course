"""
æ›¸ç±å•†åŸçˆ¬èŸ² - Books to Scrape
==============================
é€™æ˜¯ä¸€å€‹ä½¿ç”¨ BeautifulSoup çš„å®Œæ•´é›»å•†çˆ¬èŸ²æ‡‰ç”¨ç¨‹å¼ã€‚
ä½¿ç”¨ http://books.toscrape.com ä½œç‚ºç·´ç¿’ç¶²ç«™ï¼ˆå°ˆç‚ºçˆ¬èŸ²ç·´ç¿’è¨­è¨ˆçš„ç¶²ç«™ï¼‰ã€‚

åŠŸèƒ½ï¼š
1. çˆ¬å–æ‰€æœ‰æ›¸ç±åˆ†é¡
2. æ“·å–æ¯æœ¬æ›¸çš„è©³ç´°è³‡è¨Š
3. è™•ç†åˆ†é 
4. è³‡æ–™æ¸…ç†èˆ‡é©—è­‰
5. åƒ¹æ ¼åˆ†æèˆ‡çµ±è¨ˆ

ä½¿ç”¨æ–¹æ³•ï¼š
    python book_scraper.py
"""

import requests
from bs4 import BeautifulSoup
from dataclasses import dataclass, asdict, field
from typing import Optional, Generator
from urllib.parse import urljoin
import json
import csv
import re
import time
import random
from datetime import datetime
from collections import defaultdict


@dataclass
class Book:
    """æ›¸ç±è³‡æ–™çµæ§‹"""
    title: str
    price: float
    rating: int
    availability: str
    url: str
    category: str = ""
    description: str = ""
    upc: str = ""
    product_type: str = ""
    tax: float = 0.0
    num_reviews: int = 0
    image_url: str = ""
    scraped_at: str = ""


class BooksScraper:
    """Books to Scrape çˆ¬èŸ²"""

    BASE_URL = "http://books.toscrape.com"

    # è©•åˆ†å°ç…§è¡¨
    RATING_MAP = {
        'one': 1,
        'two': 2,
        'three': 3,
        'four': 4,
        'five': 5
    }

    def __init__(self, delay_range: tuple = (0.3, 1.0)):
        self.delay_range = delay_range
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        })
        self.books: list[Book] = []
        self.categories: dict[str, str] = {}

    def _request(self, url: str) -> Optional[BeautifulSoup]:
        """ç™¼é€è«‹æ±‚ä¸¦è¿”å› BeautifulSoup ç‰©ä»¶"""
        time.sleep(random.uniform(*self.delay_range))

        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except requests.RequestException as e:
            print(f"  âš ï¸ è«‹æ±‚å¤±æ•—: {url} - {e}")
            return None

    def _parse_price(self, price_text: str) -> float:
        """è§£æåƒ¹æ ¼æ–‡å­—"""
        # ç§»é™¤è²¨å¹£ç¬¦è™Ÿä¸¦è½‰æ›ç‚ºæµ®é»æ•¸
        match = re.search(r'[\d.]+', price_text)
        return float(match.group()) if match else 0.0

    def _parse_rating(self, rating_class: str) -> int:
        """è§£æè©•åˆ†"""
        for word, num in self.RATING_MAP.items():
            if word in rating_class.lower():
                return num
        return 0

    def get_categories(self) -> dict[str, str]:
        """å–å¾—æ‰€æœ‰æ›¸ç±åˆ†é¡"""
        print("\nğŸ“š æ­£åœ¨æ“·å–åˆ†é¡...")

        soup = self._request(self.BASE_URL)
        if not soup:
            return {}

        nav = soup.select_one('div.side_categories ul.nav-list > li > ul')
        if not nav:
            return {}

        for li in nav.select('li > a'):
            name = li.text.strip()
            href = li.get('href', '')
            full_url = urljoin(self.BASE_URL, href)
            self.categories[name] = full_url

        print(f"  âœ“ æ‰¾åˆ° {len(self.categories)} å€‹åˆ†é¡")
        return self.categories

    def scrape_book_list(self, url: str, category: str = "") -> Generator[str, None, None]:
        """æ“·å–æ›¸ç±åˆ—è¡¨é é¢ï¼Œè¿”å›æ›¸ç±è©³æƒ…é  URL"""
        page = 1

        while url:
            print(f"  ğŸ“„ æ­£åœ¨è™•ç†ç¬¬ {page} é ...")
            soup = self._request(url)

            if not soup:
                break

            # æ‰¾åˆ°æ‰€æœ‰æ›¸ç±é€£çµ
            for article in soup.select('article.product_pod'):
                book_link = article.select_one('h3 > a')
                if book_link:
                    href = book_link.get('href', '')
                    # è™•ç†ç›¸å°è·¯å¾‘
                    if href.startswith('../'):
                        href = href.replace('../', '')
                    full_url = urljoin(f"{self.BASE_URL}/catalogue/", href)
                    yield full_url

            # æª¢æŸ¥ä¸‹ä¸€é 
            next_btn = soup.select_one('li.next > a')
            if next_btn:
                next_href = next_btn.get('href', '')
                url = urljoin(url, next_href)
                page += 1
            else:
                url = None

    def scrape_book_detail(self, url: str, category: str = "") -> Optional[Book]:
        """æ“·å–å–®æœ¬æ›¸ç±çš„è©³ç´°è³‡è¨Š"""
        soup = self._request(url)
        if not soup:
            return None

        try:
            # æ¨™é¡Œ
            title_elem = soup.select_one('div.product_main > h1')
            title = title_elem.text.strip() if title_elem else "Unknown"

            # åƒ¹æ ¼
            price_elem = soup.select_one('p.price_color')
            price = self._parse_price(price_elem.text) if price_elem else 0.0

            # è©•åˆ†
            rating_elem = soup.select_one('p.star-rating')
            rating = 0
            if rating_elem:
                classes = rating_elem.get('class', [])
                for cls in classes:
                    if cls.lower() in self.RATING_MAP:
                        rating = self.RATING_MAP[cls.lower()]
                        break

            # åº«å­˜ç‹€æ…‹
            stock_elem = soup.select_one('p.instock')
            availability = stock_elem.text.strip() if stock_elem else "Unknown"
            # æå–æ•¸é‡
            stock_match = re.search(r'(\d+)', availability)
            availability = f"In stock ({stock_match.group(1)} available)" if stock_match else availability

            # æè¿°
            desc_elem = soup.select_one('article.product_page > p')
            description = desc_elem.text.strip() if desc_elem else ""

            # åœ–ç‰‡
            img_elem = soup.select_one('div.item.active > img')
            image_url = ""
            if img_elem:
                img_src = img_elem.get('src', '')
                image_url = urljoin(self.BASE_URL, img_src.replace('../', ''))

            # ç”¢å“è³‡è¨Šè¡¨æ ¼
            upc = ""
            product_type = ""
            tax = 0.0
            num_reviews = 0

            info_table = soup.select('table.table-striped tr')
            for row in info_table:
                th = row.select_one('th')
                td = row.select_one('td')
                if th and td:
                    key = th.text.strip().lower()
                    value = td.text.strip()

                    if 'upc' in key:
                        upc = value
                    elif 'product type' in key:
                        product_type = value
                    elif 'tax' in key:
                        tax = self._parse_price(value)
                    elif 'reviews' in key:
                        num_reviews = int(value) if value.isdigit() else 0

            # å¦‚æœæ²’æœ‰æä¾›åˆ†é¡ï¼Œå˜—è©¦å¾éºµåŒ…å±‘ç²å–
            if not category:
                breadcrumb = soup.select('ul.breadcrumb li')
                if len(breadcrumb) >= 3:
                    category = breadcrumb[2].text.strip()

            return Book(
                title=title,
                price=price,
                rating=rating,
                availability=availability,
                url=url,
                category=category,
                description=description[:500] if description else "",  # é™åˆ¶é•·åº¦
                upc=upc,
                product_type=product_type,
                tax=tax,
                num_reviews=num_reviews,
                image_url=image_url,
                scraped_at=datetime.now().isoformat()
            )

        except Exception as e:
            print(f"  âš ï¸ è§£ææ›¸ç±å¤±æ•—: {url} - {e}")
            return None

    def scrape_all(self, max_books: int = 100):
        """çˆ¬å–æ‰€æœ‰æ›¸ç±ï¼ˆé™åˆ¶æ•¸é‡ä»¥é¿å…éé•·æ™‚é–“ï¼‰"""
        print("\n" + "=" * 60)
        print("ğŸš€ é–‹å§‹çˆ¬å– Books to Scrape")
        print("=" * 60)

        # å…ˆå–å¾—åˆ†é¡
        if not self.categories:
            self.get_categories()

        count = 0
        # çˆ¬å–é¦–é çš„æ›¸ç±åˆ—è¡¨
        print("\nğŸ“š æ­£åœ¨çˆ¬å–æ›¸ç±...")

        for book_url in self.scrape_book_list(f"{self.BASE_URL}/catalogue/page-1.html"):
            if count >= max_books:
                print(f"\nâš ï¸ å·²é”åˆ°æœ€å¤§æ•¸é‡é™åˆ¶ ({max_books})")
                break

            book = self.scrape_book_detail(book_url)
            if book:
                self.books.append(book)
                count += 1
                print(f"  [{count}/{max_books}] {book.title[:40]}... - Â£{book.price}")

        print(f"\nâœ… å…±çˆ¬å– {len(self.books)} æœ¬æ›¸ç±")

    def scrape_category(self, category_name: str, max_books: int = 50):
        """çˆ¬å–ç‰¹å®šåˆ†é¡çš„æ›¸ç±"""
        if not self.categories:
            self.get_categories()

        # æ‰¾åˆ°åŒ¹é…çš„åˆ†é¡
        category_url = None
        for name, url in self.categories.items():
            if category_name.lower() in name.lower():
                category_url = url
                category_name = name
                break

        if not category_url:
            print(f"âŒ æ‰¾ä¸åˆ°åˆ†é¡: {category_name}")
            print(f"å¯ç”¨åˆ†é¡: {', '.join(self.categories.keys())}")
            return

        print(f"\nğŸ“š æ­£åœ¨çˆ¬å–åˆ†é¡: {category_name}")

        count = 0
        for book_url in self.scrape_book_list(category_url, category_name):
            if count >= max_books:
                break

            book = self.scrape_book_detail(book_url, category_name)
            if book:
                self.books.append(book)
                count += 1
                print(f"  [{count}] {book.title[:40]}... - Â£{book.price}")

    def analyze(self):
        """åˆ†æçˆ¬å–çš„è³‡æ–™"""
        if not self.books:
            print("âŒ æ²’æœ‰è³‡æ–™å¯åˆ†æ")
            return

        print("\n" + "=" * 60)
        print("ğŸ“Š è³‡æ–™åˆ†æ")
        print("=" * 60)

        # åŸºæœ¬çµ±è¨ˆ
        prices = [b.price for b in self.books]
        ratings = [b.rating for b in self.books if b.rating > 0]

        print(f"\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆ:")
        print(f"  ç¸½æ›¸ç±æ•¸é‡: {len(self.books)}")
        print(f"  åƒ¹æ ¼ç¯„åœ: Â£{min(prices):.2f} - Â£{max(prices):.2f}")
        print(f"  å¹³å‡åƒ¹æ ¼: Â£{sum(prices)/len(prices):.2f}")
        print(f"  å¹³å‡è©•åˆ†: {sum(ratings)/len(ratings):.1f}/5" if ratings else "  è©•åˆ†: N/A")

        # åˆ†é¡çµ±è¨ˆ
        category_counts = defaultdict(int)
        category_prices = defaultdict(list)
        for book in self.books:
            category_counts[book.category] += 1
            category_prices[book.category].append(book.price)

        print(f"\nğŸ“ åˆ†é¡çµ±è¨ˆ:")
        for cat, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
            avg_price = sum(category_prices[cat]) / len(category_prices[cat])
            print(f"  {cat}: {count} æœ¬ (å¹³å‡ Â£{avg_price:.2f})")

        # è©•åˆ†åˆ†å¸ƒ
        rating_counts = defaultdict(int)
        for book in self.books:
            rating_counts[book.rating] += 1

        print(f"\nâ­ è©•åˆ†åˆ†å¸ƒ:")
        for rating in range(5, 0, -1):
            count = rating_counts[rating]
            bar = "â–ˆ" * (count // 2)
            print(f"  {rating}æ˜Ÿ: {bar} ({count})")

        # æœ€è²´çš„æ›¸
        print(f"\nğŸ’° æœ€è²´çš„ 5 æœ¬æ›¸:")
        for book in sorted(self.books, key=lambda x: x.price, reverse=True)[:5]:
            print(f"  Â£{book.price:.2f} - {book.title[:40]}...")

        # æœ€ä¾¿å®œçš„æ›¸
        print(f"\nğŸ·ï¸ æœ€ä¾¿å®œçš„ 5 æœ¬æ›¸:")
        for book in sorted(self.books, key=lambda x: x.price)[:5]:
            print(f"  Â£{book.price:.2f} - {book.title[:40]}...")

    def export_json(self, filename: str = "books_data.json"):
        """åŒ¯å‡ºç‚º JSON"""
        data = [asdict(b) for b in self.books]
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²åŒ¯å‡ºåˆ°: {filename}")

    def export_csv(self, filename: str = "books_data.csv"):
        """åŒ¯å‡ºç‚º CSV"""
        if not self.books:
            return

        with open(filename, 'w', newline='', encoding='utf-8') as f:
            fieldnames = list(asdict(self.books[0]).keys())
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for book in self.books:
                writer.writerow(asdict(book))

        print(f"âœ… å·²åŒ¯å‡ºåˆ°: {filename}")


def main():
    """ä¸»ç¨‹å¼"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            æ›¸ç±å•†åŸçˆ¬èŸ² - Books to Scrape                     â•‘
â•‘           BeautifulSoup å®Œæ•´æ‡‰ç”¨ç¯„ä¾‹                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    scraper = BooksScraper()

    # é¡¯ç¤ºé¸é …
    print("è«‹é¸æ“‡çˆ¬å–æ¨¡å¼:")
    print("1. çˆ¬å–æ‰€æœ‰æ›¸ç±ï¼ˆå‰ 50 æœ¬ï¼‰")
    print("2. çˆ¬å–ç‰¹å®šåˆ†é¡")
    print("3. åªé¡¯ç¤ºåˆ†é¡åˆ—è¡¨")

    choice = input("\nè«‹é¸æ“‡ (1-3): ").strip()

    if choice == '1':
        max_books = input("è«‹è¼¸å…¥è¦çˆ¬å–çš„æ•¸é‡ (é è¨­ 50): ").strip()
        max_books = int(max_books) if max_books.isdigit() else 50
        scraper.scrape_all(max_books=max_books)

    elif choice == '2':
        scraper.get_categories()
        print("\nå¯ç”¨åˆ†é¡:")
        for i, cat in enumerate(scraper.categories.keys(), 1):
            print(f"  {i}. {cat}")

        cat_input = input("\nè«‹è¼¸å…¥åˆ†é¡åç¨±: ").strip()
        max_books = input("è«‹è¼¸å…¥è¦çˆ¬å–çš„æ•¸é‡ (é è¨­ 30): ").strip()
        max_books = int(max_books) if max_books.isdigit() else 30
        scraper.scrape_category(cat_input, max_books=max_books)

    elif choice == '3':
        scraper.get_categories()
        print("\næ‰€æœ‰åˆ†é¡:")
        for cat in scraper.categories.keys():
            print(f"  â€¢ {cat}")
        return

    # åˆ†æèˆ‡åŒ¯å‡º
    if scraper.books:
        scraper.analyze()

        print("\n" + "=" * 60)
        export = input("æ˜¯å¦åŒ¯å‡ºè³‡æ–™? (y/n): ").strip().lower()
        if export == 'y':
            scraper.export_json()
            scraper.export_csv()

    print("\nğŸ‰ å®Œæˆ!")


if __name__ == "__main__":
    main()
