# Python 網頁爬蟲概述

> 本教材整理自 2024-2025 年最新網路資源

## 什麼是網頁爬蟲 (Web Scraping)?

**網頁爬蟲**是從網際網路自動收集資訊的過程。即使複製貼上歌詞也可以被視為網頁爬蟲的一種形式！然而，「網頁爬蟲」通常是指涉及自動化的過程。

### 為什麼需要自動化網頁爬蟲？

- **節省時間**：避免手動點擊、滾動和搜尋
- **大規模資料收集**：可以從多個頁面和網站收集大量資料
- **定期監控**：可以設定定時任務自動更新資料
- **減少錯誤**：自動化減少人為錯誤

## 網頁爬蟲的挑戰

1. **多樣性**：每個網站都不同，需要針對性處理
2. **持久性**：網站經常更新，爬蟲需要維護
3. **反爬蟲機制**：許多網站會阻擋自動化請求

## Python 爬蟲工具總覽

| 工具 | 用途 | 難度 | 適用場景 |
|------|------|------|----------|
| **Requests** | HTTP 請求 | 簡單 | API 呼叫、簡單爬蟲 |
| **BeautifulSoup** | HTML 解析 | 簡單 | 靜態網頁資料擷取 |
| **lxml** | XML/HTML 解析 | 中等 | 大型資料集處理 |
| **Selenium** | 瀏覽器自動化 | 中等 | 動態網頁、JavaScript 渲染 |
| **Scrapy** | 爬蟲框架 | 進階 | 大規模爬蟲專案 |
| **Playwright** | 現代瀏覽器自動化 | 中等 | 動態網頁、複雜互動 |

## 網頁爬蟲流程

```
1. 分析目標網站結構
       ↓
2. 設定 Python 環境
       ↓
3. 選擇合適的爬蟲工具
       ↓
4. 發送 HTTP 請求取得網頁內容
       ↓
5. 解析 HTML 擷取所需資料
       ↓
6. 處理分頁與動態內容
       ↓
7. 儲存資料 (CSV, JSON, 資料庫)
       ↓
8. 遵守 robots.txt 與法律規範
```

## 環境設置

### 建立虛擬環境

```bash
# 建立虛擬環境
python -m venv scraping-env

# 啟動虛擬環境 (macOS/Linux)
source scraping-env/bin/activate

# 啟動虛擬環境 (Windows)
scraping-env\Scripts\activate
```

### 安裝必要套件

```bash
# 使用 uv 管理套件 (推薦)
uv pip install requests beautifulsoup4 lxml selenium scrapy pandas

# 或使用 pip
pip install requests beautifulsoup4 lxml selenium scrapy pandas
```

## 法律與道德考量

在進行網頁爬蟲前，請務必：

1. **檢查 robots.txt**：了解網站允許爬取的範圍
2. **遵守服務條款**：閱讀網站的使用條款
3. **控制請求頻率**：避免對伺服器造成負擔
4. **尊重版權**：不要侵犯智慧財產權
5. **保護個人隱私**：遵守 GDPR 等隱私法規

## 檢查 robots.txt

```python
import urllib.robotparser

rp = urllib.robotparser.RobotFileParser()
rp.set_url("https://example.com/robots.txt")
rp.read()

# 檢查是否允許爬取
can_fetch = rp.can_fetch("*", "https://example.com/page")
print(f"允許爬取: {can_fetch}")

# 檢查爬取延遲
delay = rp.crawl_delay("*")
print(f"建議延遲: {delay} 秒")
```

## 下一步

完成基礎概念後，請前往下一課程學習 Requests 與 HTTP 請求的使用方法。

---

*參考資源：ScrapingBee, Oxylabs, Real Python (2024-2025)*
