"""
ç¶²ç«™è³‡è¨Šåˆ†æå™¨ - Website Analyzer
================================
é€™æ˜¯ä¸€å€‹å®Œæ•´çš„ç¶²ç«™è³‡è¨Šæ“·å–å·¥å…·ï¼Œå¯ä»¥åˆ†æç¶²ç«™çš„åŸºæœ¬è³‡è¨Šã€é€£çµã€åœ–ç‰‡ã€Meta æ¨™ç±¤ç­‰ã€‚

åŠŸèƒ½ï¼š
1. æ“·å–ç¶²ç«™åŸºæœ¬è³‡è¨Šï¼ˆæ¨™é¡Œã€æè¿°ã€é—œéµå­—ï¼‰
2. åˆ†ææ‰€æœ‰é€£çµï¼ˆå…§éƒ¨é€£çµ vs å¤–éƒ¨é€£çµï¼‰
3. æ“·å–æ‰€æœ‰åœ–ç‰‡è³‡è¨Š
4. æª¢æŸ¥ç¶²ç«™æŠ€è¡“æ£§
5. è¼¸å‡ºçµæ§‹åŒ–å ±å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python website_analyzer.py https://example.com
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass, field
from typing import Optional
import json
import sys
from datetime import datetime


@dataclass
class WebsiteInfo:
    """ç¶²ç«™è³‡è¨Šè³‡æ–™çµæ§‹"""
    url: str
    title: str = ""
    description: str = ""
    keywords: str = ""
    language: str = ""
    internal_links: list = field(default_factory=list)
    external_links: list = field(default_factory=list)
    images: list = field(default_factory=list)
    scripts: list = field(default_factory=list)
    stylesheets: list = field(default_factory=list)
    meta_tags: dict = field(default_factory=dict)
    headers: dict = field(default_factory=dict)
    status_code: int = 0
    response_time: float = 0.0
    analyzed_at: str = ""


class WebsiteAnalyzer:
    """ç¶²ç«™åˆ†æå™¨é¡åˆ¥"""

    def __init__(self, url: str):
        self.url = url
        self.domain = urlparse(url).netloc
        self.soup: Optional[BeautifulSoup] = None
        self.response: Optional[requests.Response] = None
        self.info = WebsiteInfo(url=url)

        # è¨­å®šè«‹æ±‚æ¨™é ­ï¼Œæ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }

    def fetch_page(self) -> bool:
        """æ“·å–ç¶²é å…§å®¹"""
        try:
            start_time = datetime.now()
            self.response = requests.get(
                self.url,
                headers=self.headers,
                timeout=30,
                allow_redirects=True
            )
            end_time = datetime.now()

            self.info.response_time = (end_time - start_time).total_seconds()
            self.info.status_code = self.response.status_code
            self.info.headers = dict(self.response.headers)
            self.info.analyzed_at = datetime.now().isoformat()

            if self.response.status_code == 200:
                self.soup = BeautifulSoup(self.response.text, 'html.parser')
                return True
            else:
                print(f"è­¦å‘Š: HTTP ç‹€æ…‹ç¢¼ {self.response.status_code}")
                return False

        except requests.RequestException as e:
            print(f"éŒ¯èª¤: ç„¡æ³•é€£æ¥åˆ°ç¶²ç«™ - {e}")
            return False

    def extract_basic_info(self):
        """æ“·å–åŸºæœ¬è³‡è¨Š"""
        if not self.soup:
            return

        # æ¨™é¡Œ
        title_tag = self.soup.find('title')
        self.info.title = title_tag.text.strip() if title_tag else ""

        # èªè¨€
        html_tag = self.soup.find('html')
        if html_tag and html_tag.get('lang'):
            self.info.language = html_tag.get('lang')

        # Meta æ¨™ç±¤
        for meta in self.soup.find_all('meta'):
            name = meta.get('name', meta.get('property', ''))
            content = meta.get('content', '')

            if name and content:
                self.info.meta_tags[name] = content

                if name.lower() == 'description':
                    self.info.description = content
                elif name.lower() == 'keywords':
                    self.info.keywords = content

    def extract_links(self):
        """æ“·å–æ‰€æœ‰é€£çµ"""
        if not self.soup:
            return

        for a_tag in self.soup.find_all('a', href=True):
            href = a_tag.get('href', '')
            text = a_tag.text.strip()

            # è·³éç©ºé€£çµå’ŒéŒ¨é»
            if not href or href.startswith('#') or href.startswith('javascript:'):
                continue

            # è½‰æ›ç›¸å°è·¯å¾‘ç‚ºçµ•å°è·¯å¾‘
            full_url = urljoin(self.url, href)
            parsed = urlparse(full_url)

            link_info = {
                'url': full_url,
                'text': text[:100] if text else '',  # é™åˆ¶é•·åº¦
                'is_https': parsed.scheme == 'https'
            }

            # åˆ¤æ–·å…§éƒ¨æˆ–å¤–éƒ¨é€£çµ
            if parsed.netloc == self.domain or not parsed.netloc:
                self.info.internal_links.append(link_info)
            else:
                self.info.external_links.append(link_info)

    def extract_images(self):
        """æ“·å–æ‰€æœ‰åœ–ç‰‡"""
        if not self.soup:
            return

        for img in self.soup.find_all('img'):
            src = img.get('src', '')
            if not src:
                continue

            full_url = urljoin(self.url, src)

            image_info = {
                'url': full_url,
                'alt': img.get('alt', ''),
                'width': img.get('width', ''),
                'height': img.get('height', ''),
            }
            self.info.images.append(image_info)

    def extract_resources(self):
        """æ“·å– JS å’Œ CSS è³‡æº"""
        if not self.soup:
            return

        # JavaScript
        for script in self.soup.find_all('script', src=True):
            src = script.get('src', '')
            full_url = urljoin(self.url, src)
            self.info.scripts.append(full_url)

        # CSS
        for link in self.soup.find_all('link', rel='stylesheet'):
            href = link.get('href', '')
            full_url = urljoin(self.url, href)
            self.info.stylesheets.append(full_url)

    def detect_technologies(self) -> list:
        """åµæ¸¬ç¶²ç«™ä½¿ç”¨çš„æŠ€è¡“"""
        technologies = []

        if not self.soup:
            return technologies

        html = str(self.soup).lower()
        headers_str = str(self.info.headers).lower()

        # åµæ¸¬å¸¸è¦‹æ¡†æ¶å’ŒæŠ€è¡“
        tech_signatures = {
            'WordPress': ['wp-content', 'wp-includes', 'wordpress'],
            'React': ['react', '_next', 'reactroot'],
            'Vue.js': ['vue', 'v-app', 'nuxt'],
            'Angular': ['ng-', 'angular'],
            'jQuery': ['jquery'],
            'Bootstrap': ['bootstrap'],
            'Tailwind CSS': ['tailwind'],
            'Google Analytics': ['google-analytics', 'gtag', 'ga.js'],
            'Google Tag Manager': ['googletagmanager'],
            'Cloudflare': ['cloudflare'],
            'nginx': ['nginx'],
            'Apache': ['apache'],
        }

        for tech, signatures in tech_signatures.items():
            for sig in signatures:
                if sig in html or sig in headers_str:
                    if tech not in technologies:
                        technologies.append(tech)
                    break

        return technologies

    def analyze(self) -> WebsiteInfo:
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        print(f"\næ­£åœ¨åˆ†æ: {self.url}")
        print("=" * 50)

        if not self.fetch_page():
            return self.info

        print("æ“·å–åŸºæœ¬è³‡è¨Š...")
        self.extract_basic_info()

        print("åˆ†æé€£çµ...")
        self.extract_links()

        print("æ“·å–åœ–ç‰‡...")
        self.extract_images()

        print("æ“·å–è³‡æºæª”æ¡ˆ...")
        self.extract_resources()

        return self.info

    def generate_report(self) -> str:
        """ç”Ÿæˆæ–‡å­—å ±å‘Š"""
        technologies = self.detect_technologies()

        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     ç¶²ç«™åˆ†æå ±å‘Š                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Œ åŸºæœ¬è³‡è¨Š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
URL: {self.info.url}
æ¨™é¡Œ: {self.info.title}
æè¿°: {self.info.description[:100] + '...' if len(self.info.description) > 100 else self.info.description}
èªè¨€: {self.info.language}
HTTP ç‹€æ…‹ç¢¼: {self.info.status_code}
å›æ‡‰æ™‚é–“: {self.info.response_time:.2f} ç§’

ğŸ“Š é€£çµçµ±è¨ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å…§éƒ¨é€£çµ: {len(self.info.internal_links)} å€‹
å¤–éƒ¨é€£çµ: {len(self.info.external_links)} å€‹
åœ–ç‰‡æ•¸é‡: {len(self.info.images)} å€‹
JavaScript æª”æ¡ˆ: {len(self.info.scripts)} å€‹
CSS æª”æ¡ˆ: {len(self.info.stylesheets)} å€‹

ğŸ”§ åµæ¸¬åˆ°çš„æŠ€è¡“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{', '.join(technologies) if technologies else 'ç„¡æ³•åµæ¸¬'}

ğŸ“ Meta æ¨™ç±¤
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"""

        for name, content in list(self.info.meta_tags.items())[:10]:
            report += f"\n{name}: {content[:50]}..."

        report += f"""

ğŸ”— å‰ 10 å€‹å…§éƒ¨é€£çµ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"""
        for link in self.info.internal_links[:10]:
            report += f"\nâ€¢ {link['url'][:70]}..."

        report += f"""

ğŸŒ å‰ 10 å€‹å¤–éƒ¨é€£çµ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"""
        for link in self.info.external_links[:10]:
            report += f"\nâ€¢ {link['url'][:70]}..."

        report += f"""

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åˆ†ææ™‚é–“: {self.info.analyzed_at}
"""
        return report

    def export_json(self, filename: str = None):
        """åŒ¯å‡ºç‚º JSON æª”æ¡ˆ"""
        if not filename:
            # ä½¿ç”¨ç¶²ç«™åŸŸåä½œç‚ºæª”å
            safe_domain = self.domain.replace('.', '_').replace(':', '_')
            filename = f"analysis_{safe_domain}.json"

        data = {
            'url': self.info.url,
            'title': self.info.title,
            'description': self.info.description,
            'keywords': self.info.keywords,
            'language': self.info.language,
            'status_code': self.info.status_code,
            'response_time': self.info.response_time,
            'technologies': self.detect_technologies(),
            'statistics': {
                'internal_links': len(self.info.internal_links),
                'external_links': len(self.info.external_links),
                'images': len(self.info.images),
                'scripts': len(self.info.scripts),
                'stylesheets': len(self.info.stylesheets),
            },
            'meta_tags': self.info.meta_tags,
            'internal_links': self.info.internal_links[:50],  # é™åˆ¶æ•¸é‡
            'external_links': self.info.external_links[:50],
            'images': self.info.images[:50],
            'analyzed_at': self.info.analyzed_at,
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… å·²åŒ¯å‡ºåˆ°: {filename}")
        return filename


def main():
    """ä¸»ç¨‹å¼"""
    # é è¨­æ¸¬è©¦ç¶²å€
    default_url = "https://www.python.org"

    if len(sys.argv) > 1:
        url = sys.argv[1]
    else:
        url = input(f"è«‹è¼¸å…¥è¦åˆ†æçš„ç¶²å€ (é è¨­: {default_url}): ").strip()
        if not url:
            url = default_url

    # ç¢ºä¿ URL æœ‰å”è­°å‰ç¶´
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url

    # åŸ·è¡Œåˆ†æ
    analyzer = WebsiteAnalyzer(url)
    analyzer.analyze()

    # é¡¯ç¤ºå ±å‘Š
    print(analyzer.generate_report())

    # åŒ¯å‡º JSON
    export = input("\næ˜¯å¦åŒ¯å‡º JSON å ±å‘Š? (y/n): ").strip().lower()
    if export == 'y':
        analyzer.export_json()


if __name__ == "__main__":
    main()
